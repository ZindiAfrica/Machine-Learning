{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qKDvjx5HlV4c"
   },
   "source": [
    "## Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jpgoph67lFp1"
   },
   "source": [
    "Can we infer important COVID-19 public health risk factors from outdated data? In many countries census and other survey data may be incomplete or out of date. This challenge is to develop a proof-of-concept for how machine learning can help governments more accurately map COVID-19 risk in 2020 using old data, without requiring a new costly, risky, and time-consuming on-the-ground survey.\n",
    "\n",
    "The 2011 census gives us valuable information for determining who might be most vulnerable to COVID-19 in South Africa. However, the data is nearly 10 years old, and we expect that some key indicators will have changed in that time. Building an up-to-date map showing where the most vulnerable are located will be a key step in responding to the disease. A mapping effort like this requires bringing together many different inputs and tools. For this competition, we’re starting small. Can we infer important risk factors from more readily available data?\n",
    "\n",
    "The task is to predict the percentage of households that fall into a particularly vulnerable bracket - large households who must leave their homes to fetch water - using 2011 South African census data. Solving this challenge will show that with machine learning it is possible to use easy-to-measure stats to identify areas most at risk even in years when census data is not collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "colab_type": "code",
    "id": "709geX4kxNtA",
    "outputId": "0472ac08-ac64-4999-8db0-1d5e66a76d4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: catboost in /home/philmassie/snap/jupyter/common/lib/python3.7/site-packages (0.16.5)\n",
      "Requirement already satisfied: pandas>=0.19.1 in /home/philmassie/snap/jupyter/common/lib/python3.7/site-packages (from catboost) (0.25.3)\n",
      "Requirement already satisfied: numpy>=1.16.0 in /home/philmassie/snap/jupyter/common/lib/python3.7/site-packages (from catboost) (1.17.4)\n",
      "Requirement already satisfied: six in /snap/jupyter/6/lib/python3.7/site-packages (from catboost) (1.12.0)\n",
      "Requirement already satisfied: matplotlib in /home/philmassie/snap/jupyter/common/lib/python3.7/site-packages (from catboost) (3.1.2)\n",
      "Requirement already satisfied: plotly in /home/philmassie/snap/jupyter/common/lib/python3.7/site-packages (from catboost) (4.4.1)\n",
      "Requirement already satisfied: graphviz in /home/philmassie/snap/jupyter/common/lib/python3.7/site-packages (from catboost) (0.13.2)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /snap/jupyter/6/lib/python3.7/site-packages (from pandas>=0.19.1->catboost) (2.8.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/philmassie/snap/jupyter/common/lib/python3.7/site-packages (from pandas>=0.19.1->catboost) (2019.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/philmassie/snap/jupyter/common/lib/python3.7/site-packages (from matplotlib->catboost) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home/philmassie/snap/jupyter/common/lib/python3.7/site-packages (from matplotlib->catboost) (2.4.5)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/philmassie/snap/jupyter/common/lib/python3.7/site-packages (from matplotlib->catboost) (1.1.0)\n",
      "Requirement already satisfied: retrying>=1.3.3 in /home/philmassie/snap/jupyter/common/lib/python3.7/site-packages (from plotly->catboost) (1.3.3)\n",
      "Requirement already satisfied: setuptools in /snap/jupyter/6/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib->catboost) (41.0.1)\n",
      "Collecting rgf-python\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/88/f4/8fad3f0f3183b016720fd96b9b6833913188a2bbf4bf28fd757815fef5fe/rgf_python-3.8.0-py2.py3-none-manylinux1_x86_64.whl (796kB)\n",
      "\u001b[K     |████████████████████████████████| 798kB 3.0MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn>=0.18 in /home/philmassie/snap/jupyter/common/lib/python3.7/site-packages (from rgf-python) (0.22)\n",
      "Requirement already satisfied: joblib in /home/philmassie/snap/jupyter/common/lib/python3.7/site-packages (from rgf-python) (0.14.0)\n",
      "Requirement already satisfied: six in /snap/jupyter/6/lib/python3.7/site-packages (from rgf-python) (1.12.0)\n",
      "Requirement already satisfied: numpy>=1.11.0 in /home/philmassie/snap/jupyter/common/lib/python3.7/site-packages (from scikit-learn>=0.18->rgf-python) (1.17.4)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /home/philmassie/snap/jupyter/common/lib/python3.7/site-packages (from scikit-learn>=0.18->rgf-python) (1.3.3)\n",
      "Installing collected packages: rgf-python\n",
      "Successfully installed rgf-python-3.8.0\n"
     ]
    }
   ],
   "source": [
    "# Installing some of the packages we need into our collab notebook.\n",
    "!pip install catboost\n",
    "!pip install rgf-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZhejZRKQvl6b"
   },
   "outputs": [],
   "source": [
    "# Importing data wrangling and machine Learning packages used.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from io import StringIO \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVR, NuSVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from xgboost import XGBRegressor, XGBRFRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, BayesianRidge\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import RandomForestRegressor, StackingRegressor,HistGradientBoostingRegressor, ExtraTreesRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.cluster import KMeans\n",
    "import warnings\n",
    "from rgf.sklearn import RGFRegressor\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8G1wOxM3wjJ7"
   },
   "outputs": [],
   "source": [
    "# Created links to shared files via google drive\n",
    "#\n",
    "train = 'https://drive.google.com/file/d/1_wLi9i-pUk6Kaizjb5i6-Pd0Vi7L1d-E/view?usp=sharing'\n",
    "test = 'https://drive.google.com/file/d/1OeT53v7tZLnB71n1j4r6KFbtrouz38ej/view?usp=sharing'\n",
    "\n",
    "# Created a function to read a csv file shared via google and return a dataframe\n",
    "#\n",
    "def read_csv(url):\n",
    "  url = 'https://drive.google.com/uc?export=download&id=' + url.split('/')[-2]\n",
    "  csv_raw = requests.get(url).text\n",
    "  csv = StringIO(csv_raw)\n",
    "  df = pd.read_csv(csv)\n",
    "  return df\n",
    "\n",
    "# Creating training and testing datataframes\n",
    "#\n",
    "train = read_csv(train)\n",
    "test = read_csv(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"Train_maskedv2.csv\")\n",
    "test = pd.read_csv(\"Test_maskedv2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "53fiiGbLw0_o"
   },
   "outputs": [],
   "source": [
    "# Combining test and train for easy feature engineering.\n",
    "target = train.target_pct_vunerable\n",
    "\n",
    "train['separator'] = 0\n",
    "test['separator'] = 1\n",
    "\n",
    "train, test = train.align(test, join = 'inner', axis = 1)\n",
    "\n",
    "comb = pd.concat([train, test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "82nWvtOPvl6i"
   },
   "outputs": [],
   "source": [
    "# Examining feature interactions from the most important features from model's feature importances graph and creating new magic features.\n",
    "# While there is no science into it and it's mostly trial and error, the new features improved the score greatly and if we had computational power, we could have explored more interactios.\n",
    "comb['household_size'] = comb.total_individuals/comb.total_households\n",
    "comb['gf_1'] = comb['dw_01'] * comb['psa_01']\n",
    "comb['gf_2'] = comb['gf_1'] * comb['psa_00']\n",
    "comb['gf_3'] = comb['gf_1'] * comb['psa_02']\n",
    "comb['gf_4'] = comb['gf_1'] * comb['psa_03']\n",
    "comb['gf_5'] = comb['gf_1'] * comb['gf_2']\n",
    "comb['gf_6'] = comb['gf_5'] * comb['gf_2']\n",
    "comb['dw_01_2'] = comb['dw_01'] ** 2\n",
    "comb['psa_00_2'] = comb['psa_00'] ** 2\n",
    "luxury_stuff = ['psa_01','car_01','stv_00']\n",
    "not_luxury_stuff = ['psa_00','car_00','stv_01']\n",
    "comb['luxury_stuff'] = comb[luxury_stuff].sum(axis=1)\n",
    "comb['not_luxury_stuff'] = comb[not_luxury_stuff].sum(axis=1)\n",
    "comb['a_luxury_stuff'] = comb[luxury_stuff].mean(axis=1)\n",
    "comb['a_not_luxury_stuff'] = comb[not_luxury_stuff].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WSacY9pzw5Tx"
   },
   "outputs": [],
   "source": [
    "# Separating the train and test datasets.\n",
    "train = comb[comb.separator == 0]\n",
    "test = comb[comb.separator == 1]\n",
    "\n",
    "train.drop('separator', axis = 1, inplace = True)\n",
    "test.drop('separator', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rtPbBXRivl6l"
   },
   "outputs": [],
   "source": [
    "# The columns dropped were those that from the feature importance of the baseline model, were of least importance and just added noise to the model.\n",
    "X = train.drop(columns=['ward', 'dw_13', 'dw_12', 'lan_13', 'psa_03'])\n",
    "y = target.copy()\n",
    "tes = test.drop(['ward', 'dw_13', 'dw_12', 'lan_13', 'psa_03'], 1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j9o95DYKvl6o"
   },
   "outputs": [],
   "source": [
    "# In stacking, the most important thing is model diversification. from linear, SVM, KNN and Decision trees and many variations of them. The variations are different values of key parameters\n",
    "# of each model. \n",
    "# While we did not have the time to tune parameters of each model, except the meta learner Catboost, educated guesses on the parameters were made to have as much variability as possible.\n",
    "estimators_1 = [\n",
    "    ('xgb', XGBRegressor(random_state=2020, objective ='reg:squarederror', learning_rate=0.05)),\n",
    "    ('lr', LinearRegression()),\n",
    "    ('rf', RandomForestRegressor(random_state=2020)),\n",
    "    ('lgb', LGBMRegressor(learning_rate=0.2, random_state=2020)),\n",
    "    ('svr', SVR(degree=2)),\n",
    "    ('lasso', Lasso(random_state=2020)),\n",
    "    ('RGF', RGFRegressor()),\n",
    "    ('kneiba', KNeighborsRegressor(n_neighbors=4)),\n",
    "    ('cat', CatBoostRegressor(logging_level='Silent', random_state=2020))\n",
    "]\n",
    "\n",
    "predictions_1 = StackingRegressor(estimators=estimators_1, final_estimator=CatBoostRegressor(logging_level='Silent', depth=6, bagging_temperature=5, random_state=2020)).fit(X_train, y_train).predict(tes)\n",
    "\n",
    "estimators_2 = [\n",
    "    ('xgb', XGBRegressor(objective ='reg:squarederror', learning_rate=0.2, random_state=2020)),\n",
    "    ('lr', LinearRegression()),\n",
    "    ('rf', RandomForestRegressor(random_state=2020)),\n",
    "    ('lgb', LGBMRegressor(learning_rate=0.05, random_state=2020)),\n",
    "    ('svr', SVR(degree=5)),\n",
    "    ('RGF', RGFRegressor()),\n",
    "    ('lasso', Lasso(random_state=2020)),\n",
    "    ('kneiba', KNeighborsRegressor(n_neighbors=6)),\n",
    "    ('cat', CatBoostRegressor(logging_level='Silent', random_state=2020))\n",
    "]\n",
    "\n",
    "predictions_2 = StackingRegressor(estimators=estimators_2, final_estimator=CatBoostRegressor(logging_level='Silent', depth=6, bagging_temperature=5, random_state=2020)).fit(X_train, y_train).predict(tes)\n",
    "\n",
    "predictions_cat_1 = CatBoostRegressor(logging_level='Silent', depth=6, bagging_temperature=5, random_state=2020).fit(X_train, y_train).predict(tes)\n",
    "\n",
    "\n",
    "# Further averaging, blending and retraining to generalise well\n",
    "# While the rations are greater than one, it still works a treat. This is definitely one of the parameters to tune to achieve great results.\n",
    "stack = [x*0.56 + y*0.51 for x, y in zip(predictions_1, predictions_2)]\n",
    "stack_2 = [x*0.56 + y*0.51 for x, y in zip(stack, predictions_cat_1)]\n",
    "\n",
    "X,y = tes.copy(), stack_2\n",
    "preds_ridge = Ridge(random_state=2020).fit(X, y).predict(X)\n",
    "\n",
    "# We added a new feature to the test dataset, where we clustered the wards to 150 clusters, then used Catboost's encoder to encode the clusters.\n",
    "\n",
    "X['cluster'] = KMeans(150, random_state=2020).fit(X).predict(X)\n",
    "preds_cat = CatBoostRegressor(random_state=2020, verbose = False, depth=6, bagging_temperature=5, cat_features=['cluster']).fit(X, y).predict(X)\n",
    "\n",
    "# blended the Ridge and Catboost predictions.\n",
    "final_blend_2 = [x*0.2 +y*0.8 for x, y in zip(preds_ridge, preds_cat)]\n",
    "\n",
    "# Clipping the values from between 0 - 90 was also important as we know that the target variable is between 0 to 100.\n",
    "final_blend_2 = np.clip(final_blend_2, a_min=0, a_max=90)\n",
    "\n",
    "# Applying regularization to the final blend by substracting a constant from the predictions and clipping again.\n",
    "exp = final_blend_2 - 0.48\n",
    "exp = np.clip(exp, a_min=0, a_max=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2arAKrR-vl6r"
   },
   "source": [
    "## Retraining #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YiEMhoJOvl6s"
   },
   "outputs": [],
   "source": [
    "# Retraining on the test data by using the prediction of the stacked regressors as our target.\n",
    "# We also added the clusters but had to manually mean encode the clusters to the target variable as LinearRegression cannot encode categorical variables.\n",
    "X = tes.copy()\n",
    "\n",
    "X['cluster'] = KMeans(150, random_state=2020).fit(X).predict(X)\n",
    "X['target'] = exp\n",
    "X['encoded'] = X['cluster'].map(X.groupby('cluster')['target'].mean())\n",
    "y=X.target\n",
    "X=X.drop(['cluster', 'target'], 1)\n",
    "preds_1 = CatBoostRegressor(verbose = False, random_state=2020).fit(X,y).predict(X)*0.7 + LinearRegression().fit(X, y).predict(X)*0.3\n",
    "preds_2 = CatBoostRegressor(verbose = False, random_state=2020).fit(X,y).predict(X)*0.5 + LinearRegression().fit(X, y).predict(X)*0.5\n",
    "preds_3 = CatBoostRegressor(verbose = False, random_state=2020).fit(X,y).predict(X)*0.6 + LinearRegression().fit(X, y).predict(X)*0.4\n",
    "\n",
    "final = [x*0.3 + y*0.3 + z*0.4 for x, y, z in zip(preds_1, preds_2, preds_3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qQh_7r8nvl6u"
   },
   "source": [
    "## Retraining #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I_ra9JQDvl6v"
   },
   "outputs": [],
   "source": [
    "# Retraining again this time using Regularized Greedy Forests and Catboost.\n",
    "X['final'] = final\n",
    "y = X.final\n",
    "X = X.drop('final', 1)\n",
    "preds_1 = CatBoostRegressor(verbose = False, random_state=2020).fit(X,y).predict(X)*0.7 + RGFRegressor().fit(X, y).predict(X)*0.3\n",
    "preds_2 = CatBoostRegressor(verbose = False, random_state=2020).fit(X,y).predict(X)*0.5 + RGFRegressor().fit(X, y).predict(X)*0.5\n",
    "preds_3 = CatBoostRegressor(verbose = False, random_state=2020).fit(X,y).predict(X)*0.6 + RGFRegressor().fit(X, y).predict(X)*0.4\n",
    "\n",
    "final2 = [x*0.3 + y*0.3 + z*0.4 for x, y, z in zip(preds_1, preds_2, preds_3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OHsQlO8qvl6y"
   },
   "outputs": [],
   "source": [
    "# Clipping for the final time and creating the submission file.\n",
    "final2 = np.clip(final2, a_min=0, a_max=90)\n",
    "sub_df = pd.DataFrame({'ward': test.ward, 'target_pct_vunerable': final2-0.2})\n",
    "sub_df.to_csv('finally.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1zgB1jn_mRTl"
   },
   "source": [
    "### Caveat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QJe7LBkUmTcj"
   },
   "source": [
    "We faced a problem of reproducibility as the score was changing with each submission with no change in code. However, that was solved by setting the *random_state* parameter of all models that have it to the same value. Now, the solution provides a consistently similar score each time it's rerun.\n",
    "\n",
    "\n",
    "However, the solution has a better private Leader board score of *3.50354986128398* which is better than the score we uploaded in time which was *3.52760301028188*"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "final_solution_main.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
