{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyshp==2.1.0 (from -r requirement.txt (line 1))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/16/3bf15aa864fb77845fab8007eda22c2bd67bd6c1fd13496df452c8c43621/pyshp-2.1.0.tar.gz (215kB)\n",
      "\u001b[K     |████████████████████████████████| 225kB 853kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting keras (from -r requirement.txt (line 2))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/fd/6bfe87920d7f4fd475acd28500a42482b6b84479832bdc0fe9e589a60ceb/Keras-2.3.1-py2.py3-none-any.whl (377kB)\n",
      "\u001b[K     |████████████████████████████████| 378kB 978kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: shapely in /home/philmassie/snap/jupyter/common/lib/python3.7/site-packages (from -r requirement.txt (line 3)) (1.6.4.post2)\n",
      "Collecting opencv-python (from -r requirement.txt (line 4))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/34/a3/403dbaef909fee9f9f6a8eaff51d44085a14e5bb1a1ff7257117d744986a/opencv_python-4.2.0.32-cp37-cp37m-manylinux1_x86_64.whl (28.2MB)\n",
      "\u001b[K     |████████████████████████████████| 28.2MB 1.4MB/s eta 0:00:01    |███▉                            | 3.4MB 647kB/s eta 0:00:39     |████▌                           | 4.0MB 647kB/s eta 0:00:38\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /home/philmassie/snap/jupyter/common/lib/python3.7/site-packages (from -r requirement.txt (line 5)) (4.40.0)\n",
      "Collecting ray[tune] (from -r requirement.txt (line 7))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0e/65/ed3bc6bd5aa85a734badb4001e261bc069c8e27dbb5c958c0ba422229732/ray-0.8.2-cp37-cp37m-manylinux1_x86_64.whl (19.1MB)\n",
      "\u001b[K     |████████████████████████████████| 19.1MB 1.5MB/s eta 0:00:01     |████████████████████            | 11.9MB 1.4MB/s eta 0:00:06\n",
      "\u001b[?25hRequirement already satisfied: seaborn in /home/philmassie/snap/jupyter/common/lib/python3.7/site-packages (from -r requirement.txt (line 8)) (0.9.0)\n",
      "Requirement already satisfied: lightgbm in /home/philmassie/snap/jupyter/common/lib/python3.7/site-packages (from -r requirement.txt (line 9)) (2.3.1)\n",
      "Collecting xgboost==1.0.0 (from -r requirement.txt (line 10))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c1/e8/88d7afd859a52092e86e0f1d9fa5ed7718c73ee5ef68a71d0123019a197e/xgboost-1.0.0-py3-none-manylinux1_x86_64.whl (109.7MB)\n",
      "\u001b[K     |████████████████████████████████| 109.8MB 151kB/s eta 0:00:01   |▎                               | 1.0MB 1.3MB/s eta 0:01:26     |▋                               | 2.3MB 638kB/s eta 0:02:49     |█▏                              | 3.9MB 1.2MB/s eta 0:01:32     |██████▍                         | 21.9MB 465kB/s eta 0:03:09     |████████▌                       | 29.0MB 1.3MB/s eta 0:01:05     |█████████▋                      | 32.9MB 978kB/s eta 0:01:19     |██████████▉                     | 37.2MB 641kB/s eta 0:01:54     |███████████▌                    | 39.4MB 573kB/s eta 0:02:03     |███████████▋                    | 40.0MB 548kB/s eta 0:02:08     |████████████████████▋           | 70.7MB 467kB/s eta 0:01:24     |██████████████████████▍         | 76.9MB 1.7MB/s eta 0:00:20     |███████████████████████████▌    | 94.2MB 1.1MB/s eta 0:00:14     |██████████████████████████████▏ | 103.5MB 114kB/s eta 0:00:55     |██████████████████████████████▌ | 104.6MB 580kB/s eta 0:00:09\n",
      "\u001b[?25hRequirement already satisfied: catboost in /home/philmassie/snap/jupyter/common/lib/python3.7/site-packages (from -r requirement.txt (line 11)) (0.16.5)\n",
      "Collecting autofeat (from -r requirement.txt (line 12))\n",
      "  Downloading https://files.pythonhosted.org/packages/1b/c3/125f1acf2b9c0d8bbb8807478621a7543a7d059668e55c78f9c2d21f62cc/autofeat-1.1.2-py3-none-any.whl\n",
      "Collecting vincenty (from -r requirement.txt (line 13))\n",
      "  Downloading https://files.pythonhosted.org/packages/c4/de/296372fde237fdda627fb6127a34689a3c1b25b6531a180060bf8e11457f/vincenty-0.1.4.tar.gz\n",
      "Collecting tpot (from -r requirement.txt (line 14))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/9f/813faf5ec7aa95f393a07603abd01fcb925b65ffe95441b25da029a69ff7/TPOT-0.11.1-py3-none-any.whl (75kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 682kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting keras-preprocessing>=1.0.5 (from keras->-r requirement.txt (line 2))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/6a/8c1f62c37212d9fc441a7e26736df51ce6f0e38455816445471f10da4f0a/Keras_Preprocessing-1.1.0-py2.py3-none-any.whl (41kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 1.5MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.9.1 in /home/philmassie/snap/jupyter/common/lib/python3.7/site-packages (from keras->-r requirement.txt (line 2)) (1.17.4)\n",
      "Collecting pyyaml (from keras->-r requirement.txt (line 2))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3d/d9/ea9816aea31beeadccd03f1f8b625ecf8f645bd66744484d162d84803ce5/PyYAML-5.3.tar.gz (268kB)\n",
      "\u001b[K     |████████████████████████████████| 276kB 995kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.9.0 in /snap/jupyter/6/lib/python3.7/site-packages (from keras->-r requirement.txt (line 2)) (1.12.0)\n",
      "Collecting keras-applications>=1.0.6 (from keras->-r requirement.txt (line 2))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 903kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting h5py (from keras->-r requirement.txt (line 2))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/c0/abde58b837e066bca19a3f7332d9d0493521d7dd6b48248451a9e3fe2214/h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9MB 1.9MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy>=0.14 in /home/philmassie/snap/jupyter/common/lib/python3.7/site-packages (from keras->-r requirement.txt (line 2)) (1.3.3)\n",
      "Collecting funcsigs (from ray[tune]->-r requirement.txt (line 7))\n",
      "  Downloading https://files.pythonhosted.org/packages/69/cb/f5be453359271714c01b9bd06126eaf2e368f1fddfff30818754b5ac2328/funcsigs-1.0.2-py2.py3-none-any.whl\n",
      "Requirement already satisfied: jsonschema in /snap/jupyter/6/lib/python3.7/site-packages (from ray[tune]->-r requirement.txt (line 7)) (3.0.1)\n",
      "Collecting aiohttp (from ray[tune]->-r requirement.txt (line 7))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/71/6000eacb8923d9fd07aa8784a8fab4f022ae697f3c2456d7dca75c743dd6/aiohttp-3.6.2-cp37-cp37m-manylinux1_x86_64.whl (1.2MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2MB 1.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google (from ray[tune]->-r requirement.txt (line 7))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/81/51/36af1d18648574d13d8f43e863e95a97fe6f43d54a13fbcf272c638c10e9/google-2.0.3-py2.py3-none-any.whl (45kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 2.4MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting redis>=3.3.2 (from ray[tune]->-r requirement.txt (line 7))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f0/05/1fc7feedc19c123e7a95cfc9e7892eb6cdd2e5df4e9e8af6384349c1cc3d/redis-3.4.1-py2.py3-none-any.whl (71kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 1.5MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting colorama (from ray[tune]->-r requirement.txt (line 7))\n",
      "  Downloading https://files.pythonhosted.org/packages/c9/dc/45cdef1b4d119eb96316b3117e6d5708a08029992b2fee2c143c7a0a5cc5/colorama-0.4.3-py2.py3-none-any.whl\n",
      "Collecting packaging (from ray[tune]->-r requirement.txt (line 7))\n",
      "  Downloading https://files.pythonhosted.org/packages/98/42/87c585dd3b113c775e65fd6b8d9d0a43abe1819c471d7af702d4e01e9b20/packaging-20.1-py2.py3-none-any.whl\n",
      "Collecting cloudpickle (from ray[tune]->-r requirement.txt (line 7))\n",
      "  Downloading https://files.pythonhosted.org/packages/ea/0b/189cd3c19faf362ff2df5f301456c6cf8571ef6684644cfdfdbff293825c/cloudpickle-1.3.0-py2.py3-none-any.whl\n",
      "Collecting filelock (from ray[tune]->-r requirement.txt (line 7))\n",
      "  Downloading https://files.pythonhosted.org/packages/93/83/71a2ee6158bb9f39a90c0dea1637f81d5eef866e188e1971a1b1ab01a35a/filelock-3.0.12-py3-none-any.whl\n",
      "Collecting grpcio (from ray[tune]->-r requirement.txt (line 7))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fb/00/997acb1c16f84e6d3ede511500f54e7b7a1fdfe7b4da5a5c2d07ad4e91f1/grpcio-1.27.2-cp37-cp37m-manylinux2010_x86_64.whl (2.7MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 2.7MB 1.8MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: click in /home/philmassie/snap/jupyter/common/lib/python3.7/site-packages (from ray[tune]->-r requirement.txt (line 7)) (7.0)\n",
      "Collecting protobuf>=3.8.0 (from ray[tune]->-r requirement.txt (line 7))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/f1/8dcd4219bbae8aa44fe8871a89f05eca2dca9c04f8dbfed8a82b7be97a88/protobuf-3.11.3-cp37-cp37m-manylinux1_x86_64.whl (1.3MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3MB 820kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pytest (from ray[tune]->-r requirement.txt (line 7))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/c0/34033b2df7718b91c667bd259d5ce632ec3720198b7068c0ba6f6104ff89/pytest-5.3.5-py3-none-any.whl (235kB)\n",
      "\u001b[K     |████████████████████████████████| 235kB 2.2MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting py-spy>=0.2.0 (from ray[tune]->-r requirement.txt (line 7))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8e/a7/ab45c9ee3c4654edda3efbd6b8e2fa4962226718a7e3e3be6e3926bf3617/py_spy-0.3.3-py2.py3-none-manylinux1_x86_64.whl (2.9MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9MB 652kB/s eta 0:00:01     |████████████████████████████▉   | 2.6MB 652kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboardX; extra == \"tune\" (from ray[tune]->-r requirement.txt (line 7))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/35/f1/5843425495765c8c2dd0784a851a93ef204d314fc87bcc2bbb9f662a3ad1/tensorboardX-2.0-py2.py3-none-any.whl (195kB)\n",
      "\u001b[K     |████████████████████████████████| 204kB 1.7MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tabulate; extra == \"tune\" in /home/philmassie/snap/jupyter/common/lib/python3.7/site-packages (from ray[tune]->-r requirement.txt (line 7)) (0.8.6)\n",
      "Requirement already satisfied: pandas>=0.15.2 in /home/philmassie/snap/jupyter/common/lib/python3.7/site-packages (from seaborn->-r requirement.txt (line 8)) (0.25.3)\n",
      "Requirement already satisfied: matplotlib>=1.4.3 in /home/philmassie/snap/jupyter/common/lib/python3.7/site-packages (from seaborn->-r requirement.txt (line 8)) (3.1.2)\n",
      "Requirement already satisfied: scikit-learn in /home/philmassie/snap/jupyter/common/lib/python3.7/site-packages (from lightgbm->-r requirement.txt (line 9)) (0.22)\n",
      "Requirement already satisfied: graphviz in /home/philmassie/snap/jupyter/common/lib/python3.7/site-packages (from catboost->-r requirement.txt (line 11)) (0.13.2)\n",
      "Requirement already satisfied: plotly in /home/philmassie/snap/jupyter/common/lib/python3.7/site-packages (from catboost->-r requirement.txt (line 11)) (4.4.1)\n",
      "Collecting future (from autofeat->-r requirement.txt (line 12))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/0b/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9/future-0.18.2.tar.gz (829kB)\n",
      "\u001b[K     |████████████████████████████████| 829kB 846kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: joblib in /home/philmassie/snap/jupyter/common/lib/python3.7/site-packages (from autofeat->-r requirement.txt (line 12)) (0.14.0)\n",
      "Collecting pint (from autofeat->-r requirement.txt (line 12))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9d/db/7a2204b03c22069839958df5723eb2718d50c33052e0da84c9a83de14ea4/Pint-0.11-py2.py3-none-any.whl (186kB)\n",
      "\u001b[K     |████████████████████████████████| 194kB 345kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sympy (from autofeat->-r requirement.txt (line 12))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ce/5b/acc12e3c0d0be685601fc2b2d20ed18dc0bf461380e763afc9d0a548deb0/sympy-1.5.1-py2.py3-none-any.whl (5.6MB)\n",
      "\u001b[K     |████████████████████████████████| 5.6MB 1.9MB/s eta 0:00:01     |██████▊                         | 1.2MB 678kB/s eta 0:00:07     |█████████████████████████████▎  | 5.1MB 1.9MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting update-checker>=0.16 (from tpot->-r requirement.txt (line 14))\n",
      "  Downloading https://files.pythonhosted.org/packages/17/c9/ab11855af164d03be0ff4fddd4c46a5bd44799a9ecc1770e01a669c21168/update_checker-0.16-py2.py3-none-any.whl\n",
      "Collecting stopit>=1.1.1 (from tpot->-r requirement.txt (line 14))\n",
      "  Downloading https://files.pythonhosted.org/packages/35/58/e8bb0b0fb05baf07bbac1450c447d753da65f9701f551dca79823ce15d50/stopit-1.1.2.tar.gz\n",
      "Collecting deap>=1.2 (from tpot->-r requirement.txt (line 14))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/d1/803c7a387d8a7e6866160b1541307f88d534da4291572fb32f69d2548afb/deap-1.3.1-cp37-cp37m-manylinux2010_x86_64.whl (157kB)\n",
      "\u001b[K     |████████████████████████████████| 163kB 1.1MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: attrs>=17.4.0 in /snap/jupyter/6/lib/python3.7/site-packages (from jsonschema->ray[tune]->-r requirement.txt (line 7)) (19.1.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /snap/jupyter/6/lib/python3.7/site-packages (from jsonschema->ray[tune]->-r requirement.txt (line 7)) (0.15.2)\n",
      "Requirement already satisfied: setuptools in /snap/jupyter/6/lib/python3.7/site-packages (from jsonschema->ray[tune]->-r requirement.txt (line 7)) (41.0.1)\n",
      "Requirement already satisfied: chardet<4.0,>=2.0 in /home/philmassie/snap/jupyter/common/lib/python3.7/site-packages (from aiohttp->ray[tune]->-r requirement.txt (line 7)) (3.0.4)\n",
      "Collecting async-timeout<4.0,>=3.0 (from aiohttp->ray[tune]->-r requirement.txt (line 7))\n",
      "  Downloading https://files.pythonhosted.org/packages/e1/1e/5a4441be21b0726c4464f3f23c8b19628372f606755a9d2e46c187e65ec4/async_timeout-3.0.1-py3-none-any.whl\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->ray[tune]->-r requirement.txt (line 7))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/70/54/7cb8eabe7dea1b441a7487ae3a1adb319f2d2e44a062a669f730a24dc474/yarl-1.4.2-cp37-cp37m-manylinux1_x86_64.whl (256kB)\n",
      "\u001b[K     |████████████████████████████████| 266kB 536kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting multidict<5.0,>=4.5 (from aiohttp->ray[tune]->-r requirement.txt (line 7))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/3f/e53dea9face4115abfb796f398bf74747a0d294537b12935d1bbf697c11a/multidict-4.7.5-cp37-cp37m-manylinux1_x86_64.whl (149kB)\n",
      "\u001b[K     |████████████████████████████████| 153kB 641kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting beautifulsoup4 (from google->ray[tune]->-r requirement.txt (line 7))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cb/a1/c698cf319e9cfed6b17376281bd0efc6bfc8465698f54170ef60a485ab5d/beautifulsoup4-4.8.2-py3-none-any.whl (106kB)\n",
      "\u001b[K     |████████████████████████████████| 112kB 2.6MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /home/philmassie/snap/jupyter/common/lib/python3.7/site-packages (from packaging->ray[tune]->-r requirement.txt (line 7)) (2.4.5)\n",
      "Collecting pluggy<1.0,>=0.12 (from pytest->ray[tune]->-r requirement.txt (line 7))\n",
      "  Downloading https://files.pythonhosted.org/packages/a0/28/85c7aa31b80d150b772fbe4a229487bc6644da9ccb7e427dd8cc60cb8a62/pluggy-0.13.1-py2.py3-none-any.whl\n",
      "Collecting py>=1.5.0 (from pytest->ray[tune]->-r requirement.txt (line 7))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/8d/21e1767c009211a62a8e3067280bfce76e89c9f876180308515942304d2d/py-1.8.1-py2.py3-none-any.whl (83kB)\n",
      "\u001b[K     |████████████████████████████████| 92kB 2.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting importlib-metadata>=0.12; python_version < \"3.8\" (from pytest->ray[tune]->-r requirement.txt (line 7))\n",
      "  Downloading https://files.pythonhosted.org/packages/8b/03/a00d504808808912751e64ccf414be53c29cad620e3de2421135fcae3025/importlib_metadata-1.5.0-py2.py3-none-any.whl\n",
      "Collecting more-itertools>=4.0.0 (from pytest->ray[tune]->-r requirement.txt (line 7))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/72/96/4297306cc270eef1e3461da034a3bebe7c84eff052326b130824e98fc3fb/more_itertools-8.2.0-py3-none-any.whl (43kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 2.3MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wcwidth in /snap/jupyter/6/lib/python3.7/site-packages (from pytest->ray[tune]->-r requirement.txt (line 7)) (0.1.7)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/philmassie/snap/jupyter/common/lib/python3.7/site-packages (from pandas>=0.15.2->seaborn->-r requirement.txt (line 8)) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /snap/jupyter/6/lib/python3.7/site-packages (from pandas>=0.15.2->seaborn->-r requirement.txt (line 8)) (2.8.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/philmassie/snap/jupyter/common/lib/python3.7/site-packages (from matplotlib>=1.4.3->seaborn->-r requirement.txt (line 8)) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/philmassie/snap/jupyter/common/lib/python3.7/site-packages (from matplotlib>=1.4.3->seaborn->-r requirement.txt (line 8)) (1.1.0)\n",
      "Requirement already satisfied: retrying>=1.3.3 in /home/philmassie/snap/jupyter/common/lib/python3.7/site-packages (from plotly->catboost->-r requirement.txt (line 11)) (1.3.3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mpmath>=0.19 (from sympy->autofeat->-r requirement.txt (line 12))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/63/3384ebb3b51af9610086b23ea976e6d27d6d97bf140a76a365bd77a3eb32/mpmath-1.1.0.tar.gz (512kB)\n",
      "\u001b[K     |████████████████████████████████| 522kB 1.6MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.3.0 in /home/philmassie/snap/jupyter/common/lib/python3.7/site-packages (from update-checker>=0.16->tpot->-r requirement.txt (line 14)) (2.22.0)\n",
      "Requirement already satisfied: idna>=2.0 in /home/philmassie/snap/jupyter/common/lib/python3.7/site-packages (from yarl<2.0,>=1.0->aiohttp->ray[tune]->-r requirement.txt (line 7)) (2.8)\n",
      "Collecting soupsieve>=1.2 (from beautifulsoup4->google->ray[tune]->-r requirement.txt (line 7))\n",
      "  Downloading https://files.pythonhosted.org/packages/05/cf/ea245e52f55823f19992447b008bcbb7f78efc5960d77f6c34b5b45b36dd/soupsieve-2.0-py2.py3-none-any.whl\n",
      "Collecting zipp>=0.5 (from importlib-metadata>=0.12; python_version < \"3.8\"->pytest->ray[tune]->-r requirement.txt (line 7))\n",
      "  Downloading https://files.pythonhosted.org/packages/b2/34/bfcb43cc0ba81f527bc4f40ef41ba2ff4080e047acb0586b56b3d017ace4/zipp-3.1.0-py3-none-any.whl\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/philmassie/snap/jupyter/common/lib/python3.7/site-packages (from requests>=2.3.0->update-checker>=0.16->tpot->-r requirement.txt (line 14)) (1.25.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /snap/jupyter/6/lib/python3.7/site-packages (from requests>=2.3.0->update-checker>=0.16->tpot->-r requirement.txt (line 14)) (2019.3.9)\n",
      "Building wheels for collected packages: pyshp, vincenty, pyyaml, future, stopit, mpmath\n",
      "  Building wheel for pyshp (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/philmassie/snap/jupyter/6/.cache/pip/wheels/a6/0c/de/321b5192ad416b328975a2f0385f72c64db4656501eba7cc1a\n",
      "  Building wheel for vincenty (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/philmassie/snap/jupyter/6/.cache/pip/wheels/51/33/73/9b9898fb2becf6a3b3082fc863daeade08ccd8900ea9c947c4\n",
      "  Building wheel for pyyaml (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/philmassie/snap/jupyter/6/.cache/pip/wheels/e4/76/4d/a95b8dd7b452b69e8ed4f68b69e1b55e12c9c9624dd962b191\n",
      "  Building wheel for future (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/philmassie/snap/jupyter/6/.cache/pip/wheels/8b/99/a0/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\n",
      "  Building wheel for stopit (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/philmassie/snap/jupyter/6/.cache/pip/wheels/3c/85/2b/2580190404636bfc63e8de3dff629c03bb795021e1983a6cc7\n",
      "  Building wheel for mpmath (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/philmassie/snap/jupyter/6/.cache/pip/wheels/63/9d/8e/37c3f6506ed3f152733a699e92d8e0c9f5e5f01dea262f80ad\n",
      "Successfully built pyshp vincenty pyyaml future stopit mpmath\n",
      "Installing collected packages: pyshp, keras-preprocessing, pyyaml, h5py, keras-applications, keras, opencv-python, funcsigs, async-timeout, multidict, yarl, aiohttp, soupsieve, beautifulsoup4, google, redis, colorama, packaging, cloudpickle, filelock, grpcio, protobuf, zipp, importlib-metadata, pluggy, py, more-itertools, pytest, py-spy, tensorboardX, ray, xgboost, future, pint, mpmath, sympy, autofeat, vincenty, update-checker, stopit, deap, tpot\n",
      "  Found existing installation: xgboost 0.90\n",
      "    Uninstalling xgboost-0.90:\n",
      "      Successfully uninstalled xgboost-0.90\n",
      "Successfully installed aiohttp-3.6.2 async-timeout-3.0.1 autofeat-1.1.2 beautifulsoup4-4.8.2 cloudpickle-1.3.0 colorama-0.4.3 deap-1.3.1 filelock-3.0.12 funcsigs-1.0.2 future-0.18.2 google-2.0.3 grpcio-1.27.2 h5py-2.10.0 importlib-metadata-1.5.0 keras-2.3.1 keras-applications-1.0.8 keras-preprocessing-1.1.0 more-itertools-8.2.0 mpmath-1.1.0 multidict-4.7.5 opencv-python-4.2.0.32 packaging-20.1 pint-0.11 pluggy-0.13.1 protobuf-3.11.3 py-1.8.1 py-spy-0.3.3 pyshp-2.1.0 pytest-5.3.5 pyyaml-5.3 ray-0.8.2 redis-3.4.1 soupsieve-2.0 stopit-1.1.2 sympy-1.5.1 tensorboardX-2.0 tpot-0.11.1 update-checker-0.16 vincenty-0.1.4 xgboost-1.0.0 yarl-1.4.2 zipp-3.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirement.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import os \n",
    "import numpy as np \n",
    "from sklearn.cluster import KMeans\n",
    "from shapely.geometry import Polygon, Point\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import  mean_squared_error\n",
    "from sklearn.decomposition import PCA\n",
    "from os.path import join\n",
    "import catboost as cat\n",
    "import shapefile as shp\n",
    "from tqdm import tqdm\n",
    "from vincenty import vincenty\n",
    "import requests\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_shapefile(path, num):\n",
    "    sh = shp.Reader(join(path, 'zaf_admbnda_adm{}_2016SADB_OCHA.shp'.format(num)))\n",
    "    print(len(sh.shapes()))\n",
    "\n",
    "    shapeRecs = sh.shapeRecords()\n",
    "    all_shapes = sh.shapes()\n",
    "    data_ = []\n",
    "    shapes = []\n",
    "    for i in range(len(sh.shapes())-1):\n",
    "        data_.append(shapeRecs[i].record)\n",
    "        shapes.append(all_shapes[i].points)\n",
    "    df = pd.DataFrame.from_records(data_)\n",
    "    fields = sh.fields\n",
    "    df.columns =[f[0] for f in fields[1:]]\n",
    "    df['shape_adm{}'.format(num)] = shapes\n",
    "    df['shape_adm{}'.format(num)] = df['shape_adm{}'.format(num)].apply(lambda x:Polygon(x))\n",
    "    df['area_adm{}'.format(num)] = df['shape_adm{}'.format(num)].apply(lambda x:x.area)\n",
    "    df['centroid_lon{}'.format(num)] = df['shape_adm{}'.format(num)].apply(lambda x:(x.centroid).coords[0][0])\n",
    "    df['centroid_lat{}'.format(num)] = df['shape_adm{}'.format(num)].apply(lambda x:(x.centroid).coords[0][1])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv(\"./Train.csv\")\n",
    "test=pd.read_csv(\"./Test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/snap/jupyter/6/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "data=pd.concat([train,test],ignore_index=True)\n",
    "max_1=data[\"total_households\"].max()\n",
    "max_2=data[\"total_individuals\"].max()\n",
    "\n",
    "data[\"total_households\"]/=max_1\n",
    "data[\"total_individuals\"]/=max_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kmean clusters\n",
    "data_copy=data.copy()\n",
    "columns=data_copy.drop([\"ward\",\"ADM4_PCODE\",\"target\"],1).columns\n",
    "data_copy=data_copy[columns]\n",
    "km=KMeans(10,random_state=1992)\n",
    "km=km.fit(data_copy[columns])\n",
    "# predict the cluster of each area\n",
    "data[\"cluster\"]=km.predict(data_copy[columns]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce dimentionality for dwelling features\n",
    "pca = PCA()\n",
    "dwelling_features =  data.filter(regex='dw_.*')\n",
    "df_pca = pca.fit_transform(dwelling_features)\n",
    "\n",
    "data['pca_dw_0'] = df_pca[:,0]\n",
    "data['pca_dw_1'] = df_pca[:,1]\n",
    "# reduce dimentionality for language features\n",
    "pca = PCA()\n",
    "lan_features =  data.filter(regex='lan_.*')\n",
    "df_pca = pca.fit_transform(lan_features)\n",
    "\n",
    "data['pca_lan_0'] = df_pca[:,0]\n",
    "data['pca_lan_1'] = df_pca[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the household size\n",
    "data['Household_Size'] = (data['total_individuals']/data['total_households'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the most frequent dwelling type\n",
    "arank = data.filter(regex='dw_.*').apply(np.argsort, axis=1)\n",
    "ranked_lang = pd.DataFrame(data.filter(regex='dw_.*').columns.to_series()[arank.values[:,::-1][:,1:4]])\n",
    "data['dwelling_type_1'] = ranked_lang[0]\n",
    "data['dwelling_type_2'] = ranked_lang[1]\n",
    "data['dwelling_type_3'] = ranked_lang[2]\n",
    "\n",
    "\n",
    "data.loc[data.dwelling_type_1.isin(['dw_07','dw_08']), 'dwelling_type_1'] = 'shack'\n",
    "data.loc[data.dwelling_type_2.isin(['dw_07','dw_08']), 'dwelling_type_2'] = 'shack'\n",
    "data.loc[data.dwelling_type_3.isin(['dw_07','dw_08']), 'dwelling_type_3'] = 'shack'\n",
    "\n",
    "\n",
    "luxury_dwelling_type = ['dw_02', 'dw_03', 'dw_04', 'dw_05', 'dw_06', 'dw_09']\n",
    "data.loc[data.dwelling_type_1.isin(luxury_dwelling_type), 'dwelling_type_1'] = 'luxury'\n",
    "data.loc[data.dwelling_type_2.isin(luxury_dwelling_type), 'dwelling_type_2'] = 'luxury'\n",
    "data.loc[data.dwelling_type_3.isin(luxury_dwelling_type), 'dwelling_type_3'] = 'luxury'\n",
    "data['dw_luxury'] = data[luxury_dwelling_type[0:6]].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disposal of luxury items\n",
    "luxury_stuff = ['psa_01','car_01','stv_00']\n",
    "not_luxury_stuff = ['psa_00','car_00','stv_01']\n",
    "data['luxury_stuff'] = data[luxury_stuff].sum(axis=1)\n",
    "data['not_luxury_stuff'] = data[not_luxury_stuff].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#water access\n",
    "data['water_access'] = data.filter(regex='pw_.*').idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 75/3835 [07:09<6:20:30,  6.07s/it]"
     ]
    }
   ],
   "source": [
    "# get the top points of interests(POIs) of each ADM in specific categories such as 'Facilities', 'Education Facility', etc\n",
    "\n",
    "BASE_URL = \"https://places.sit.ls.hereapi.com/places/v1/\"\n",
    "NEARBY_URL = BASE_URL + \"discover/here?at={},{}&apiKey={}\"\n",
    "SUGGEST_URL = BASE_URL + \"/autosuggest?in={},{};r=300000&q={}'&apiKey={}\"\n",
    "\n",
    "categories = ['hospital or health care facility','Facilities', 'Education Facility','Public Transport',\\\n",
    " 'Government or Community Facility']\n",
    "\n",
    "list_dicts = []\n",
    "for i in tqdm(range(data.shape[0])):\n",
    "    for cat in categories:\n",
    "        resp = requests.get(url=SUGGEST_URL.format(data.loc[i,'lat'], data.loc[i,'lon'], cat,\"h5lCzAFIoFjRPp94SKO1uTIjjcIkaR3h_uWduXUvcRI\" ))\n",
    "        try:\n",
    "            resp = resp.json()\n",
    "            resp['id'] = i\n",
    "            resp['suggested_type'] = cat\n",
    "            list_dicts.append(resp)\n",
    "        except: \n",
    "            print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate mean distance to each category type\n",
    "for list_ in tqdm(list_dicts): \n",
    "    id_ = list_['id']\n",
    "    try:\n",
    "        results = list_['results']\n",
    "        cat = results[0]['category']\n",
    "        data.loc[id_, 'nb_establishments_type_{}'.format(cat)] = len(results)\n",
    "        distances = []\n",
    "        for i in range(len(results)):\n",
    "            if 'distance' in results[i].keys():\n",
    "                distances.append(results[i]['distance'])\n",
    "        data.loc[id_, 'mean_distance_to_establishments_type_{}'.format(cat)] = np.mean(distances)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adm4 = read_shapefile('./zaf_adm_2016SADB_OCHA_SHP/',4)\n",
    "df_adm3 = read_shapefile('./zaf_adm_2016SADB_OCHA_SHP/',3)\n",
    "df_adm2 = read_shapefile('./zaf_adm_2016SADB_OCHA_SHP/',2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adm4 = df_adm4.merge(df_adm3[['ADM3_PCODE', 'area_adm3','centroid_lon3', 'centroid_lat3']], on=['ADM3_PCODE'], how='left')\n",
    "df_adm4 = df_adm4.merge(df_adm2[['ADM2_PCODE', 'area_adm2','centroid_lon2', 'centroid_lat2']], on=['ADM2_PCODE'], how='left')\n",
    "df = data.copy()\n",
    "df = df.merge(df_adm4.filter(regex='(area.*)|(ADM4_PCODE)|(centroid.*)'), on=['ADM4_PCODE'], how='left')\n",
    "assert(df.shape[0]==data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get ID of ADM4_PCODE\n",
    "data[\"ADM4_PCODE\"]=data[\"ADM4_PCODE\"].str[2:].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distance to center of each ADM\n",
    "data[\"distance_to_center\"] = data.apply(\n",
    "    lambda x: vincenty((x[\"lat\"], x[\"lon\"]), (x[\"centroid_lat3\"], x[\"centroid_lon3\"]),'kilometers'), axis = 1)\n",
    "\n",
    "data[\"distance_to_center2\"] = data.apply(\n",
    "    lambda x: vincenty((x[\"lat\"], x[\"lon\"]), (x[\"centroid_lat2\"], x[\"centroid_lon2\"]),'kilometers'), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"total_households\"]*=max_1\n",
    "data[\"total_individuals\"]*=max_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=data[~data.target.isna()]\n",
    "test=data[data.target.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./submissions/'):\n",
    "    os.mkdir('./submissions/')\n",
    "i = len(os.listdir('./submissions'))+1\n",
    "def train_func(train,test,target_name,features,params,num_class,submission_name='sub_'.format(i)):\n",
    "    oof_train = np.zeros((len(train),num_class))\n",
    "    oof_test  = np.zeros((len(test),num_class))\n",
    "    importances=[]\n",
    "    cv_scores = []\n",
    "    train_scores=[]\n",
    "    cat_features = []\n",
    "    dtest=cat.Pool(data=test[features], cat_features=cat_features, feature_names=features)\n",
    "    \n",
    "    for ind, (trn_ind, val_ind) in ( enumerate(kfolds.split(train,train[target_name])) ):\n",
    "\n",
    "        print('fold_n',ind+1, 'started at', time.ctime())\n",
    "        X_train, X_valid = train.loc[trn_ind][features], train.loc[val_ind][features]\n",
    "        y_train, y_valid = train.loc[trn_ind][target_name], train.loc[val_ind][target_name]\n",
    "        dtrain=cat.Pool(data=X_train[features],label=y_train,cat_features=cat_features,\n",
    "                        feature_names=features)\n",
    "        dval=cat.Pool(data=X_valid[features],label=y_valid,cat_features=cat_features,\n",
    "                        feature_names=features)\n",
    " \n",
    "        model = cat.CatBoost(params)\n",
    "    \n",
    "        model.fit(dtrain, eval_set=dval, use_best_model=True)\n",
    "\n",
    "        val_pred = model.predict(dval)\n",
    "        train_pred = model.predict(dtrain,thread_count=4)\n",
    "        test_pred = model.predict(dtest,thread_count=4)\n",
    "\n",
    "              \n",
    "        oof_train[val_ind,:] += np.reshape(val_pred,(-1,num_class))\n",
    "        oof_test += np.reshape(test_pred,(-1,num_class))\n",
    "        score_fold_validation=np.sqrt(mean_squared_error(y_valid, val_pred))\n",
    "        score_fold_train=np.sqrt(mean_squared_error(y_train, train_pred))\n",
    "\n",
    "        train_scores.append(score_fold_train)\n",
    "        cv_scores.append(score_fold_validation)\n",
    "        print('-Train Score: {} - CV Score : {}'.format(score_fold_train, score_fold_validation))\n",
    "    \n",
    "    end_train_score=np.mean(train_scores)\n",
    "    train_scores.append(end_train_score)\n",
    "    \n",
    "    oof_score=np.sqrt(mean_squared_error(train[target_name], oof_train))\n",
    "    cv_scores.append(oof_score)\n",
    "    print(\"\\n\\ntraining is done : train score {} - oof Score {}\".format(str(end_train_score),str(oof_score)))\n",
    "    # create and save the submission\n",
    "    submission=test[[\"ward\"]]\n",
    "    submission[\"target\"]=oof_test/nfolds\n",
    "    # check if the submission folder exists. If not, create it.\n",
    "    submission.to_csv(\"./submissions/{}\".format(submission_name),index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.reset_index(drop=True,inplace=True)\n",
    "num_boost_round=10000000\n",
    "nfolds = 5\n",
    "kfolds = KFold(n_splits=nfolds, shuffle=True, random_state=15468)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['ADM4_PCODE', 'car_00', 'car_01', 'dw_01', 'dw_03', 'dw_04', 'dw_05', 'dw_07', 'dw_08', 'dw_09', 'dw_10', 'dw_11', 'lan_00', 'lan_01', 'lan_06', 'lan_09', 'lan_10', 'lan_11', 'lan_12', 'lan_14', 'lat', 'lgt_00', 'lln_00', 'lln_01', 'lon', 'pg_00', 'pg_01', 'pg_02', 'pg_03', 'pg_04', 'psa_00', 'psa_01', 'psa_03', 'psa_04', 'pw_00', 'pw_02', 'pw_03', 'pw_04', 'pw_05', 'pw_06', 'stv_00', 'stv_01', 'total_households', 'total_individuals', 'luxury_stuff', 'not_luxury_stuff', 'cluster','area_adm4']\n",
    "print(len(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model and predictions\n",
    "params = {\n",
    "    \"learning_rate\": 0.055,\n",
    "    \"random_seed\": 0,\n",
    "    \n",
    "    \"thread_count\": -1,\n",
    "    \"iterations\": 10000000,\n",
    "    \n",
    "    \"loss_function\": \"RMSE\",\n",
    "    \"eval_metric\": \"RMSE\",\n",
    "        \n",
    "    \"l2_leaf_reg\": 3,\n",
    "    \"bagging_temperature\": 1,  \n",
    "    \n",
    "    \"depth\": 4,\n",
    "    \"od_type\": \"Iter\",\n",
    "    \"od_wait\": 55,\n",
    "\n",
    "    \"verbose_eval\": False,\n",
    "    \"use_best_model\": True,\n",
    "}\n",
    "train_func(train,test,\"target\",features,params=params,num_class=1, submission_name='catboost_surface.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
