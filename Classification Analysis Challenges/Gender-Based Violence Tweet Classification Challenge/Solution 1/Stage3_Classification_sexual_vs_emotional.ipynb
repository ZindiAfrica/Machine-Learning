{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"Stage3_Classification_sexual_vs_emotional.ipynb","provenance":[]}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tnXgn9RddrEs","executionInfo":{"status":"ok","timestamp":1629109057857,"user_tz":-180,"elapsed":4794,"user":{"displayName":"Cedric Manouan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnI55k9QnJV4mtok5wRGE36cZcj1DZIjihnSx6=s64","userId":"00242929915047742214"}},"outputId":"246b015a-c875-4003-af7c-4458f31a948e"},"source":["!pip install emoji"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting emoji\n","  Downloading emoji-1.4.2.tar.gz (184 kB)\n","\u001b[?25l\r\u001b[K     |█▊                              | 10 kB 26.7 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 20 kB 34.3 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 30 kB 23.7 MB/s eta 0:00:01\r\u001b[K     |███████                         | 40 kB 18.4 MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 51 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 61 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 71 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 81 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 92 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 102 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 112 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 122 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 133 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 143 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 153 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 163 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 174 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 184 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 184 kB 7.6 MB/s \n","\u001b[?25hBuilding wheels for collected packages: emoji\n","  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for emoji: filename=emoji-1.4.2-py3-none-any.whl size=186469 sha256=1c10e4f5e0c75cc0b38c68fc791204d46cfb12057978366679bd1e04fec7229e\n","  Stored in directory: /root/.cache/pip/wheels/e4/61/e7/2fc1ac8f306848fc66c6c013ab511f0a39ef4b1825b11363b2\n","Successfully built emoji\n","Installing collected packages: emoji\n","Successfully installed emoji-1.4.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FigkYI28dhT9","executionInfo":{"status":"ok","timestamp":1629109057858,"user_tz":-180,"elapsed":11,"user":{"displayName":"Cedric Manouan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnI55k9QnJV4mtok5wRGE36cZcj1DZIjihnSx6=s64","userId":"00242929915047742214"}},"outputId":"1c9877db-fe71-42f4-ef2b-b77d79b8fbb4"},"source":["import pandas as pd\n","import numpy as np\n","import re\n","\n","import nltk\n","nltk.download('punkt')\n","from nltk.tokenize import TreebankWordTokenizer\n","from collections import Counter\n","from nltk.tokenize import word_tokenize\n","from nltk.probability import FreqDist\n","import emoji\n","import spacy\n","from operator import itemgetter\n","import itertools\n","from itertools import combinations\n","\n","\n","from nltk.stem.snowball import SnowballStemmer\n","\n","\n","from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n","from tqdm import tqdm\n","tqdm.pandas()\n"],"execution_count":3,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Pis9QKBFdzP8","executionInfo":{"status":"ok","timestamp":1629109106517,"user_tz":-180,"elapsed":346,"user":{"displayName":"Cedric Manouan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnI55k9QnJV4mtok5wRGE36cZcj1DZIjihnSx6=s64","userId":"00242929915047742214"}},"outputId":"260496b3-fdbc-4ea9-bde8-d961d0c586ec"},"source":["from google.colab import drive\n","drive.mount(\"/content/gdrive\")"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"zEJYuVvRdhUB"},"source":["## Stopwords and spacy model\n","* We extract a list of stopwords from - https://gist.github.com/sebleier/554280 because the available list of NLTK/Spacy stopwords are highly insufficient and incomplete.\n","* We extract the Spacy's english pipeline optimized for CPU and will be using it for stemming and lemmatization tasks\n","* We also instantiate SnowballStemmer to perform stemming after lemmatization is done in later stages of the notebook"]},{"cell_type":"code","metadata":{"id":"f4UA9MLpdhUB","executionInfo":{"status":"ok","timestamp":1629109110385,"user_tz":-180,"elapsed":1292,"user":{"displayName":"Cedric Manouan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnI55k9QnJV4mtok5wRGE36cZcj1DZIjihnSx6=s64","userId":"00242929915047742214"}}},"source":["#### Stopwords list taken from - https://gist.github.com/sebleier/554280 (reason - the available list of NLTK/ Spacy stopwords are too less and don't cover all possible words)\n","stopwords_list = [\"0o\", \"0s\", \"3a\", \"3b\", \"3d\", \"6b\", \"6o\", \"a\", \"a1\", \"a2\", \"a3\", \"a4\", \"ab\", \"able\", \"about\", \"above\", \"abst\", \"ac\", \"accordance\", \"according\", \"accordingly\", \"across\", \"act\", \"actually\", \"ad\", \"added\", \"adj\", \"ae\", \"af\", \"affected\", \"affecting\", \"affects\", \"after\", \"afterwards\", \"ag\", \"again\", \"against\", \"ah\", \"ain\", \"ain't\", \"aj\", \"al\", \"all\", \"allow\", \"allows\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\", \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"announce\", \"another\", \"any\", \"anybody\", \"anyhow\", \"anymore\", \"anyone\", \"anything\", \"anyway\", \"anyways\", \"anywhere\", \"ao\", \"ap\", \"apart\", \"apparently\", \"appear\", \"appreciate\", \"appropriate\", \"approximately\", \"ar\", \"are\", \"aren\", \"arent\", \"aren't\", \"arise\", \"around\", \"as\", \"a's\", \"aside\", \"ask\", \"asking\", \"associated\", \"at\", \"au\", \"auth\", \"av\", \"available\", \"aw\", \"away\", \"awfully\", \"ax\", \"ay\", \"az\", \"b\", \"b1\", \"b2\", \"b3\", \"ba\", \"back\", \"bc\", \"bd\", \"be\", \"became\", \"because\", \"become\", \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"begin\", \"beginning\", \"beginnings\", \"begins\", \"behind\", \"being\", \"believe\", \"below\", \"beside\", \"besides\", \"best\", \"better\", \"between\", \"beyond\", \"bi\", \"bill\", \"biol\", \"bj\", \"bk\", \"bl\", \"bn\", \"both\", \"bottom\", \"bp\", \"br\", \"brief\", \"briefly\", \"bs\", \"bt\", \"bu\", \"but\", \"bx\", \"by\", \"c\", \"c1\", \"c2\", \"c3\", \"ca\", \"call\", \"came\", \"can\", \"cannot\", \"cant\", \"can't\", \"cause\", \"causes\", \"cc\", \"cd\", \"ce\", \"certain\", \"certainly\", \"cf\", \"cg\", \"ch\", \"changes\", \"ci\", \"cit\", \"cj\", \"cl\", \"clearly\", \"cm\", \"c'mon\", \"cn\", \"co\", \"com\", \"come\", \"comes\", \"con\", \"concerning\", \"consequently\", \"consider\", \"considering\", \"contain\", \"containing\", \"contains\", \"corresponding\", \"could\", \"couldn\", \"couldnt\", \"couldn't\", \"course\", \"cp\", \"cq\", \"cr\", \"cry\", \"cs\", \"c's\", \"ct\", \"cu\", \"currently\", \"cv\", \"cx\", \"cy\", \"cz\", \"d\", \"d2\", \"da\", \"date\", \"dc\", \"dd\", \"de\", \"definitely\", \"describe\", \"described\", \"despite\", \"detail\", \"df\", \"di\", \"did\", \"didn\", \"didn't\", \"different\", \"dj\", \"dk\", \"dl\", \"do\", \"does\", \"doesn\", \"doesn't\", \"doing\", \"don\", \"done\", \"don't\", \"down\", \"downwards\", \"dp\", \"dr\", \"ds\", \"dt\", \"du\", \"due\", \"during\", \"dx\", \"dy\", \"e\", \"e2\", \"e3\", \"ea\", \"each\", \"ec\", \"ed\", \"edu\", \"ee\", \"ef\", \"effect\", \"eg\", \"ei\", \"eight\", \"eighty\", \"either\", \"ej\", \"el\", \"eleven\", \"else\", \"elsewhere\", \"em\", \"empty\", \"en\", \"end\", \"ending\", \"enough\", \"entirely\", \"eo\", \"ep\", \"eq\", \"er\", \"es\", \"especially\", \"est\", \"et\", \"et-al\", \"etc\", \"eu\", \"ev\", \"even\", \"ever\", \"every\", \"everybody\", \"everyone\", \"everything\", \"everywhere\", \"ex\", \"exactly\", \"example\", \"except\", \"ey\", \"f\", \"f2\", \"fa\", \"far\", \"fc\", \"few\", \"ff\", \"fi\", \"fifteen\", \"fifth\", \"fify\", \"fill\", \"find\", \"fire\", \"first\", \"five\", \"fix\", \"fj\", \"fl\", \"fn\", \"fo\", \"followed\", \"following\", \"follows\", \"for\", \"former\", \"formerly\", \"forth\", \"forty\", \"found\", \"four\", \"fr\", \"from\", \"front\", \"fs\", \"ft\", \"fu\", \"full\", \"further\", \"furthermore\", \"fy\", \"g\", \"ga\", \"gave\", \"ge\", \"get\", \"gets\", \"getting\", \"gi\", \"give\", \"given\", \"gives\", \"giving\", \"gj\", \"gl\", \"go\", \"goes\", \"going\", \"gone\", \"got\", \"gotten\", \"gr\", \"greetings\", \"gs\", \"gy\", \"h\", \"h2\", \"h3\", \"had\", \"hadn\", \"hadn't\", \"happens\", \"hardly\", \"has\", \"hasn\", \"hasnt\", \"hasn't\", \"have\", \"haven\", \"haven't\", \"having\", \"he\", \"hed\", \"he'd\", \"he'll\", \"hello\", \"help\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"heres\", \"here's\", \"hereupon\", \"hers\", \"herself\", \"hes\", \"he's\", \"hh\", \"hi\", \"hid\", \"him\", \"himself\", \"his\", \"hither\", \"hj\", \"ho\", \"home\", \"hopefully\", \"how\", \"howbeit\", \"however\", \"how's\", \"hr\", \"hs\", \"http\", \"hu\", \"hundred\", \"hy\", \"i\", \"i2\", \"i3\", \"i4\", \"i6\", \"i7\", \"i8\", \"ia\", \"ib\", \"ibid\", \"ic\", \"id\", \"i'd\", \"ie\", \"if\", \"ig\", \"ignored\", \"ih\", \"ii\", \"ij\", \"il\", \"i'll\", \"im\", \"i'm\", \"immediate\", \"immediately\", \"importance\", \"important\", \"in\", \"inasmuch\", \"inc\", \"indeed\", \"index\", \"indicate\", \"indicated\", \"indicates\", \"information\", \"inner\", \"insofar\", \"instead\", \"interest\", \"into\", \"invention\", \"inward\", \"io\", \"ip\", \"iq\", \"ir\", \"is\", \"isn\", \"isn't\", \"it\", \"itd\", \"it'd\", \"it'll\", \"its\", \"it's\", \"itself\", \"iv\", \"i've\", \"ix\", \"iy\", \"iz\", \"j\", \"jj\", \"jr\", \"js\", \"jt\", \"ju\", \"just\", \"k\", \"ke\", \"keep\", \"keeps\", \"kept\", \"kg\", \"kj\", \"km\", \"know\", \"known\", \"knows\", \"ko\", \"l\", \"l2\", \"la\", \"largely\", \"last\", \"lately\", \"later\", \"latter\", \"latterly\", \"lb\", \"lc\", \"le\", \"least\", \"les\", \"less\", \"lest\", \"let\", \"lets\", \"let's\", \"lf\", \"like\", \"liked\", \"likely\", \"line\", \"little\", \"lj\", \"ll\", \"ll\", \"ln\", \"lo\", \"look\", \"looking\", \"looks\", \"los\", \"lr\", \"ls\", \"lt\", \"ltd\", \"m\", \"m2\", \"ma\", \"made\", \"mainly\", \"make\", \"makes\", \"many\", \"may\", \"maybe\", \"me\", \"mean\", \"means\", \"meantime\", \"meanwhile\", \"merely\", \"mg\", \"might\", \"mightn\", \"mightn't\", \"mill\", \"million\", \"mine\", \"miss\", \"ml\", \"mn\", \"mo\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"mr\", \"mrs\", \"ms\", \"mt\", \"mu\", \"much\", \"mug\", \"must\", \"mustn\", \"mustn't\", \"my\", \"myself\", \"n\", \"n2\", \"na\", \"name\", \"namely\", \"nay\", \"nc\", \"nd\", \"ne\", \"near\", \"nearly\", \"necessarily\", \"necessary\", \"need\", \"needn\", \"needn't\", \"needs\", \"neither\", \"never\", \"nevertheless\", \"new\", \"next\", \"ng\", \"ni\", \"nine\", \"ninety\", \"nj\", \"nl\", \"nn\", \"no\", \"nobody\", \"non\", \"none\", \"nonetheless\", \"noone\", \"nor\", \"normally\", \"nos\", \"not\", \"noted\", \"nothing\", \"novel\", \"now\", \"nowhere\", \"nr\", \"ns\", \"nt\", \"ny\", \"o\", \"oa\", \"ob\", \"obtain\", \"obtained\", \"obviously\", \"oc\", \"od\", \"of\", \"off\", \"often\", \"og\", \"oh\", \"oi\", \"oj\", \"ok\", \"okay\", \"ol\", \"old\", \"om\", \"omitted\", \"on\", \"once\", \"one\", \"ones\", \"only\", \"onto\", \"oo\", \"op\", \"oq\", \"or\", \"ord\", \"os\", \"ot\", \"other\", \"others\", \"otherwise\", \"ou\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"outside\", \"over\", \"overall\", \"ow\", \"owing\", \"own\", \"ox\", \"oz\", \"p\", \"p1\", \"p2\", \"p3\", \"page\", \"pagecount\", \"pages\", \"par\", \"part\", \"particular\", \"particularly\", \"pas\", \"past\", \"pc\", \"pd\", \"pe\", \"per\", \"perhaps\", \"pf\", \"ph\", \"pi\", \"pj\", \"pk\", \"pl\", \"placed\", \"please\", \"plus\", \"pm\", \"pn\", \"po\", \"poorly\", \"possible\", \"possibly\", \"potentially\", \"pp\", \"pq\", \"pr\", \"predominantly\", \"present\", \"presumably\", \"previously\", \"primarily\", \"probably\", \"promptly\", \"proud\", \"provides\", \"ps\", \"pt\", \"pu\", \"put\", \"py\", \"q\", \"qj\", \"qu\", \"que\", \"quickly\", \"quite\", \"qv\", \"r\", \"r2\", \"ra\", \"ran\", \"rather\", \"rc\", \"rd\", \"re\", \"readily\", \"really\", \"reasonably\", \"recent\", \"recently\", \"ref\", \"refs\", \"regarding\", \"regardless\", \"regards\", \"related\", \"relatively\", \"research\", \"research-articl\", \"respectively\", \"resulted\", \"resulting\", \"results\", \"rf\", \"rh\", \"ri\", \"right\", \"rj\", \"rl\", \"rm\", \"rn\", \"ro\", \"rq\", \"rr\", \"rs\", \"rt\", \"ru\", \"run\", \"rv\", \"ry\", \"s\", \"s2\", \"sa\", \"said\", \"same\", \"saw\", \"say\", \"saying\", \"says\", \"sc\", \"sd\", \"se\", \"sec\", \"second\", \"secondly\", \"section\", \"see\", \"seeing\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"seen\", \"self\", \"selves\", \"sensible\", \"sent\", \"serious\", \"seriously\", \"seven\", \"several\", \"sf\", \"shall\", \"shan\", \"shan't\", \"she\", \"shed\", \"she'd\", \"she'll\", \"shes\", \"she's\", \"should\", \"shouldn\", \"shouldn't\", \"should've\", \"show\", \"showed\", \"shown\", \"showns\", \"shows\", \"si\", \"side\", \"significant\", \"significantly\", \"similar\", \"similarly\", \"since\", \"sincere\", \"six\", \"sixty\", \"sj\", \"sl\", \"slightly\", \"sm\", \"sn\", \"so\", \"some\", \"somebody\", \"somehow\", \"someone\", \"somethan\", \"something\", \"sometime\", \"sometimes\", \"somewhat\", \"somewhere\", \"soon\", \"sorry\", \"sp\", \"specifically\", \"specified\", \"specify\", \"specifying\", \"sq\", \"sr\", \"ss\", \"st\", \"still\", \"stop\", \"strongly\", \"sub\", \"substantially\", \"successfully\", \"such\", \"sufficiently\", \"suggest\", \"sup\", \"sure\", \"sy\", \"system\", \"sz\", \"t\", \"t1\", \"t2\", \"t3\", \"take\", \"taken\", \"taking\", \"tb\", \"tc\", \"td\", \"te\", \"tell\", \"ten\", \"tends\", \"tf\", \"th\", \"than\", \"thank\", \"thanks\", \"thanx\", \"that\", \"that'll\", \"thats\", \"that's\", \"that've\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"thered\", \"therefore\", \"therein\", \"there'll\", \"thereof\", \"therere\", \"theres\", \"there's\", \"thereto\", \"thereupon\", \"there've\", \"these\", \"they\", \"theyd\", \"they'd\", \"they'll\", \"theyre\", \"they're\", \"they've\", \"thickv\", \"thin\", \"think\", \"third\", \"this\", \"thorough\", \"thoroughly\", \"those\", \"thou\", \"though\", \"thoughh\", \"thousand\", \"three\", \"throug\", \"through\", \"throughout\", \"thru\", \"thus\", \"ti\", \"til\", \"tip\", \"tj\", \"tl\", \"tm\", \"tn\", \"to\", \"together\", \"too\", \"took\", \"top\", \"toward\", \"towards\", \"tp\", \"tq\", \"tr\", \"tried\", \"tries\", \"truly\", \"try\", \"trying\", \"ts\", \"t's\", \"tt\", \"tv\", \"twelve\", \"twenty\", \"twice\", \"two\", \"tx\", \"u\", \"u201d\", \"ue\", \"ui\", \"uj\", \"uk\", \"um\", \"un\", \"under\", \"unfortunately\", \"unless\", \"unlike\", \"unlikely\", \"until\", \"unto\", \"uo\", \"up\", \"upon\", \"ups\", \"ur\", \"us\", \"use\", \"used\", \"useful\", \"usefully\", \"usefulness\", \"uses\", \"using\", \"usually\", \"ut\", \"v\", \"va\", \"value\", \"various\", \"vd\", \"ve\", \"ve\", \"very\", \"via\", \"viz\", \"vj\", \"vo\", \"vol\", \"vols\", \"volumtype\", \"vq\", \"vs\", \"vt\", \"vu\", \"w\", \"wa\", \"want\", \"wants\", \"was\", \"wasn\", \"wasnt\", \"wasn't\", \"way\", \"we\", \"wed\", \"we'd\", \"welcome\", \"well\", \"we'll\", \"well-b\", \"went\", \"were\", \"we're\", \"weren\", \"werent\", \"weren't\", \"we've\", \"what\", \"whatever\", \"what'll\", \"whats\", \"what's\", \"when\", \"whence\", \"whenever\", \"when's\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"wheres\", \"where's\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whim\", \"whither\", \"who\", \"whod\", \"whoever\", \"whole\", \"who'll\", \"whom\", \"whomever\", \"whos\", \"who's\", \"whose\", \"why\", \"why's\", \"wi\", \"widely\", \"will\", \"willing\", \"wish\", \"with\", \"within\", \"without\", \"wo\", \"won\", \"wonder\", \"wont\", \"won't\", \"words\", \"world\", \"would\", \"wouldn\", \"wouldnt\", \"wouldn't\", \"www\", \"x\", \"x1\", \"x2\", \"x3\", \"xf\", \"xi\", \"xj\", \"xk\", \"xl\", \"xn\", \"xo\", \"xs\", \"xt\", \"xv\", \"xx\", \"y\", \"y2\", \"yes\", \"yet\", \"yj\", \"yl\", \"you\", \"youd\", \"you'd\", \"you'll\", \"your\", \"youre\", \"you're\", \"yours\", \"yourself\", \"yourselves\", \"you've\", \"yr\", \"ys\", \"yt\", \"z\", \"zero\", \"zi\", \"zz\",]\n","\n","nlp        = spacy.load(\"en_core_web_sm\")\n","\n","stemmer = SnowballStemmer(language='english')"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z5PTF9zcdhUG"},"source":["## Custom preprocessing functions for Vocabulary extraction\n","We declare basic functions to perform tasks like removal of stopwords, stemming, lemmatization in batches, removal and emojis etc. "]},{"cell_type":"code","metadata":{"id":"viKJkyyDdhUH","executionInfo":{"status":"ok","timestamp":1629109110389,"user_tz":-180,"elapsed":6,"user":{"displayName":"Cedric Manouan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnI55k9QnJV4mtok5wRGE36cZcj1DZIjihnSx6=s64","userId":"00242929915047742214"}}},"source":["def remove_stopwords(text):\n","    '''\n","      Removes stopwords from the text\n","    '''\n","\n","    text_split   = text.split()\n","\n","    text_list    = [word for word in text_split if not word in stopwords_list]\n","\n","    return ' '.join(text_list)\n","\n","def stemming_perform(text):\n","    '''\n","      Performs lemmatization in batches\n","    '''\n","    \n","    token = text.split()\n","    stemmed_text = []\n","    for tok in token:\n","        stemmed_text.append(stemmer.stem(tok))\n","    return ' '.join(stemmed_text)\n","\n","\n","def cleaning_text(text):\n","    '''\n","      Operations performed:- \n","      1. Converting the entire text to lowercase\n","      2. Removal of punctuations from the text\n","      3. Removal of numbers from the text\n","    '''\n","\n","    text = text.lower()\n","\n","    text = re.sub(r'[^\\w\\s]', ' ', text)\n","    text = re.sub(r'\\d+', '', text)\n","    text = remove_stopwords(text)\n","\n","    text = text.replace('amp','') ## this was actually ampersand which kept coming in frequently that made no sense\n","    return text\n","\n","\n","def give_emoji_free_text(text):\n","    ''' \n","      Removes all possible emojis from the text (because our text is basically tweets that can have emoijs).\n","      The input is a text and the output is emoji free text\n","    '''\n","    allchars = [str for str in text.encode().decode('utf-8')]\n","    emoji_list = [c for c in allchars if c in emoji.UNICODE_EMOJI]\n","    clean_text = ' '.join([str for str in text.encode().decode('utf-8').split() if not any(i in str for i in emoji_list)])\n","    return clean_text\n","\n","\n","def preprocess_custom(text):\n","    text = give_emoji_free_text(text)\n","    text = cleaning_text(text)\n","\n","    return text\n","\n","def lemmatize_batches(docs):\n","    '''\n","      Performs lemmatization in batches\n","    '''\n","    lemmatized_text = []\n","\n","    for doc in nlp.pipe(docs, batch_size=256, n_process=3,disable=[\"parser\", \"ner\"]):\n","        lemmatized_text.append(str(' '.join([token.lemma_ for token in doc])))\n","    return lemmatized_text\n","\n","\n","def lemmatize_df(df):\n","    lemmatized = []\n","    for i in tqdm(range(0,df.shape[0],1000)):\n","        z = lemmatize_batches(df['preprocessed_text'].iloc[i:i+1000])\n","        lemmatized.extend(z)\n","    df['lemmatize'] = lemmatized\n","    return df\n","\n","\n","\n","def return_frequency_dict_sorted(df):\n","    sent = df['stemmed'].values\n","    sent = ' '.join(sent)\n","\n","    fdist = FreqDist()\n","    \n","    for word in word_tokenize(sent):\n","        fdist[word.lower()] += 1\n","        \n","    return sorted(fdist.items(), key=lambda x: x[1], reverse=True)\n","\n","def replace(prob,x2,thresh1,thresh2):\n","    \n","    if str(prob)!='nan':\n","        if ((prob>thresh1) and (prob<thresh2)):\n","            return 'sexual_violence'\n","        else:\n","            return x2\n","    else:\n","        return x2\n"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U_58Dai7dhUH"},"source":["## Vocab extraction and Preprocessing Stage\n","We will preprocess the text in two stages\n","1. Here we will consider only the 'Harmful_traditional practice' class and find the vocabulary corresponding to it\n","2. Next we will consider the class 'sexual_violence' and find the vocabulary corresponding to it.\n","3. Out combined vocabulary will be the vocabulary from both the stages"]},{"cell_type":"markdown","metadata":{"id":"Q2BdVb8tdhUH"},"source":["####  Our input data will be the Data from previous stage"]},{"cell_type":"markdown","metadata":{"id":"n9WiOq7ldhUI"},"source":["## Text Preprocessing for Vocab extraction for class 'emotional_violence'\n","We preprocess the test data tweets and then find the frequency occurance of the preprocessed words from the dataset. We will now select top 20 words from the test data tweets which will be used as a vocabulary for building our model. We consider only the class 'emotional_violence' \n","<br>\n","__Reason for selecting vocabulary from only 'emotional_violence' -__ <br>\n","In train data, the class - 'emotional_violence' is very small compared to the other class 'sexual_violence'. If we take only the relevant keywords from 'sexual_violence', it will be tough to capture the distribution of 'emotional_violence' from the train data that is needed which will result in overfitting (with high accuracies while training) and the poor performance on test data and eventually a lot of misclassification due to bias. To give proper representation to the class - 'emotional_violence', we consider only the vocabulary of 'emotional_violence' and see how the same vocabulary performs in the class sexual_violence"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"usFjG9bEdhUI","executionInfo":{"status":"ok","timestamp":1629109148147,"user_tz":-180,"elapsed":3584,"user":{"displayName":"Cedric Manouan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnI55k9QnJV4mtok5wRGE36cZcj1DZIjihnSx6=s64","userId":"00242929915047742214"}},"outputId":"b51019b5-8b4d-43bf-eabf-9b0294426ea3"},"source":["subdf   = pd.read_csv('/content/gdrive/MyDrive/Hackathon/outcome_stage2.csv')\n","\n","subdf   = subdf[(subdf['type']=='emotional_violence')]\n","\n","print(f'The shape of subsetted data with subsetting on emotional_violence will be {subdf.shape}')\n","\n","test_df = pd.read_csv('/content/gdrive/MyDrive/Hackathon/Test.csv')\n","\n","temp_df = subdf.merge(test_df,on=['Tweet_ID'],how='left')\n","\n","_       = temp_df.rename(columns={'type':'type1'},inplace=True)\n","\n","print(f'The shape of data after assigning the labels attained from BERT to test is {temp_df.shape}')\n","\n","temp_df['preprocessed_text'] = temp_df['tweet'].progress_apply(preprocess_custom)\n","temp_df                      = lemmatize_df(temp_df)\n","temp_df['stemmed']           = temp_df['lemmatize'].progress_apply(stemming_perform)\n","sorted_freq_dict             = return_frequency_dict_sorted(temp_df)\n","\n","threshold2consider           = 10\n","vocab_emo                    = [word[0] for word in sorted_freq_dict[:threshold2consider]]\n"],"execution_count":13,"outputs":[{"output_type":"stream","text":["The shape of subsetted data with subsetting on emotional_violence will be (659, 2)\n","The shape of data after assigning the labels attained from BERT to test is (659, 3)\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 659/659 [00:00<00:00, 1770.65it/s]\n","100%|██████████| 1/1 [00:01<00:00,  1.59s/it]\n","100%|██████████| 659/659 [00:00<00:00, 4781.38it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":376},"id":"K9i_KSJkdhUJ","executionInfo":{"status":"ok","timestamp":1629109152434,"user_tz":-180,"elapsed":333,"user":{"displayName":"Cedric Manouan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnI55k9QnJV4mtok5wRGE36cZcj1DZIjihnSx6=s64","userId":"00242929915047742214"}},"outputId":"c10ebb2d-0ef4-4e35-a6fb-d8fc948dde05"},"source":["print('The top 10 words (preprocessed) based on the frequency distribution and their relative frequencies are given below')\n","pd.DataFrame(sorted_freq_dict,columns = ['word','Frequency']).head(10)"],"execution_count":14,"outputs":[{"output_type":"stream","text":["The top 10 words (preprocessed) based on the frequency distribution and their relative frequencies are given below\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>word</th>\n","      <th>Frequency</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>public</td>\n","      <td>677</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>humili</td>\n","      <td>256</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>grab</td>\n","      <td>196</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>insult</td>\n","      <td>159</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>ass</td>\n","      <td>155</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>call</td>\n","      <td>120</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>stupid</td>\n","      <td>90</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>time</td>\n","      <td>67</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>man</td>\n","      <td>63</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>woman</td>\n","      <td>63</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     word  Frequency\n","0  public        677\n","1  humili        256\n","2    grab        196\n","3  insult        159\n","4     ass        155\n","5    call        120\n","6  stupid         90\n","7    time         67\n","8     man         63\n","9   woman         63"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"Ul001tQhdhUK"},"source":["# Classification model on the vocabulary attained\n","We will now preprocess our train and BERT Classified physical and sexual data to finetune it <br>\n","To perform that our test data was already preprocessed in the previous step. We will now preprocess our train dataset"]},{"cell_type":"markdown","metadata":{"id":"kQ1ZNynmdhUK"},"source":["## Text Preprocessing on Train data and test data\n","We subset our train data having only Emotional Violence and Sexual Violence labels and perform the exact same preprocessing steps we did for the test data above "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XoOaF-yLdhUK","executionInfo":{"status":"ok","timestamp":1629109265332,"user_tz":-180,"elapsed":86157,"user":{"displayName":"Cedric Manouan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnI55k9QnJV4mtok5wRGE36cZcj1DZIjihnSx6=s64","userId":"00242929915047742214"}},"outputId":"5482959e-25ff-45aa-e5ae-fbd5e07d25ec"},"source":["test_df                         = pd.read_csv('/content/gdrive/MyDrive/Hackathon/Test.csv')\n","subdf                           = pd.read_csv('/content/gdrive/MyDrive/Hackathon/outcome_stage2.csv')\n","\n","test_df                         = test_df.merge(subdf,on=['Tweet_ID'],how='left')\n","\n","\n","subdf                           = subdf[(subdf['type']=='sexual_violence') | (subdf['type']=='emotional_violence')]\n","\n","\n","temp_df                         = subdf.merge(test_df,on=['Tweet_ID'],how='left')\n","\n","_                               = temp_df.rename(columns={'type':'type1'},inplace=True)\n","\n","temp_df['preprocessed_text']    = temp_df['tweet'].progress_apply(preprocess_custom)\n","temp_df                         = lemmatize_df(temp_df)\n","temp_df['stemmed']              = temp_df['lemmatize'].progress_apply(stemming_perform)\n","\n","\n","train_df                        = pd.read_csv('/content/gdrive/MyDrive/Hackathon/Train.csv')\n","train_data                      = train_df[train_df['type'].isin(['emotional_violence','sexual_violence'])]\n","train_data                      = train_data.reset_index()\n","\n","train_data['preprocessed_text'] = train_data['tweet'].progress_apply(preprocess_custom)\n","train_data                      = lemmatize_df(train_data)\n","train_data['stemmed']           = train_data['lemmatize'].progress_apply(stemming_perform)\n","\n","print(test_df.shape,train_df.shape,temp_df.shape)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["100%|██████████| 8547/8547 [00:04<00:00, 1791.93it/s]\n","100%|██████████| 9/9 [00:11<00:00,  1.27s/it]\n","100%|██████████| 8547/8547 [00:01<00:00, 4563.49it/s]\n","100%|██████████| 33299/33299 [00:20<00:00, 1622.69it/s]\n","100%|██████████| 34/34 [00:39<00:00,  1.15s/it]\n","100%|██████████| 33299/33299 [00:06<00:00, 4860.40it/s]"],"name":"stderr"},{"output_type":"stream","text":["(15581, 3) (39650, 3) (8547, 7)\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"P5g9vTRQdhUK"},"source":["## Count vectorization with binarization\n","We apply count vectorizer on the train and test datasets "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-Zp7BWLNdhUL","executionInfo":{"status":"ok","timestamp":1629109265906,"user_tz":-180,"elapsed":586,"user":{"displayName":"Cedric Manouan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnI55k9QnJV4mtok5wRGE36cZcj1DZIjihnSx6=s64","userId":"00242929915047742214"}},"outputId":"a21c4fe0-b40b-4e46-fc71-e38a6109aa26"},"source":["train_X              = train_data['stemmed'].values\n","test_X               = temp_df['stemmed'].values\n","\n","cv                   = CountVectorizer(vocabulary = vocab_emo,binary=True)\n","\n","train                = cv.fit_transform(train_X)\n","test                 = cv.transform(test_X)\n","\n","#### Converting the sparse matrices to dense \n","train                = train.todense()\n","test                 = test.todense()\n","print(f'The shape of preprocessed train data matrix is {train.shape} and preprocessed test data matrix is {test.shape}')\n","\n","### label encoding done here\n","train_data['labels'] = train_data['type'].map({'sexual_violence':0,'emotional_violence':1})\n","\n","train_y              = train_data['labels'].values"],"execution_count":16,"outputs":[{"output_type":"stream","text":["The shape of preprocessed train data matrix is (33299, 10) and preprocessed test data matrix is (8547, 10)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"SdfH1BcGdhUL"},"source":["## Fitting a model on preprocessed data and finetuning\n","* We fit a XGBClassifier model on the preprocessed data \n","* We will now update the previous labels of sexual_violence and Physical_violence with the outcomes attained from the model \n","* The outcomes will be used for second stage classification"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XD5UOycGdhUL","executionInfo":{"status":"ok","timestamp":1629109265907,"user_tz":-180,"elapsed":7,"user":{"displayName":"Cedric Manouan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnI55k9QnJV4mtok5wRGE36cZcj1DZIjihnSx6=s64","userId":"00242929915047742214"}},"outputId":"31d43c20-d81f-4e9b-a716-287bbe9097c2"},"source":["train_data['labels'].value_counts()"],"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0    32648\n","1      651\n","Name: labels, dtype: int64"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"id":"LDfyKMZcdhUM","executionInfo":{"status":"ok","timestamp":1629109265907,"user_tz":-180,"elapsed":4,"user":{"displayName":"Cedric Manouan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnI55k9QnJV4mtok5wRGE36cZcj1DZIjihnSx6=s64","userId":"00242929915047742214"}}},"source":[""],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"DVuKkkJDdhUM","executionInfo":{"status":"ok","timestamp":1629109266426,"user_tz":-180,"elapsed":523,"user":{"displayName":"Cedric Manouan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnI55k9QnJV4mtok5wRGE36cZcj1DZIjihnSx6=s64","userId":"00242929915047742214"}}},"source":["from xgboost import XGBClassifier\n","model      = XGBClassifier(n_estimators =40,random_state=13,max_depth = 4,scale_pos_weight=np.sqrt(32648/651))\n","_          = model.fit(train,train_y)"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"VFGRsebkdhUM","executionInfo":{"status":"ok","timestamp":1629109266426,"user_tz":-180,"elapsed":10,"user":{"displayName":"Cedric Manouan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnI55k9QnJV4mtok5wRGE36cZcj1DZIjihnSx6=s64","userId":"00242929915047742214"}}},"source":[""],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"jrjErIocdhUM","executionInfo":{"status":"ok","timestamp":1629109266427,"user_tz":-180,"elapsed":9,"user":{"displayName":"Cedric Manouan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnI55k9QnJV4mtok5wRGE36cZcj1DZIjihnSx6=s64","userId":"00242929915047742214"}}},"source":["\n","temp_df['prob']  = model.predict_proba(test)[:,1]\n","\n","findf            = test_df.merge(temp_df,on = ['Tweet_ID','tweet'],how='left')\n"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gn73MJGxdhUM","executionInfo":{"status":"ok","timestamp":1629109266428,"user_tz":-180,"elapsed":10,"user":{"displayName":"Cedric Manouan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnI55k9QnJV4mtok5wRGE36cZcj1DZIjihnSx6=s64","userId":"00242929915047742214"}}},"source":["# findf['prob'].describe([0.05,0.1])"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SoXmUrbJdhUM","executionInfo":{"status":"ok","timestamp":1629109266915,"user_tz":-180,"elapsed":496,"user":{"displayName":"Cedric Manouan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnI55k9QnJV4mtok5wRGE36cZcj1DZIjihnSx6=s64","userId":"00242929915047742214"}},"outputId":"79dc2d4c-1be3-4d2b-e5de-0973b92120aa"},"source":["### We update the previous classes with the updated ones \n","findf['fintype'] = findf.apply(lambda z: replace(z['prob'],z['type'],0.45,0.7),axis=1)\n","findf['flag']    = (findf['type']!=findf['fintype']).astype(int)\n","\n","print(f'Ensuring the shape of outcome dataframe is maintained, we have shape of output data as - {findf.shape}')"],"execution_count":21,"outputs":[{"output_type":"stream","text":["Ensuring the shape of outcome dataframe is maintained, we have shape of output data as - (15581, 11)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tTznb3LCdhUN","executionInfo":{"status":"ok","timestamp":1629109266922,"user_tz":-180,"elapsed":25,"user":{"displayName":"Cedric Manouan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnI55k9QnJV4mtok5wRGE36cZcj1DZIjihnSx6=s64","userId":"00242929915047742214"}},"outputId":"2730f230-3bc6-475b-f7a3-f2dc364620c5"},"source":["findf['fintype'].value_counts()"],"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["sexual_violence                 8101\n","Physical_violence               3378\n","Harmful_Traditional_practice    3158\n","economic_violence                498\n","emotional_violence               446\n","Name: fintype, dtype: int64"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"markdown","metadata":{"id":"TfjrkxaVdhUN"},"source":["## Reviewing some modifications \n","We will see some mismatches done by BERT model that were modified in this stage of supervised classification based on machine learning "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"fDhRZP_WdhUN","executionInfo":{"status":"ok","timestamp":1629109266930,"user_tz":-180,"elapsed":27,"user":{"displayName":"Cedric Manouan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnI55k9QnJV4mtok5wRGE36cZcj1DZIjihnSx6=s64","userId":"00242929915047742214"}},"outputId":"06e082c0-2596-4fca-9962-81b376116392"},"source":["mismatch = findf[findf['flag']==1][['Tweet_ID','tweet','type','fintype','prob']]\n","mismatch = mismatch.sort_values(by = ['prob'])\n","mismatch.head(20)"],"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Tweet_ID</th>\n","      <th>tweet</th>\n","      <th>type</th>\n","      <th>fintype</th>\n","      <th>prob</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>15537</th>\n","      <td>ID_D21M6ENC</td>\n","      <td>Although my violence and my compulsive mutenes...</td>\n","      <td>emotional_violence</td>\n","      <td>sexual_violence</td>\n","      <td>0.495256</td>\n","    </tr>\n","    <tr>\n","      <th>11187</th>\n","      <td>ID_DGODHR4M</td>\n","      <td>You just called me stupid in an instance when ...</td>\n","      <td>emotional_violence</td>\n","      <td>sexual_violence</td>\n","      <td>0.495256</td>\n","    </tr>\n","    <tr>\n","      <th>4413</th>\n","      <td>ID_N6L58FEA</td>\n","      <td>I don't know why this guy keep bagging us He j...</td>\n","      <td>emotional_violence</td>\n","      <td>sexual_violence</td>\n","      <td>0.495256</td>\n","    </tr>\n","    <tr>\n","      <th>4410</th>\n","      <td>ID_6RFVV5AC</td>\n","      <td>the president has called handicap disabled peo...</td>\n","      <td>emotional_violence</td>\n","      <td>sexual_violence</td>\n","      <td>0.495256</td>\n","    </tr>\n","    <tr>\n","      <th>11429</th>\n","      <td>ID_70813QCS</td>\n","      <td>What will AnnCoulter's next book be called? \"I...</td>\n","      <td>emotional_violence</td>\n","      <td>sexual_violence</td>\n","      <td>0.495256</td>\n","    </tr>\n","    <tr>\n","      <th>11485</th>\n","      <td>ID_4IHCQ46S</td>\n","      <td>I just remembered my ex yelled at me and calle...</td>\n","      <td>emotional_violence</td>\n","      <td>sexual_violence</td>\n","      <td>0.495256</td>\n","    </tr>\n","    <tr>\n","      <th>3999</th>\n","      <td>ID_E6NX3DEN</td>\n","      <td>First of all he shouldn’t of touched me and he...</td>\n","      <td>emotional_violence</td>\n","      <td>sexual_violence</td>\n","      <td>0.495256</td>\n","    </tr>\n","    <tr>\n","      <th>3849</th>\n","      <td>ID_SNK3C7N6</td>\n","      <td>He used me to validate his poor life choices. ...</td>\n","      <td>emotional_violence</td>\n","      <td>sexual_violence</td>\n","      <td>0.495256</td>\n","    </tr>\n","    <tr>\n","      <th>4797</th>\n","      <td>ID_TDUK48Z5</td>\n","      <td>Even when my ex called me puddin' in public I ...</td>\n","      <td>emotional_violence</td>\n","      <td>sexual_violence</td>\n","      <td>0.495256</td>\n","    </tr>\n","    <tr>\n","      <th>3836</th>\n","      <td>ID_PUSA13F3</td>\n","      <td>today i was working backcash at mcdonalds &amp;amp...</td>\n","      <td>emotional_violence</td>\n","      <td>sexual_violence</td>\n","      <td>0.495256</td>\n","    </tr>\n","    <tr>\n","      <th>11958</th>\n","      <td>ID_D2FIDNG7</td>\n","      <td>BLACK PEOPLE DONALD CALLED YOU STUPID, TOO STU...</td>\n","      <td>emotional_violence</td>\n","      <td>sexual_violence</td>\n","      <td>0.495256</td>\n","    </tr>\n","    <tr>\n","      <th>3662</th>\n","      <td>ID_0IPY5PTP</td>\n","      <td>ON THE FLIP SIDE: used to say if I'd lash out,...</td>\n","      <td>emotional_violence</td>\n","      <td>sexual_violence</td>\n","      <td>0.495256</td>\n","    </tr>\n","    <tr>\n","      <th>12006</th>\n","      <td>ID_BZGZAP8D</td>\n","      <td>AND?! He shouldn't've called me a STUPID ASS B...</td>\n","      <td>emotional_violence</td>\n","      <td>sexual_violence</td>\n","      <td>0.495256</td>\n","    </tr>\n","    <tr>\n","      <th>12093</th>\n","      <td>ID_BN56QZWZ</td>\n","      <td>In the end without evidence i find this whole ...</td>\n","      <td>emotional_violence</td>\n","      <td>sexual_violence</td>\n","      <td>0.495256</td>\n","    </tr>\n","    <tr>\n","      <th>3314</th>\n","      <td>ID_N76HN2AM</td>\n","      <td>he (unintentionally according to him) called m...</td>\n","      <td>emotional_violence</td>\n","      <td>sexual_violence</td>\n","      <td>0.495256</td>\n","    </tr>\n","    <tr>\n","      <th>3176</th>\n","      <td>ID_HY9L4U4V</td>\n","      <td>Bro I was at a public pool and some guys calle...</td>\n","      <td>emotional_violence</td>\n","      <td>sexual_violence</td>\n","      <td>0.495256</td>\n","    </tr>\n","    <tr>\n","      <th>3126</th>\n","      <td>ID_PDFIHPHE</td>\n","      <td>I once told I guy to stop trying to feed me in...</td>\n","      <td>emotional_violence</td>\n","      <td>sexual_violence</td>\n","      <td>0.495256</td>\n","    </tr>\n","    <tr>\n","      <th>3833</th>\n","      <td>ID_7C1SNIQO</td>\n","      <td>Your way of thinking is beyond stupid bro. He ...</td>\n","      <td>emotional_violence</td>\n","      <td>sexual_violence</td>\n","      <td>0.495256</td>\n","    </tr>\n","    <tr>\n","      <th>5096</th>\n","      <td>ID_FXEUVDTW</td>\n","      <td>He is right about certain lyrics today about g...</td>\n","      <td>emotional_violence</td>\n","      <td>sexual_violence</td>\n","      <td>0.495256</td>\n","    </tr>\n","    <tr>\n","      <th>5145</th>\n","      <td>ID_2X4FPGG9</td>\n","      <td>Among those speaking in support during public ...</td>\n","      <td>emotional_violence</td>\n","      <td>sexual_violence</td>\n","      <td>0.495256</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["          Tweet_ID  ...      prob\n","15537  ID_D21M6ENC  ...  0.495256\n","11187  ID_DGODHR4M  ...  0.495256\n","4413   ID_N6L58FEA  ...  0.495256\n","4410   ID_6RFVV5AC  ...  0.495256\n","11429  ID_70813QCS  ...  0.495256\n","11485  ID_4IHCQ46S  ...  0.495256\n","3999   ID_E6NX3DEN  ...  0.495256\n","3849   ID_SNK3C7N6  ...  0.495256\n","4797   ID_TDUK48Z5  ...  0.495256\n","3836   ID_PUSA13F3  ...  0.495256\n","11958  ID_D2FIDNG7  ...  0.495256\n","3662   ID_0IPY5PTP  ...  0.495256\n","12006  ID_BZGZAP8D  ...  0.495256\n","12093  ID_BN56QZWZ  ...  0.495256\n","3314   ID_N76HN2AM  ...  0.495256\n","3176   ID_HY9L4U4V  ...  0.495256\n","3126   ID_PDFIHPHE  ...  0.495256\n","3833   ID_7C1SNIQO  ...  0.495256\n","5096   ID_FXEUVDTW  ...  0.495256\n","5145   ID_2X4FPGG9  ...  0.495256\n","\n","[20 rows x 5 columns]"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"code","metadata":{"id":"LNtN9qGpdhUN","executionInfo":{"status":"ok","timestamp":1629109266931,"user_tz":-180,"elapsed":24,"user":{"displayName":"Cedric Manouan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnI55k9QnJV4mtok5wRGE36cZcj1DZIjihnSx6=s64","userId":"00242929915047742214"}}},"source":[""],"execution_count":23,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":142},"id":"5CaUCQ1fdhUN","executionInfo":{"status":"ok","timestamp":1629109266931,"user_tz":-180,"elapsed":24,"user":{"displayName":"Cedric Manouan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnI55k9QnJV4mtok5wRGE36cZcj1DZIjihnSx6=s64","userId":"00242929915047742214"}},"outputId":"4e0a5cbe-4172-4432-cb81-fa3f0523f8a0"},"source":["final_out = findf[['Tweet_ID','fintype']]\n","final_out = final_out.rename({'fintype':'type'},axis=1)\n","final_out.head(3)"],"execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Tweet_ID</th>\n","      <th>type</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>ID_D9ONL553</td>\n","      <td>sexual_violence</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>ID_263YTILY</td>\n","      <td>emotional_violence</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>ID_62VS6IXC</td>\n","      <td>emotional_violence</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      Tweet_ID                type\n","0  ID_D9ONL553     sexual_violence\n","1  ID_263YTILY  emotional_violence\n","2  ID_62VS6IXC  emotional_violence"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hr67z5yCdhUO","executionInfo":{"status":"ok","timestamp":1629109266932,"user_tz":-180,"elapsed":23,"user":{"displayName":"Cedric Manouan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnI55k9QnJV4mtok5wRGE36cZcj1DZIjihnSx6=s64","userId":"00242929915047742214"}},"outputId":"a570bbd8-13b3-4b8d-aa45-d9ea7faa60a5"},"source":["final_out['type'].value_counts()"],"execution_count":25,"outputs":[{"output_type":"execute_result","data":{"text/plain":["sexual_violence                 8101\n","Physical_violence               3378\n","Harmful_Traditional_practice    3158\n","economic_violence                498\n","emotional_violence               446\n","Name: type, dtype: int64"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"code","metadata":{"id":"LBrzrHgXdhUO","executionInfo":{"status":"ok","timestamp":1629109266933,"user_tz":-180,"elapsed":22,"user":{"displayName":"Cedric Manouan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnI55k9QnJV4mtok5wRGE36cZcj1DZIjihnSx6=s64","userId":"00242929915047742214"}}},"source":["final_out.to_csv('outcome_stage3.csv',index=False)"],"execution_count":26,"outputs":[]},{"cell_type":"code","metadata":{"id":"9GXEw_YIdhUO","executionInfo":{"status":"ok","timestamp":1629109266933,"user_tz":-180,"elapsed":21,"user":{"displayName":"Cedric Manouan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnI55k9QnJV4mtok5wRGE36cZcj1DZIjihnSx6=s64","userId":"00242929915047742214"}}},"source":[""],"execution_count":26,"outputs":[]}]}