{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"Stage2_Classification_sexual_vs_harmful_traditional_violence.ipynb","provenance":[]}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5qs-W7OTYX-z","executionInfo":{"status":"ok","timestamp":1629108570867,"user_tz":-180,"elapsed":3099,"user":{"displayName":"Cedric Manouan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnI55k9QnJV4mtok5wRGE36cZcj1DZIjihnSx6=s64","userId":"00242929915047742214"}},"outputId":"b8c7e3f0-c79d-430e-e2ad-b58a02720d3b"},"source":["!pip install emoji"],"execution_count":15,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: emoji in /usr/local/lib/python3.7/dist-packages (1.4.2)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BGgBtKskYM6D","executionInfo":{"status":"ok","timestamp":1629108570867,"user_tz":-180,"elapsed":21,"user":{"displayName":"Cedric Manouan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnI55k9QnJV4mtok5wRGE36cZcj1DZIjihnSx6=s64","userId":"00242929915047742214"}},"outputId":"a704cd8c-ab88-4553-de0b-dac0a6637e7e"},"source":["import pandas as pd\n","import numpy as np\n","import re\n","\n","import nltk\n","nltk.download('punkt')\n","from nltk.tokenize import TreebankWordTokenizer\n","from collections import Counter\n","from nltk.tokenize import word_tokenize\n","from nltk.probability import FreqDist\n","import emoji\n","import spacy\n","from operator import itemgetter\n","import itertools\n","from itertools import combinations\n","\n","\n","from nltk.stem.snowball import SnowballStemmer\n","\n","\n","from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n","from tqdm import tqdm\n","tqdm.pandas()\n"],"execution_count":16,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6Khwf2x8YU9t","executionInfo":{"status":"ok","timestamp":1629108570868,"user_tz":-180,"elapsed":15,"user":{"displayName":"Cedric Manouan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnI55k9QnJV4mtok5wRGE36cZcj1DZIjihnSx6=s64","userId":"00242929915047742214"}},"outputId":"f16a93ec-f6d0-4a40-9e36-2cebb5692991"},"source":["from google.colab import drive\n","drive.mount(\"/content/gdrive\")"],"execution_count":17,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"bOqU0AGOYM6G"},"source":["## Stopwords and spacy model\n","* We extract a list of stopwords from - https://gist.github.com/sebleier/554280 because the available list of NLTK/Spacy stopwords are highly insufficient and incomplete.\n","* We extract the Spacy's english pipeline optimized for CPU and will be using it for stemming and lemmatization tasks\n","* We also instantiate SnowballStemmer to perform stemming after lemmatization is done in later stages of the notebook"]},{"cell_type":"code","metadata":{"id":"qkeHcHl_YM6G","executionInfo":{"status":"ok","timestamp":1629108571443,"user_tz":-180,"elapsed":580,"user":{"displayName":"Cedric Manouan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnI55k9QnJV4mtok5wRGE36cZcj1DZIjihnSx6=s64","userId":"00242929915047742214"}}},"source":["#### Stopwords list taken from - https://gist.github.com/sebleier/554280 (reason - the available list of NLTK/ Spacy stopwords are too less and don't cover all possible words)\n","stopwords_list = [\"0o\", \"0s\", \"3a\", \"3b\", \"3d\", \"6b\", \"6o\", \"a\", \"a1\", \"a2\", \"a3\", \"a4\", \"ab\", \"able\", \"about\", \"above\", \"abst\", \"ac\", \"accordance\", \"according\", \"accordingly\", \"across\", \"act\", \"actually\", \"ad\", \"added\", \"adj\", \"ae\", \"af\", \"affected\", \"affecting\", \"affects\", \"after\", \"afterwards\", \"ag\", \"again\", \"against\", \"ah\", \"ain\", \"ain't\", \"aj\", \"al\", \"all\", \"allow\", \"allows\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\", \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"announce\", \"another\", \"any\", \"anybody\", \"anyhow\", \"anymore\", \"anyone\", \"anything\", \"anyway\", \"anyways\", \"anywhere\", \"ao\", \"ap\", \"apart\", \"apparently\", \"appear\", \"appreciate\", \"appropriate\", \"approximately\", \"ar\", \"are\", \"aren\", \"arent\", \"aren't\", \"arise\", \"around\", \"as\", \"a's\", \"aside\", \"ask\", \"asking\", \"associated\", \"at\", \"au\", \"auth\", \"av\", \"available\", \"aw\", \"away\", \"awfully\", \"ax\", \"ay\", \"az\", \"b\", \"b1\", \"b2\", \"b3\", \"ba\", \"back\", \"bc\", \"bd\", \"be\", \"became\", \"because\", \"become\", \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"begin\", \"beginning\", \"beginnings\", \"begins\", \"behind\", \"being\", \"believe\", \"below\", \"beside\", \"besides\", \"best\", \"better\", \"between\", \"beyond\", \"bi\", \"bill\", \"biol\", \"bj\", \"bk\", \"bl\", \"bn\", \"both\", \"bottom\", \"bp\", \"br\", \"brief\", \"briefly\", \"bs\", \"bt\", \"bu\", \"but\", \"bx\", \"by\", \"c\", \"c1\", \"c2\", \"c3\", \"ca\", \"call\", \"came\", \"can\", \"cannot\", \"cant\", \"can't\", \"cause\", \"causes\", \"cc\", \"cd\", \"ce\", \"certain\", \"certainly\", \"cf\", \"cg\", \"ch\", \"changes\", \"ci\", \"cit\", \"cj\", \"cl\", \"clearly\", \"cm\", \"c'mon\", \"cn\", \"co\", \"com\", \"come\", \"comes\", \"con\", \"concerning\", \"consequently\", \"consider\", \"considering\", \"contain\", \"containing\", \"contains\", \"corresponding\", \"could\", \"couldn\", \"couldnt\", \"couldn't\", \"course\", \"cp\", \"cq\", \"cr\", \"cry\", \"cs\", \"c's\", \"ct\", \"cu\", \"currently\", \"cv\", \"cx\", \"cy\", \"cz\", \"d\", \"d2\", \"da\", \"date\", \"dc\", \"dd\", \"de\", \"definitely\", \"describe\", \"described\", \"despite\", \"detail\", \"df\", \"di\", \"did\", \"didn\", \"didn't\", \"different\", \"dj\", \"dk\", \"dl\", \"do\", \"does\", \"doesn\", \"doesn't\", \"doing\", \"don\", \"done\", \"don't\", \"down\", \"downwards\", \"dp\", \"dr\", \"ds\", \"dt\", \"du\", \"due\", \"during\", \"dx\", \"dy\", \"e\", \"e2\", \"e3\", \"ea\", \"each\", \"ec\", \"ed\", \"edu\", \"ee\", \"ef\", \"effect\", \"eg\", \"ei\", \"eight\", \"eighty\", \"either\", \"ej\", \"el\", \"eleven\", \"else\", \"elsewhere\", \"em\", \"empty\", \"en\", \"end\", \"ending\", \"enough\", \"entirely\", \"eo\", \"ep\", \"eq\", \"er\", \"es\", \"especially\", \"est\", \"et\", \"et-al\", \"etc\", \"eu\", \"ev\", \"even\", \"ever\", \"every\", \"everybody\", \"everyone\", \"everything\", \"everywhere\", \"ex\", \"exactly\", \"example\", \"except\", \"ey\", \"f\", \"f2\", \"fa\", \"far\", \"fc\", \"few\", \"ff\", \"fi\", \"fifteen\", \"fifth\", \"fify\", \"fill\", \"find\", \"fire\", \"first\", \"five\", \"fix\", \"fj\", \"fl\", \"fn\", \"fo\", \"followed\", \"following\", \"follows\", \"for\", \"former\", \"formerly\", \"forth\", \"forty\", \"found\", \"four\", \"fr\", \"from\", \"front\", \"fs\", \"ft\", \"fu\", \"full\", \"further\", \"furthermore\", \"fy\", \"g\", \"ga\", \"gave\", \"ge\", \"get\", \"gets\", \"getting\", \"gi\", \"give\", \"given\", \"gives\", \"giving\", \"gj\", \"gl\", \"go\", \"goes\", \"going\", \"gone\", \"got\", \"gotten\", \"gr\", \"greetings\", \"gs\", \"gy\", \"h\", \"h2\", \"h3\", \"had\", \"hadn\", \"hadn't\", \"happens\", \"hardly\", \"has\", \"hasn\", \"hasnt\", \"hasn't\", \"have\", \"haven\", \"haven't\", \"having\", \"he\", \"hed\", \"he'd\", \"he'll\", \"hello\", \"help\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"heres\", \"here's\", \"hereupon\", \"hers\", \"herself\", \"hes\", \"he's\", \"hh\", \"hi\", \"hid\", \"him\", \"himself\", \"his\", \"hither\", \"hj\", \"ho\", \"home\", \"hopefully\", \"how\", \"howbeit\", \"however\", \"how's\", \"hr\", \"hs\", \"http\", \"hu\", \"hundred\", \"hy\", \"i\", \"i2\", \"i3\", \"i4\", \"i6\", \"i7\", \"i8\", \"ia\", \"ib\", \"ibid\", \"ic\", \"id\", \"i'd\", \"ie\", \"if\", \"ig\", \"ignored\", \"ih\", \"ii\", \"ij\", \"il\", \"i'll\", \"im\", \"i'm\", \"immediate\", \"immediately\", \"importance\", \"important\", \"in\", \"inasmuch\", \"inc\", \"indeed\", \"index\", \"indicate\", \"indicated\", \"indicates\", \"information\", \"inner\", \"insofar\", \"instead\", \"interest\", \"into\", \"invention\", \"inward\", \"io\", \"ip\", \"iq\", \"ir\", \"is\", \"isn\", \"isn't\", \"it\", \"itd\", \"it'd\", \"it'll\", \"its\", \"it's\", \"itself\", \"iv\", \"i've\", \"ix\", \"iy\", \"iz\", \"j\", \"jj\", \"jr\", \"js\", \"jt\", \"ju\", \"just\", \"k\", \"ke\", \"keep\", \"keeps\", \"kept\", \"kg\", \"kj\", \"km\", \"know\", \"known\", \"knows\", \"ko\", \"l\", \"l2\", \"la\", \"largely\", \"last\", \"lately\", \"later\", \"latter\", \"latterly\", \"lb\", \"lc\", \"le\", \"least\", \"les\", \"less\", \"lest\", \"let\", \"lets\", \"let's\", \"lf\", \"like\", \"liked\", \"likely\", \"line\", \"little\", \"lj\", \"ll\", \"ll\", \"ln\", \"lo\", \"look\", \"looking\", \"looks\", \"los\", \"lr\", \"ls\", \"lt\", \"ltd\", \"m\", \"m2\", \"ma\", \"made\", \"mainly\", \"make\", \"makes\", \"many\", \"may\", \"maybe\", \"me\", \"mean\", \"means\", \"meantime\", \"meanwhile\", \"merely\", \"mg\", \"might\", \"mightn\", \"mightn't\", \"mill\", \"million\", \"mine\", \"miss\", \"ml\", \"mn\", \"mo\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"mr\", \"mrs\", \"ms\", \"mt\", \"mu\", \"much\", \"mug\", \"must\", \"mustn\", \"mustn't\", \"my\", \"myself\", \"n\", \"n2\", \"na\", \"name\", \"namely\", \"nay\", \"nc\", \"nd\", \"ne\", \"near\", \"nearly\", \"necessarily\", \"necessary\", \"need\", \"needn\", \"needn't\", \"needs\", \"neither\", \"never\", \"nevertheless\", \"new\", \"next\", \"ng\", \"ni\", \"nine\", \"ninety\", \"nj\", \"nl\", \"nn\", \"no\", \"nobody\", \"non\", \"none\", \"nonetheless\", \"noone\", \"nor\", \"normally\", \"nos\", \"not\", \"noted\", \"nothing\", \"novel\", \"now\", \"nowhere\", \"nr\", \"ns\", \"nt\", \"ny\", \"o\", \"oa\", \"ob\", \"obtain\", \"obtained\", \"obviously\", \"oc\", \"od\", \"of\", \"off\", \"often\", \"og\", \"oh\", \"oi\", \"oj\", \"ok\", \"okay\", \"ol\", \"old\", \"om\", \"omitted\", \"on\", \"once\", \"one\", \"ones\", \"only\", \"onto\", \"oo\", \"op\", \"oq\", \"or\", \"ord\", \"os\", \"ot\", \"other\", \"others\", \"otherwise\", \"ou\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"outside\", \"over\", \"overall\", \"ow\", \"owing\", \"own\", \"ox\", \"oz\", \"p\", \"p1\", \"p2\", \"p3\", \"page\", \"pagecount\", \"pages\", \"par\", \"part\", \"particular\", \"particularly\", \"pas\", \"past\", \"pc\", \"pd\", \"pe\", \"per\", \"perhaps\", \"pf\", \"ph\", \"pi\", \"pj\", \"pk\", \"pl\", \"placed\", \"please\", \"plus\", \"pm\", \"pn\", \"po\", \"poorly\", \"possible\", \"possibly\", \"potentially\", \"pp\", \"pq\", \"pr\", \"predominantly\", \"present\", \"presumably\", \"previously\", \"primarily\", \"probably\", \"promptly\", \"proud\", \"provides\", \"ps\", \"pt\", \"pu\", \"put\", \"py\", \"q\", \"qj\", \"qu\", \"que\", \"quickly\", \"quite\", \"qv\", \"r\", \"r2\", \"ra\", \"ran\", \"rather\", \"rc\", \"rd\", \"re\", \"readily\", \"really\", \"reasonably\", \"recent\", \"recently\", \"ref\", \"refs\", \"regarding\", \"regardless\", \"regards\", \"related\", \"relatively\", \"research\", \"research-articl\", \"respectively\", \"resulted\", \"resulting\", \"results\", \"rf\", \"rh\", \"ri\", \"right\", \"rj\", \"rl\", \"rm\", \"rn\", \"ro\", \"rq\", \"rr\", \"rs\", \"rt\", \"ru\", \"run\", \"rv\", \"ry\", \"s\", \"s2\", \"sa\", \"said\", \"same\", \"saw\", \"say\", \"saying\", \"says\", \"sc\", \"sd\", \"se\", \"sec\", \"second\", \"secondly\", \"section\", \"see\", \"seeing\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"seen\", \"self\", \"selves\", \"sensible\", \"sent\", \"serious\", \"seriously\", \"seven\", \"several\", \"sf\", \"shall\", \"shan\", \"shan't\", \"she\", \"shed\", \"she'd\", \"she'll\", \"shes\", \"she's\", \"should\", \"shouldn\", \"shouldn't\", \"should've\", \"show\", \"showed\", \"shown\", \"showns\", \"shows\", \"si\", \"side\", \"significant\", \"significantly\", \"similar\", \"similarly\", \"since\", \"sincere\", \"six\", \"sixty\", \"sj\", \"sl\", \"slightly\", \"sm\", \"sn\", \"so\", \"some\", \"somebody\", \"somehow\", \"someone\", \"somethan\", \"something\", \"sometime\", \"sometimes\", \"somewhat\", \"somewhere\", \"soon\", \"sorry\", \"sp\", \"specifically\", \"specified\", \"specify\", \"specifying\", \"sq\", \"sr\", \"ss\", \"st\", \"still\", \"stop\", \"strongly\", \"sub\", \"substantially\", \"successfully\", \"such\", \"sufficiently\", \"suggest\", \"sup\", \"sure\", \"sy\", \"system\", \"sz\", \"t\", \"t1\", \"t2\", \"t3\", \"take\", \"taken\", \"taking\", \"tb\", \"tc\", \"td\", \"te\", \"tell\", \"ten\", \"tends\", \"tf\", \"th\", \"than\", \"thank\", \"thanks\", \"thanx\", \"that\", \"that'll\", \"thats\", \"that's\", \"that've\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"thered\", \"therefore\", \"therein\", \"there'll\", \"thereof\", \"therere\", \"theres\", \"there's\", \"thereto\", \"thereupon\", \"there've\", \"these\", \"they\", \"theyd\", \"they'd\", \"they'll\", \"theyre\", \"they're\", \"they've\", \"thickv\", \"thin\", \"think\", \"third\", \"this\", \"thorough\", \"thoroughly\", \"those\", \"thou\", \"though\", \"thoughh\", \"thousand\", \"three\", \"throug\", \"through\", \"throughout\", \"thru\", \"thus\", \"ti\", \"til\", \"tip\", \"tj\", \"tl\", \"tm\", \"tn\", \"to\", \"together\", \"too\", \"took\", \"top\", \"toward\", \"towards\", \"tp\", \"tq\", \"tr\", \"tried\", \"tries\", \"truly\", \"try\", \"trying\", \"ts\", \"t's\", \"tt\", \"tv\", \"twelve\", \"twenty\", \"twice\", \"two\", \"tx\", \"u\", \"u201d\", \"ue\", \"ui\", \"uj\", \"uk\", \"um\", \"un\", \"under\", \"unfortunately\", \"unless\", \"unlike\", \"unlikely\", \"until\", \"unto\", \"uo\", \"up\", \"upon\", \"ups\", \"ur\", \"us\", \"use\", \"used\", \"useful\", \"usefully\", \"usefulness\", \"uses\", \"using\", \"usually\", \"ut\", \"v\", \"va\", \"value\", \"various\", \"vd\", \"ve\", \"ve\", \"very\", \"via\", \"viz\", \"vj\", \"vo\", \"vol\", \"vols\", \"volumtype\", \"vq\", \"vs\", \"vt\", \"vu\", \"w\", \"wa\", \"want\", \"wants\", \"was\", \"wasn\", \"wasnt\", \"wasn't\", \"way\", \"we\", \"wed\", \"we'd\", \"welcome\", \"well\", \"we'll\", \"well-b\", \"went\", \"were\", \"we're\", \"weren\", \"werent\", \"weren't\", \"we've\", \"what\", \"whatever\", \"what'll\", \"whats\", \"what's\", \"when\", \"whence\", \"whenever\", \"when's\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"wheres\", \"where's\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whim\", \"whither\", \"who\", \"whod\", \"whoever\", \"whole\", \"who'll\", \"whom\", \"whomever\", \"whos\", \"who's\", \"whose\", \"why\", \"why's\", \"wi\", \"widely\", \"will\", \"willing\", \"wish\", \"with\", \"within\", \"without\", \"wo\", \"won\", \"wonder\", \"wont\", \"won't\", \"words\", \"world\", \"would\", \"wouldn\", \"wouldnt\", \"wouldn't\", \"www\", \"x\", \"x1\", \"x2\", \"x3\", \"xf\", \"xi\", \"xj\", \"xk\", \"xl\", \"xn\", \"xo\", \"xs\", \"xt\", \"xv\", \"xx\", \"y\", \"y2\", \"yes\", \"yet\", \"yj\", \"yl\", \"you\", \"youd\", \"you'd\", \"you'll\", \"your\", \"youre\", \"you're\", \"yours\", \"yourself\", \"yourselves\", \"you've\", \"yr\", \"ys\", \"yt\", \"z\", \"zero\", \"zi\", \"zz\",]\n","\n","nlp        = spacy.load(\"en_core_web_sm\")\n","\n","stemmer = SnowballStemmer(language='english')"],"execution_count":18,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Hf1NwLD6YM6K"},"source":["## Custom preprocessing functions for Vocabulary extraction\n","We declare basic functions to perform tasks like removal of stopwords, stemming, lemmatization in batches, removal and emojis etc. "]},{"cell_type":"code","metadata":{"id":"GiySejRLYM6L","executionInfo":{"status":"ok","timestamp":1629108571443,"user_tz":-180,"elapsed":4,"user":{"displayName":"Cedric Manouan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnI55k9QnJV4mtok5wRGE36cZcj1DZIjihnSx6=s64","userId":"00242929915047742214"}}},"source":["def remove_stopwords(text):\n","    '''\n","      Removes stopwords from the text\n","    '''\n","\n","    text_split   = text.split()\n","\n","    text_list    = [word for word in text_split if not word in stopwords_list]\n","\n","    return ' '.join(text_list)\n","\n","def stemming_perform(text):\n","    '''\n","      Performs lemmatization in batches\n","    '''\n","    \n","    token = text.split()\n","    stemmed_text = []\n","    for tok in token:\n","        stemmed_text.append(stemmer.stem(tok))\n","    return ' '.join(stemmed_text)\n","\n","\n","def cleaning_text(text):\n","    '''\n","      Operations performed:- \n","      1. Converting the entire text to lowercase\n","      2. Removal of punctuations from the text\n","      3. Removal of numbers from the text\n","    '''\n","\n","    text = text.lower()\n","\n","    text = re.sub(r'[^\\w\\s]', ' ', text)\n","    text = re.sub(r'\\d+', '', text)\n","    text = remove_stopwords(text)\n","\n","    text = text.replace('amp','') ## this was actually ampersand which kept coming in frequently that made no sense\n","    return text\n","\n","\n","def give_emoji_free_text(text):\n","    ''' \n","      Removes all possible emojis from the text (because our text is basically tweets that can have emoijs).\n","      The input is a text and the output is emoji free text\n","    '''\n","    allchars = [str for str in text.encode().decode('utf-8')]\n","    emoji_list = [c for c in allchars if c in emoji.UNICODE_EMOJI]\n","    clean_text = ' '.join([str for str in text.encode().decode('utf-8').split() if not any(i in str for i in emoji_list)])\n","    return clean_text\n","\n","\n","def preprocess_custom(text):\n","    text = give_emoji_free_text(text)\n","    text = cleaning_text(text)\n","\n","    return text\n","\n","def lemmatize_batches(docs):\n","    '''\n","      Performs lemmatization in batches\n","    '''\n","    lemmatized_text = []\n","\n","    for doc in nlp.pipe(docs, batch_size=256, n_process=3,disable=[\"parser\", \"ner\"]):\n","        lemmatized_text.append(str(' '.join([token.lemma_ for token in doc])))\n","    return lemmatized_text\n","\n","\n","def lemmatize_df(df):\n","    lemmatized = []\n","    for i in tqdm(range(0,df.shape[0],1000)):\n","        z = lemmatize_batches(df['preprocessed_text'].iloc[i:i+1000])\n","        lemmatized.extend(z)\n","    df['lemmatize'] = lemmatized\n","    return df\n","\n","\n","\n","def return_frequency_dict_sorted(df):\n","    sent = df['stemmed'].values\n","    sent = ' '.join(sent)\n","\n","    fdist = FreqDist()\n","    \n","    for word in word_tokenize(sent):\n","        fdist[word.lower()] += 1\n","        \n","    return sorted(fdist.items(), key=lambda x: x[1], reverse=True)\n","\n","def replace(prob,x2):\n","    \n","    if str(prob)!='nan':\n","        if prob>0.6:\n","            return 'Harmful_Traditional_practice'\n","        else:\n","            return x2\n","    else:\n","        return x2\n"],"execution_count":19,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HZcIm6Z0YM6M"},"source":["## Vocab extraction and Preprocessing Stage\n","We will preprocess the text in two stages\n","1. Here we will consider only the 'Harmful_traditional practice' class and find the vocabulary corresponding to it\n","2. Next we will consider the class 'sexual_violence' and find the vocabulary corresponding to it.\n","3. Out combined vocabulary will be the vocabulary from both the stages"]},{"cell_type":"markdown","metadata":{"id":"RekuNYlPYM6M"},"source":["####  Our input test data will be the Data from previous stage - outcome_stage1.csv"]},{"cell_type":"markdown","metadata":{"id":"CIrpImD-YM6M"},"source":["## Text Preprocessing for Vocab extraction for class 'Harmful_Traditional_practice'\n","We preprocess the test data tweets and then find the frequency occurance of the preprocessed words from the dataset. We will now select top 20 words from the test data tweets which will be used as a vocabulary for building our model. We consider only the class ' Harmful_Traditional_practice' "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SUjn00yOYM6N","executionInfo":{"status":"ok","timestamp":1629108581723,"user_tz":-180,"elapsed":10283,"user":{"displayName":"Cedric Manouan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnI55k9QnJV4mtok5wRGE36cZcj1DZIjihnSx6=s64","userId":"00242929915047742214"}},"outputId":"69554e6a-128f-45d3-dd4e-1a712bf25ffd"},"source":["subdf   = pd.read_csv('/content/gdrive/MyDrive/Hackathon/outcome_stage1.csv')\n","\n","subdf   = subdf[(subdf['type']=='Harmful_Traditional_practice')]\n","\n","print(f'The shape of subsetted data with subsetting on Harmful_Traditional_practice will be {subdf.shape}')\n","\n","test_df = pd.read_csv('/content/gdrive/MyDrive/Hackathon/Test.csv')\n","\n","temp_df = subdf.merge(test_df,on=['Tweet_ID'],how='left')\n","\n","_       = temp_df.rename(columns={'type':'type1'},inplace=True)\n","\n","print(f'The shape of data after assigning the labels attained from BERT to test is {temp_df.shape}')\n","\n","temp_df['preprocessed_text'] = temp_df['tweet'].progress_apply(preprocess_custom)\n","temp_df                      = lemmatize_df(temp_df)\n","temp_df['stemmed']           = temp_df['lemmatize'].progress_apply(stemming_perform)\n","sorted_freq_dict             = return_frequency_dict_sorted(temp_df)\n","\n","threshold2consider           = 10\n","vocab_htp                    = [word[0] for word in sorted_freq_dict[:threshold2consider]]\n"],"execution_count":20,"outputs":[{"output_type":"stream","text":["The shape of subsetted data with subsetting on Harmful_Traditional_practice will be (3100, 2)\n","The shape of data after assigning the labels attained from BERT to test is (3100, 3)\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 3100/3100 [00:01<00:00, 1576.35it/s]\n","100%|██████████| 4/4 [00:06<00:00,  1.68s/it]\n","100%|██████████| 3100/3100 [00:00<00:00, 3443.51it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":376},"id":"gn-qrtlmYM6O","executionInfo":{"status":"ok","timestamp":1629108581737,"user_tz":-180,"elapsed":67,"user":{"displayName":"Cedric Manouan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnI55k9QnJV4mtok5wRGE36cZcj1DZIjihnSx6=s64","userId":"00242929915047742214"}},"outputId":"f711b48c-e251-4412-db06-44b44476c6de"},"source":["print('The top 10 words (preprocessed) based on the frequency distribution and their relative frequencies are given below')\n","pd.DataFrame(sorted_freq_dict,columns = ['word','Frequency']).head(10)"],"execution_count":21,"outputs":[{"output_type":"stream","text":["The top 10 words (preprocessed) based on the frequency distribution and their relative frequencies are given below\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>word</th>\n","      <th>Frequency</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>forc</td>\n","      <td>3315</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>fgm</td>\n","      <td>2803</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>marriag</td>\n","      <td>2383</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>woman</td>\n","      <td>1456</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>child</td>\n","      <td>1169</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>girl</td>\n","      <td>1148</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>femal</td>\n","      <td>580</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>sex</td>\n","      <td>465</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>violenc</td>\n","      <td>460</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>rape</td>\n","      <td>458</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      word  Frequency\n","0     forc       3315\n","1      fgm       2803\n","2  marriag       2383\n","3    woman       1456\n","4    child       1169\n","5     girl       1148\n","6    femal        580\n","7      sex        465\n","8  violenc        460\n","9     rape        458"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"markdown","metadata":{"id":"8AEsko9UYM6O"},"source":["## Text Preprocessing for Vocab extraction for class 'sexual_violence'\n","We preprocess the test data tweets and then find the frequency occurance of the preprocessed words from the dataset. We will now select top 20 words from the test data tweets which will be used as a vocabulary for building our model. We consider only the class ' sexual_violence' "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CxuFGy64YM6O","executionInfo":{"status":"ok","timestamp":1629108657863,"user_tz":-180,"elapsed":20127,"user":{"displayName":"Cedric Manouan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnI55k9QnJV4mtok5wRGE36cZcj1DZIjihnSx6=s64","userId":"00242929915047742214"}},"outputId":"38f77734-0a88-4894-9979-1a68d60dd642"},"source":["subdf   = pd.read_csv('/content/gdrive/MyDrive/Hackathon/outcome_stage1.csv')\n","\n","subdf   = subdf[(subdf['type']=='sexual_violence')]\n","\n","print(f'The shape of subsetted data with subsetting on sexual_violence will be {subdf.shape}')\n","\n","test_df = pd.read_csv('/content/gdrive/MyDrive/Hackathon/Test.csv')\n","\n","temp_df = subdf.merge(test_df,on=['Tweet_ID'],how='left')\n","\n","_       = temp_df.rename(columns={'type':'type1'},inplace=True)\n","\n","print(f'The shape of data after assigning the labels attained from BERT to test is {temp_df.shape}')\n","\n","temp_df['preprocessed_text'] = temp_df['tweet'].progress_apply(preprocess_custom)\n","temp_df                      = lemmatize_df(temp_df)\n","temp_df['stemmed']           = temp_df['lemmatize'].progress_apply(stemming_perform)\n","sorted_freq_dict             = return_frequency_dict_sorted(temp_df)\n","\n","threshold2consider           = 20\n","vocab_sex                    = [word[0] for word in sorted_freq_dict[:threshold2consider]]\n"],"execution_count":23,"outputs":[{"output_type":"stream","text":["The shape of subsetted data with subsetting on sexual_violence will be (7946, 2)\n","The shape of data after assigning the labels attained from BERT to test is (7946, 3)\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 7946/7946 [00:04<00:00, 1618.45it/s]\n","100%|██████████| 8/8 [00:11<00:00,  1.39s/it]\n","100%|██████████| 7946/7946 [00:01<00:00, 4072.57it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":686},"id":"6f8eci1kYM6O","executionInfo":{"status":"ok","timestamp":1629108658370,"user_tz":-180,"elapsed":9,"user":{"displayName":"Cedric Manouan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnI55k9QnJV4mtok5wRGE36cZcj1DZIjihnSx6=s64","userId":"00242929915047742214"}},"outputId":"55ea10d2-940f-4ffe-f879-fd9be6d16c6f"},"source":["print('The top 20 words (preprocessed) based on the frequency distribution and their relative frequencies are given below')\n","pd.DataFrame(sorted_freq_dict,columns = ['word','Frequency']).head(20)"],"execution_count":24,"outputs":[{"output_type":"stream","text":["The top 20 words (preprocessed) based on the frequency distribution and their relative frequencies are given below\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>word</th>\n","      <th>Frequency</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>rape</td>\n","      <td>8659</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>boyfriend</td>\n","      <td>3762</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>student</td>\n","      <td>1840</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>woman</td>\n","      <td>1675</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>year</td>\n","      <td>1632</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>univers</td>\n","      <td>1630</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>sex</td>\n","      <td>1456</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>girl</td>\n","      <td>1385</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>forc</td>\n","      <td>1332</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>man</td>\n","      <td>1136</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>priest</td>\n","      <td>1121</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>pastor</td>\n","      <td>982</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>friend</td>\n","      <td>945</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>kill</td>\n","      <td>718</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>time</td>\n","      <td>715</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>abus</td>\n","      <td>665</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>murder</td>\n","      <td>655</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>tell</td>\n","      <td>651</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>peopl</td>\n","      <td>582</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>church</td>\n","      <td>561</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         word  Frequency\n","0        rape       8659\n","1   boyfriend       3762\n","2     student       1840\n","3       woman       1675\n","4        year       1632\n","5     univers       1630\n","6         sex       1456\n","7        girl       1385\n","8        forc       1332\n","9         man       1136\n","10     priest       1121\n","11     pastor        982\n","12     friend        945\n","13       kill        718\n","14       time        715\n","15       abus        665\n","16     murder        655\n","17       tell        651\n","18      peopl        582\n","19     church        561"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MyuSBxaxYM6P","executionInfo":{"status":"ok","timestamp":1629108658814,"user_tz":-180,"elapsed":11,"user":{"displayName":"Cedric Manouan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnI55k9QnJV4mtok5wRGE36cZcj1DZIjihnSx6=s64","userId":"00242929915047742214"}},"outputId":"68c9b41e-6642-4816-b5b5-5d1319392c88"},"source":["total_vocab = list(set(vocab_sex+vocab_htp))\n","total_vocab"],"execution_count":25,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['year',\n"," 'man',\n"," 'murder',\n"," 'woman',\n"," 'tell',\n"," 'student',\n"," 'kill',\n"," 'child',\n"," 'rape',\n"," 'friend',\n"," 'forc',\n"," 'fgm',\n"," 'femal',\n"," 'church',\n"," 'marriag',\n"," 'priest',\n"," 'girl',\n"," 'sex',\n"," 'time',\n"," 'boyfriend',\n"," 'abus',\n"," 'univers',\n"," 'peopl',\n"," 'pastor',\n"," 'violenc']"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"markdown","metadata":{"id":"z3YL28cJYM6P"},"source":["# Classification model on the vocabulary attained\n","We will now preprocess our train and BERT Classified physical and sexual data to finetune it <br>\n","To perform that our test data was already preprocessed in the previous step. We will now preprocess our train dataset"]},{"cell_type":"markdown","metadata":{"id":"_bYC-VMQYM6P"},"source":["## Text Preprocessing on Train data\n","We subset our train data having only Physical Violence and Sexual Violence labels and perform the exact same preprocessing steps we did for the test data above "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"75tqDVs0YM6P","executionInfo":{"status":"ok","timestamp":1629108751551,"user_tz":-180,"elapsed":92744,"user":{"displayName":"Cedric Manouan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnI55k9QnJV4mtok5wRGE36cZcj1DZIjihnSx6=s64","userId":"00242929915047742214"}},"outputId":"1db8a60e-7268-4f9b-9392-4b744cbf25af"},"source":["test_df                         = pd.read_csv('/content/gdrive/MyDrive/Hackathon/Test.csv')\n","subdf                           = pd.read_csv('/content/gdrive/MyDrive/Hackathon/outcome_stage1.csv')\n","\n","test_df                         = test_df.merge(subdf,on=['Tweet_ID'],how='left')\n","\n","\n","subdf                           = subdf[(subdf['type']=='sexual_violence') | (subdf['type']=='Harmful_Traditional_practice')]\n","\n","\n","temp_df                         = subdf.merge(test_df,on=['Tweet_ID'],how='left')\n","\n","_                               = temp_df.rename(columns={'type':'type1'},inplace=True)\n","\n","temp_df['preprocessed_text']    = temp_df['tweet'].progress_apply(preprocess_custom)\n","temp_df                         = lemmatize_df(temp_df)\n","temp_df['stemmed']              = temp_df['lemmatize'].progress_apply(stemming_perform)\n","\n","\n","train_df                        = pd.read_csv('/content/gdrive/MyDrive/Hackathon/Train.csv')\n","train_data                      = train_df[train_df['type'].isin(['Harmful_Traditional_practice','sexual_violence'])]\n","train_data                      = train_data.reset_index()\n","\n","train_data['preprocessed_text'] = train_data['tweet'].progress_apply(preprocess_custom)\n","train_data                      = lemmatize_df(train_data)\n","train_data['stemmed']           = train_data['lemmatize'].progress_apply(stemming_perform)\n","\n","print(test_df.shape,train_df.shape,temp_df.shape)"],"execution_count":26,"outputs":[{"output_type":"stream","text":["100%|██████████| 11046/11046 [00:06<00:00, 1615.40it/s]\n","100%|██████████| 12/12 [00:11<00:00,  1.05it/s]\n","100%|██████████| 11046/11046 [00:02<00:00, 3855.10it/s]\n","100%|██████████| 32836/32836 [00:21<00:00, 1502.96it/s]\n","100%|██████████| 33/33 [00:40<00:00,  1.23s/it]\n","100%|██████████| 32836/32836 [00:07<00:00, 4317.26it/s]"],"name":"stderr"},{"output_type":"stream","text":["(15581, 3) (39650, 3) (11046, 7)\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"IDyFL2qDYM6Q","executionInfo":{"status":"ok","timestamp":1629108751557,"user_tz":-180,"elapsed":10,"user":{"displayName":"Cedric Manouan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnI55k9QnJV4mtok5wRGE36cZcj1DZIjihnSx6=s64","userId":"00242929915047742214"}}},"source":[""],"execution_count":26,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JFOOp2XLYM6Q"},"source":["## Count vectorization with binarization\n","We apply count vectorizer on the train and test datasets "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ithhugZmYM6Q","executionInfo":{"status":"ok","timestamp":1629108752164,"user_tz":-180,"elapsed":616,"user":{"displayName":"Cedric Manouan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnI55k9QnJV4mtok5wRGE36cZcj1DZIjihnSx6=s64","userId":"00242929915047742214"}},"outputId":"9ddd5a76-5ddb-4634-985a-d0b076de1dbd"},"source":["train_X              = train_data['stemmed'].values\n","test_X               = temp_df['stemmed'].values\n","\n","cv                   = CountVectorizer(vocabulary = total_vocab,binary=True)\n","\n","train                = cv.fit_transform(train_X)\n","test                 = cv.transform(test_X)\n","\n","#### Converting the sparse matrices to dense \n","train                = train.todense()\n","test                 = test.todense()\n","print(f'The shape of preprocessed train data matrix is {train.shape} and preprocessed test data matrix is {test.shape}')\n","\n","### label encoding done here\n","train_data['labels'] = train_data['type'].map({'sexual_violence':0,'Harmful_Traditional_practice':1})\n","\n","train_y              = train_data['labels'].values"],"execution_count":27,"outputs":[{"output_type":"stream","text":["The shape of preprocessed train data matrix is (32836, 25) and preprocessed test data matrix is (11046, 25)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"I_2lr0QCYM6Q"},"source":["## Fitting a model on preprocessed data and finetuning\n","* We fit a XGBClassifier model on the preprocessed data \n","* We will now update the previous labels of sexual_violence and Physical_violence with the outcomes attained from the model \n","* The outcomes will be used for second stage classification"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2yZYZFF-YM6R","executionInfo":{"status":"ok","timestamp":1629108752165,"user_tz":-180,"elapsed":8,"user":{"displayName":"Cedric Manouan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnI55k9QnJV4mtok5wRGE36cZcj1DZIjihnSx6=s64","userId":"00242929915047742214"}},"outputId":"1a8588e4-40bd-4b05-da82-27a518df63ca"},"source":["train_data['labels'].value_counts()"],"execution_count":28,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0    32648\n","1      188\n","Name: labels, dtype: int64"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"code","metadata":{"id":"sZqmSmgQYM6R","executionInfo":{"status":"ok","timestamp":1629108753041,"user_tz":-180,"elapsed":880,"user":{"displayName":"Cedric Manouan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnI55k9QnJV4mtok5wRGE36cZcj1DZIjihnSx6=s64","userId":"00242929915047742214"}}},"source":["from xgboost import XGBClassifier\n","model      = XGBClassifier(n_estimators =15,random_state=13,depth = 4,scale_pos_weight=np.sqrt(32648/188))\n","_          = model.fit(train,train_y)"],"execution_count":29,"outputs":[]},{"cell_type":"code","metadata":{"id":"-15Qt-qJYM6R","executionInfo":{"status":"ok","timestamp":1629108753042,"user_tz":-180,"elapsed":29,"user":{"displayName":"Cedric Manouan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnI55k9QnJV4mtok5wRGE36cZcj1DZIjihnSx6=s64","userId":"00242929915047742214"}}},"source":["\n","temp_df['prob']  = model.predict_proba(test)[:,1]\n","\n","findf            = test_df.merge(temp_df,on = ['Tweet_ID','tweet'],how='left')\n"],"execution_count":30,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O30RSkwPYM6S","executionInfo":{"status":"ok","timestamp":1629108753043,"user_tz":-180,"elapsed":28,"user":{"displayName":"Cedric Manouan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnI55k9QnJV4mtok5wRGE36cZcj1DZIjihnSx6=s64","userId":"00242929915047742214"}},"outputId":"e7f215e3-294f-469a-f30e-b3bf46dea947"},"source":["### We update the previous classes with the updated ones \n","findf['fintype'] = findf.apply(lambda z: replace(z['prob'],z['type']),axis=1)\n","findf['flag']    = (findf['type']!=findf['fintype']).astype(int)\n","\n","print(f'Ensuring the shape of outcome dataframe is maintained, we have shape of output data as - {findf.shape}')"],"execution_count":31,"outputs":[{"output_type":"stream","text":["Ensuring the shape of outcome dataframe is maintained, we have shape of output data as - (15581, 11)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0yKika5eYM6S","executionInfo":{"status":"ok","timestamp":1629108753043,"user_tz":-180,"elapsed":23,"user":{"displayName":"Cedric Manouan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnI55k9QnJV4mtok5wRGE36cZcj1DZIjihnSx6=s64","userId":"00242929915047742214"}},"outputId":"2d4652dd-f695-42a0-a4b5-96a9e8f552a4"},"source":["findf['fintype'].value_counts()"],"execution_count":32,"outputs":[{"output_type":"execute_result","data":{"text/plain":["sexual_violence                 7888\n","Physical_violence               3378\n","Harmful_Traditional_practice    3158\n","emotional_violence               659\n","economic_violence                498\n","Name: fintype, dtype: int64"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"markdown","metadata":{"id":"ig_1eVaDYM6S"},"source":["## Reviewing some modifications \n","We will see some mismatches done by BERT model that were modified in this stage of supervised classification based on machine learning "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":669},"id":"Q49e-rXqYM6T","executionInfo":{"status":"ok","timestamp":1629108753044,"user_tz":-180,"elapsed":20,"user":{"displayName":"Cedric Manouan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnI55k9QnJV4mtok5wRGE36cZcj1DZIjihnSx6=s64","userId":"00242929915047742214"}},"outputId":"b9ebef9a-bab0-4fc2-f105-641cecdae420"},"source":["mismatch = findf[findf['flag']==1][['Tweet_ID','tweet','type','fintype','prob']]\n","mismatch = mismatch.sort_values(by = ['prob'])\n","mismatch.head(20)"],"execution_count":33,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Tweet_ID</th>\n","      <th>tweet</th>\n","      <th>type</th>\n","      <th>fintype</th>\n","      <th>prob</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>154</th>\n","      <td>ID_RY9AXKL0</td>\n","      <td>I have seen small girls who fled there Islamic...</td>\n","      <td>sexual_violence</td>\n","      <td>Harmful_Traditional_practice</td>\n","      <td>0.791841</td>\n","    </tr>\n","    <tr>\n","      <th>13609</th>\n","      <td>ID_J9RA4YXQ</td>\n","      <td>How patriarchy affects men: they feel social p...</td>\n","      <td>sexual_violence</td>\n","      <td>Harmful_Traditional_practice</td>\n","      <td>0.791841</td>\n","    </tr>\n","    <tr>\n","      <th>13598</th>\n","      <td>ID_FCTWMV7A</td>\n","      <td>Nope. For a start, even being in the womb as a...</td>\n","      <td>sexual_violence</td>\n","      <td>Harmful_Traditional_practice</td>\n","      <td>0.791841</td>\n","    </tr>\n","    <tr>\n","      <th>13571</th>\n","      <td>ID_COWR29RO</td>\n","      <td>off the top of my head: FGM, that’s pretty sex...</td>\n","      <td>sexual_violence</td>\n","      <td>Harmful_Traditional_practice</td>\n","      <td>0.791841</td>\n","    </tr>\n","    <tr>\n","      <th>11223</th>\n","      <td>ID_QK74NA6Y</td>\n","      <td>it goes as far as having baby girls killed for...</td>\n","      <td>sexual_violence</td>\n","      <td>Harmful_Traditional_practice</td>\n","      <td>0.791841</td>\n","    </tr>\n","    <tr>\n","      <th>10591</th>\n","      <td>ID_O0J4ZQLI</td>\n","      <td>A child is raped and is forced to live with it...</td>\n","      <td>sexual_violence</td>\n","      <td>Harmful_Traditional_practice</td>\n","      <td>0.791841</td>\n","    </tr>\n","    <tr>\n","      <th>10172</th>\n","      <td>ID_NWGRUVUB</td>\n","      <td>Can you hear yourself? This is what misogynist...</td>\n","      <td>sexual_violence</td>\n","      <td>Harmful_Traditional_practice</td>\n","      <td>0.791841</td>\n","    </tr>\n","    <tr>\n","      <th>9641</th>\n","      <td>ID_7TJK7JOO</td>\n","      <td>You look women in the eye who've been raped, t...</td>\n","      <td>sexual_violence</td>\n","      <td>Harmful_Traditional_practice</td>\n","      <td>0.791841</td>\n","    </tr>\n","    <tr>\n","      <th>9220</th>\n","      <td>ID_Y732BCAI</td>\n","      <td>Rape? Trans women are raped. FGM? Let’s talk a...</td>\n","      <td>sexual_violence</td>\n","      <td>Harmful_Traditional_practice</td>\n","      <td>0.791841</td>\n","    </tr>\n","    <tr>\n","      <th>9159</th>\n","      <td>ID_MHPCHC0Z</td>\n","      <td>People who suffer FGM, people who get acid thr...</td>\n","      <td>sexual_violence</td>\n","      <td>Harmful_Traditional_practice</td>\n","      <td>0.791841</td>\n","    </tr>\n","    <tr>\n","      <th>15247</th>\n","      <td>ID_TJ7WO1BT</td>\n","      <td>Women should shut up regarding denial of free ...</td>\n","      <td>sexual_violence</td>\n","      <td>Harmful_Traditional_practice</td>\n","      <td>0.791841</td>\n","    </tr>\n","    <tr>\n","      <th>7789</th>\n","      <td>ID_QGZBRR6C</td>\n","      <td>Iran, the country that charges women for being...</td>\n","      <td>sexual_violence</td>\n","      <td>Harmful_Traditional_practice</td>\n","      <td>0.791841</td>\n","    </tr>\n","    <tr>\n","      <th>7201</th>\n","      <td>ID_QBK9SSO6</td>\n","      <td>People who suffer FGM, people who get aborted ...</td>\n","      <td>sexual_violence</td>\n","      <td>Harmful_Traditional_practice</td>\n","      <td>0.791841</td>\n","    </tr>\n","    <tr>\n","      <th>7028</th>\n","      <td>ID_YFM37QVU</td>\n","      <td>ignores women forced to wear the Burka, FGM, y...</td>\n","      <td>sexual_violence</td>\n","      <td>Harmful_Traditional_practice</td>\n","      <td>0.791841</td>\n","    </tr>\n","    <tr>\n","      <th>6388</th>\n","      <td>ID_7PC9YGSB</td>\n","      <td>All the victims of FGM &amp;amp; forced pregnancy,...</td>\n","      <td>sexual_violence</td>\n","      <td>Harmful_Traditional_practice</td>\n","      <td>0.791841</td>\n","    </tr>\n","    <tr>\n","      <th>6249</th>\n","      <td>ID_DNTS8MJY</td>\n","      <td>So women who have gone through fgm choose to? ...</td>\n","      <td>sexual_violence</td>\n","      <td>Harmful_Traditional_practice</td>\n","      <td>0.791841</td>\n","    </tr>\n","    <tr>\n","      <th>8894</th>\n","      <td>ID_J6PZXTM9</td>\n","      <td>It has too many implications that impact women...</td>\n","      <td>sexual_violence</td>\n","      <td>Harmful_Traditional_practice</td>\n","      <td>0.791841</td>\n","    </tr>\n","    <tr>\n","      <th>2891</th>\n","      <td>ID_AG9D1PTY</td>\n","      <td>So girls and women who are raped, experience f...</td>\n","      <td>sexual_violence</td>\n","      <td>Harmful_Traditional_practice</td>\n","      <td>0.791841</td>\n","    </tr>\n","    <tr>\n","      <th>1873</th>\n","      <td>ID_7AMTOQE8</td>\n","      <td>Women have a slew of issues that TW have not e...</td>\n","      <td>sexual_violence</td>\n","      <td>Harmful_Traditional_practice</td>\n","      <td>0.791841</td>\n","    </tr>\n","    <tr>\n","      <th>1263</th>\n","      <td>ID_E2YOK7OG</td>\n","      <td>Women and girls are being trafficked, raped, m...</td>\n","      <td>sexual_violence</td>\n","      <td>Harmful_Traditional_practice</td>\n","      <td>0.791841</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["          Tweet_ID  ...      prob\n","154    ID_RY9AXKL0  ...  0.791841\n","13609  ID_J9RA4YXQ  ...  0.791841\n","13598  ID_FCTWMV7A  ...  0.791841\n","13571  ID_COWR29RO  ...  0.791841\n","11223  ID_QK74NA6Y  ...  0.791841\n","10591  ID_O0J4ZQLI  ...  0.791841\n","10172  ID_NWGRUVUB  ...  0.791841\n","9641   ID_7TJK7JOO  ...  0.791841\n","9220   ID_Y732BCAI  ...  0.791841\n","9159   ID_MHPCHC0Z  ...  0.791841\n","15247  ID_TJ7WO1BT  ...  0.791841\n","7789   ID_QGZBRR6C  ...  0.791841\n","7201   ID_QBK9SSO6  ...  0.791841\n","7028   ID_YFM37QVU  ...  0.791841\n","6388   ID_7PC9YGSB  ...  0.791841\n","6249   ID_DNTS8MJY  ...  0.791841\n","8894   ID_J6PZXTM9  ...  0.791841\n","2891   ID_AG9D1PTY  ...  0.791841\n","1873   ID_7AMTOQE8  ...  0.791841\n","1263   ID_E2YOK7OG  ...  0.791841\n","\n","[20 rows x 5 columns]"]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"code","metadata":{"id":"5vcC7WZaYM6T","executionInfo":{"status":"ok","timestamp":1629108753045,"user_tz":-180,"elapsed":19,"user":{"displayName":"Cedric Manouan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnI55k9QnJV4mtok5wRGE36cZcj1DZIjihnSx6=s64","userId":"00242929915047742214"}}},"source":["# findf[['Tweet_ID','tweet','type','fintype','prob']].to_csv('9711.csv',index=False)"],"execution_count":34,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":142},"id":"mhYJWlltYM6T","executionInfo":{"status":"ok","timestamp":1629108753046,"user_tz":-180,"elapsed":19,"user":{"displayName":"Cedric Manouan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnI55k9QnJV4mtok5wRGE36cZcj1DZIjihnSx6=s64","userId":"00242929915047742214"}},"outputId":"5c996cd6-fc86-4a07-a254-072d43eaa099"},"source":["final_out = findf[['Tweet_ID','fintype']]\n","final_out = final_out.rename({'fintype':'type'},axis=1)\n","final_out.head(3)"],"execution_count":35,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Tweet_ID</th>\n","      <th>type</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>ID_D9ONL553</td>\n","      <td>sexual_violence</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>ID_263YTILY</td>\n","      <td>emotional_violence</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>ID_62VS6IXC</td>\n","      <td>emotional_violence</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      Tweet_ID                type\n","0  ID_D9ONL553     sexual_violence\n","1  ID_263YTILY  emotional_violence\n","2  ID_62VS6IXC  emotional_violence"]},"metadata":{"tags":[]},"execution_count":35}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G6aAixpJYM6U","executionInfo":{"status":"ok","timestamp":1629108753046,"user_tz":-180,"elapsed":18,"user":{"displayName":"Cedric Manouan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnI55k9QnJV4mtok5wRGE36cZcj1DZIjihnSx6=s64","userId":"00242929915047742214"}},"outputId":"9d8e88eb-92a9-4a1d-d213-d1803d95df08"},"source":["final_out['type'].value_counts()"],"execution_count":36,"outputs":[{"output_type":"execute_result","data":{"text/plain":["sexual_violence                 7888\n","Physical_violence               3378\n","Harmful_Traditional_practice    3158\n","emotional_violence               659\n","economic_violence                498\n","Name: type, dtype: int64"]},"metadata":{"tags":[]},"execution_count":36}]},{"cell_type":"code","metadata":{"id":"HghburRpYM6U","executionInfo":{"status":"ok","timestamp":1629108753608,"user_tz":-180,"elapsed":575,"user":{"displayName":"Cedric Manouan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnI55k9QnJV4mtok5wRGE36cZcj1DZIjihnSx6=s64","userId":"00242929915047742214"}}},"source":["final_out.to_csv('outcome_stage2.csv',index=False)"],"execution_count":37,"outputs":[]},{"cell_type":"code","metadata":{"id":"eBABdsUpYM6U","executionInfo":{"status":"ok","timestamp":1629108753609,"user_tz":-180,"elapsed":8,"user":{"displayName":"Cedric Manouan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnI55k9QnJV4mtok5wRGE36cZcj1DZIjihnSx6=s64","userId":"00242929915047742214"}}},"source":[""],"execution_count":37,"outputs":[]}]}