{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"Stage4_Classification_physical_vs_harmful.ipynb","provenance":[]}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Sq_pw7nLfWA0","executionInfo":{"status":"ok","timestamp":1629109511181,"user_tz":-180,"elapsed":3008,"user":{"displayName":"Cedric Manouan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnI55k9QnJV4mtok5wRGE36cZcj1DZIjihnSx6=s64","userId":"00242929915047742214"}},"outputId":"c90d6c0d-2210-4b22-8f5e-a7b48ce3925c"},"source":["!pip install emoji"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: emoji in /usr/local/lib/python3.7/dist-packages (1.4.2)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e57I4mZdfQnL","executionInfo":{"status":"ok","timestamp":1629109511182,"user_tz":-180,"elapsed":13,"user":{"displayName":"Cedric Manouan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnI55k9QnJV4mtok5wRGE36cZcj1DZIjihnSx6=s64","userId":"00242929915047742214"}},"outputId":"0b4cc8bd-7472-42a0-823a-fe0bc339d845"},"source":["import pandas as pd\n","import numpy as np\n","import re\n","\n","import nltk\n","nltk.download('punkt')\n","from nltk.tokenize import TreebankWordTokenizer\n","from collections import Counter\n","from nltk.tokenize import word_tokenize\n","from nltk.probability import FreqDist\n","import emoji\n","import spacy\n","from operator import itemgetter\n","import itertools\n","from itertools import combinations\n","\n","\n","from nltk.stem.snowball import SnowballStemmer\n","\n","\n","from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n","from tqdm import tqdm\n","tqdm.pandas()\n"],"execution_count":7,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qRSWwHw7fe3Y","executionInfo":{"status":"ok","timestamp":1629109511183,"user_tz":-180,"elapsed":10,"user":{"displayName":"Cedric Manouan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnI55k9QnJV4mtok5wRGE36cZcj1DZIjihnSx6=s64","userId":"00242929915047742214"}},"outputId":"75acb7b3-200c-4a4a-f954-cabf38d70ad0"},"source":["from google.colab import drive\n","drive.mount(\"/content/gdrive\")"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_ZCo8ttpfQnO"},"source":["## Stopwords and spacy model\n","* We extract a list of stopwords from - https://gist.github.com/sebleier/554280 because the available list of NLTK/Spacy stopwords are highly insufficient and incomplete.\n","* We extract the Spacy's english pipeline optimized for CPU and will be using it for stemming and lemmatization tasks\n","* We also instantiate SnowballStemmer to perform stemming after lemmatization is done in later stages of the notebook"]},{"cell_type":"code","metadata":{"id":"xay7gw3xfQnP","executionInfo":{"status":"ok","timestamp":1629109511748,"user_tz":-180,"elapsed":571,"user":{"displayName":"Cedric Manouan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnI55k9QnJV4mtok5wRGE36cZcj1DZIjihnSx6=s64","userId":"00242929915047742214"}}},"source":["#### Stopwords list taken from - https://gist.github.com/sebleier/554280 (reason - the available list of NLTK/ Spacy stopwords are too less and don't cover all possible words)\n","stopwords_list = [\"0o\", \"0s\", \"3a\", \"3b\", \"3d\", \"6b\", \"6o\", \"a\", \"a1\", \"a2\", \"a3\", \"a4\", \"ab\", \"able\", \"about\", \"above\", \"abst\", \"ac\", \"accordance\", \"according\", \"accordingly\", \"across\", \"act\", \"actually\", \"ad\", \"added\", \"adj\", \"ae\", \"af\", \"affected\", \"affecting\", \"affects\", \"after\", \"afterwards\", \"ag\", \"again\", \"against\", \"ah\", \"ain\", \"ain't\", \"aj\", \"al\", \"all\", \"allow\", \"allows\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\", \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"announce\", \"another\", \"any\", \"anybody\", \"anyhow\", \"anymore\", \"anyone\", \"anything\", \"anyway\", \"anyways\", \"anywhere\", \"ao\", \"ap\", \"apart\", \"apparently\", \"appear\", \"appreciate\", \"appropriate\", \"approximately\", \"ar\", \"are\", \"aren\", \"arent\", \"aren't\", \"arise\", \"around\", \"as\", \"a's\", \"aside\", \"ask\", \"asking\", \"associated\", \"at\", \"au\", \"auth\", \"av\", \"available\", \"aw\", \"away\", \"awfully\", \"ax\", \"ay\", \"az\", \"b\", \"b1\", \"b2\", \"b3\", \"ba\", \"back\", \"bc\", \"bd\", \"be\", \"became\", \"because\", \"become\", \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"begin\", \"beginning\", \"beginnings\", \"begins\", \"behind\", \"being\", \"believe\", \"below\", \"beside\", \"besides\", \"best\", \"better\", \"between\", \"beyond\", \"bi\", \"bill\", \"biol\", \"bj\", \"bk\", \"bl\", \"bn\", \"both\", \"bottom\", \"bp\", \"br\", \"brief\", \"briefly\", \"bs\", \"bt\", \"bu\", \"but\", \"bx\", \"by\", \"c\", \"c1\", \"c2\", \"c3\", \"ca\", \"call\", \"came\", \"can\", \"cannot\", \"cant\", \"can't\", \"cause\", \"causes\", \"cc\", \"cd\", \"ce\", \"certain\", \"certainly\", \"cf\", \"cg\", \"ch\", \"changes\", \"ci\", \"cit\", \"cj\", \"cl\", \"clearly\", \"cm\", \"c'mon\", \"cn\", \"co\", \"com\", \"come\", \"comes\", \"con\", \"concerning\", \"consequently\", \"consider\", \"considering\", \"contain\", \"containing\", \"contains\", \"corresponding\", \"could\", \"couldn\", \"couldnt\", \"couldn't\", \"course\", \"cp\", \"cq\", \"cr\", \"cry\", \"cs\", \"c's\", \"ct\", \"cu\", \"currently\", \"cv\", \"cx\", \"cy\", \"cz\", \"d\", \"d2\", \"da\", \"date\", \"dc\", \"dd\", \"de\", \"definitely\", \"describe\", \"described\", \"despite\", \"detail\", \"df\", \"di\", \"did\", \"didn\", \"didn't\", \"different\", \"dj\", \"dk\", \"dl\", \"do\", \"does\", \"doesn\", \"doesn't\", \"doing\", \"don\", \"done\", \"don't\", \"down\", \"downwards\", \"dp\", \"dr\", \"ds\", \"dt\", \"du\", \"due\", \"during\", \"dx\", \"dy\", \"e\", \"e2\", \"e3\", \"ea\", \"each\", \"ec\", \"ed\", \"edu\", \"ee\", \"ef\", \"effect\", \"eg\", \"ei\", \"eight\", \"eighty\", \"either\", \"ej\", \"el\", \"eleven\", \"else\", \"elsewhere\", \"em\", \"empty\", \"en\", \"end\", \"ending\", \"enough\", \"entirely\", \"eo\", \"ep\", \"eq\", \"er\", \"es\", \"especially\", \"est\", \"et\", \"et-al\", \"etc\", \"eu\", \"ev\", \"even\", \"ever\", \"every\", \"everybody\", \"everyone\", \"everything\", \"everywhere\", \"ex\", \"exactly\", \"example\", \"except\", \"ey\", \"f\", \"f2\", \"fa\", \"far\", \"fc\", \"few\", \"ff\", \"fi\", \"fifteen\", \"fifth\", \"fify\", \"fill\", \"find\", \"fire\", \"first\", \"five\", \"fix\", \"fj\", \"fl\", \"fn\", \"fo\", \"followed\", \"following\", \"follows\", \"for\", \"former\", \"formerly\", \"forth\", \"forty\", \"found\", \"four\", \"fr\", \"from\", \"front\", \"fs\", \"ft\", \"fu\", \"full\", \"further\", \"furthermore\", \"fy\", \"g\", \"ga\", \"gave\", \"ge\", \"get\", \"gets\", \"getting\", \"gi\", \"give\", \"given\", \"gives\", \"giving\", \"gj\", \"gl\", \"go\", \"goes\", \"going\", \"gone\", \"got\", \"gotten\", \"gr\", \"greetings\", \"gs\", \"gy\", \"h\", \"h2\", \"h3\", \"had\", \"hadn\", \"hadn't\", \"happens\", \"hardly\", \"has\", \"hasn\", \"hasnt\", \"hasn't\", \"have\", \"haven\", \"haven't\", \"having\", \"he\", \"hed\", \"he'd\", \"he'll\", \"hello\", \"help\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"heres\", \"here's\", \"hereupon\", \"hers\", \"herself\", \"hes\", \"he's\", \"hh\", \"hi\", \"hid\", \"him\", \"himself\", \"his\", \"hither\", \"hj\", \"ho\", \"home\", \"hopefully\", \"how\", \"howbeit\", \"however\", \"how's\", \"hr\", \"hs\", \"http\", \"hu\", \"hundred\", \"hy\", \"i\", \"i2\", \"i3\", \"i4\", \"i6\", \"i7\", \"i8\", \"ia\", \"ib\", \"ibid\", \"ic\", \"id\", \"i'd\", \"ie\", \"if\", \"ig\", \"ignored\", \"ih\", \"ii\", \"ij\", \"il\", \"i'll\", \"im\", \"i'm\", \"immediate\", \"immediately\", \"importance\", \"important\", \"in\", \"inasmuch\", \"inc\", \"indeed\", \"index\", \"indicate\", \"indicated\", \"indicates\", \"information\", \"inner\", \"insofar\", \"instead\", \"interest\", \"into\", \"invention\", \"inward\", \"io\", \"ip\", \"iq\", \"ir\", \"is\", \"isn\", \"isn't\", \"it\", \"itd\", \"it'd\", \"it'll\", \"its\", \"it's\", \"itself\", \"iv\", \"i've\", \"ix\", \"iy\", \"iz\", \"j\", \"jj\", \"jr\", \"js\", \"jt\", \"ju\", \"just\", \"k\", \"ke\", \"keep\", \"keeps\", \"kept\", \"kg\", \"kj\", \"km\", \"know\", \"known\", \"knows\", \"ko\", \"l\", \"l2\", \"la\", \"largely\", \"last\", \"lately\", \"later\", \"latter\", \"latterly\", \"lb\", \"lc\", \"le\", \"least\", \"les\", \"less\", \"lest\", \"let\", \"lets\", \"let's\", \"lf\", \"like\", \"liked\", \"likely\", \"line\", \"little\", \"lj\", \"ll\", \"ll\", \"ln\", \"lo\", \"look\", \"looking\", \"looks\", \"los\", \"lr\", \"ls\", \"lt\", \"ltd\", \"m\", \"m2\", \"ma\", \"made\", \"mainly\", \"make\", \"makes\", \"many\", \"may\", \"maybe\", \"me\", \"mean\", \"means\", \"meantime\", \"meanwhile\", \"merely\", \"mg\", \"might\", \"mightn\", \"mightn't\", \"mill\", \"million\", \"mine\", \"miss\", \"ml\", \"mn\", \"mo\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"mr\", \"mrs\", \"ms\", \"mt\", \"mu\", \"much\", \"mug\", \"must\", \"mustn\", \"mustn't\", \"my\", \"myself\", \"n\", \"n2\", \"na\", \"name\", \"namely\", \"nay\", \"nc\", \"nd\", \"ne\", \"near\", \"nearly\", \"necessarily\", \"necessary\", \"need\", \"needn\", \"needn't\", \"needs\", \"neither\", \"never\", \"nevertheless\", \"new\", \"next\", \"ng\", \"ni\", \"nine\", \"ninety\", \"nj\", \"nl\", \"nn\", \"no\", \"nobody\", \"non\", \"none\", \"nonetheless\", \"noone\", \"nor\", \"normally\", \"nos\", \"not\", \"noted\", \"nothing\", \"novel\", \"now\", \"nowhere\", \"nr\", \"ns\", \"nt\", \"ny\", \"o\", \"oa\", \"ob\", \"obtain\", \"obtained\", \"obviously\", \"oc\", \"od\", \"of\", \"off\", \"often\", \"og\", \"oh\", \"oi\", \"oj\", \"ok\", \"okay\", \"ol\", \"old\", \"om\", \"omitted\", \"on\", \"once\", \"one\", \"ones\", \"only\", \"onto\", \"oo\", \"op\", \"oq\", \"or\", \"ord\", \"os\", \"ot\", \"other\", \"others\", \"otherwise\", \"ou\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"outside\", \"over\", \"overall\", \"ow\", \"owing\", \"own\", \"ox\", \"oz\", \"p\", \"p1\", \"p2\", \"p3\", \"page\", \"pagecount\", \"pages\", \"par\", \"part\", \"particular\", \"particularly\", \"pas\", \"past\", \"pc\", \"pd\", \"pe\", \"per\", \"perhaps\", \"pf\", \"ph\", \"pi\", \"pj\", \"pk\", \"pl\", \"placed\", \"please\", \"plus\", \"pm\", \"pn\", \"po\", \"poorly\", \"possible\", \"possibly\", \"potentially\", \"pp\", \"pq\", \"pr\", \"predominantly\", \"present\", \"presumably\", \"previously\", \"primarily\", \"probably\", \"promptly\", \"proud\", \"provides\", \"ps\", \"pt\", \"pu\", \"put\", \"py\", \"q\", \"qj\", \"qu\", \"que\", \"quickly\", \"quite\", \"qv\", \"r\", \"r2\", \"ra\", \"ran\", \"rather\", \"rc\", \"rd\", \"re\", \"readily\", \"really\", \"reasonably\", \"recent\", \"recently\", \"ref\", \"refs\", \"regarding\", \"regardless\", \"regards\", \"related\", \"relatively\", \"research\", \"research-articl\", \"respectively\", \"resulted\", \"resulting\", \"results\", \"rf\", \"rh\", \"ri\", \"right\", \"rj\", \"rl\", \"rm\", \"rn\", \"ro\", \"rq\", \"rr\", \"rs\", \"rt\", \"ru\", \"run\", \"rv\", \"ry\", \"s\", \"s2\", \"sa\", \"said\", \"same\", \"saw\", \"say\", \"saying\", \"says\", \"sc\", \"sd\", \"se\", \"sec\", \"second\", \"secondly\", \"section\", \"see\", \"seeing\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"seen\", \"self\", \"selves\", \"sensible\", \"sent\", \"serious\", \"seriously\", \"seven\", \"several\", \"sf\", \"shall\", \"shan\", \"shan't\", \"she\", \"shed\", \"she'd\", \"she'll\", \"shes\", \"she's\", \"should\", \"shouldn\", \"shouldn't\", \"should've\", \"show\", \"showed\", \"shown\", \"showns\", \"shows\", \"si\", \"side\", \"significant\", \"significantly\", \"similar\", \"similarly\", \"since\", \"sincere\", \"six\", \"sixty\", \"sj\", \"sl\", \"slightly\", \"sm\", \"sn\", \"so\", \"some\", \"somebody\", \"somehow\", \"someone\", \"somethan\", \"something\", \"sometime\", \"sometimes\", \"somewhat\", \"somewhere\", \"soon\", \"sorry\", \"sp\", \"specifically\", \"specified\", \"specify\", \"specifying\", \"sq\", \"sr\", \"ss\", \"st\", \"still\", \"stop\", \"strongly\", \"sub\", \"substantially\", \"successfully\", \"such\", \"sufficiently\", \"suggest\", \"sup\", \"sure\", \"sy\", \"system\", \"sz\", \"t\", \"t1\", \"t2\", \"t3\", \"take\", \"taken\", \"taking\", \"tb\", \"tc\", \"td\", \"te\", \"tell\", \"ten\", \"tends\", \"tf\", \"th\", \"than\", \"thank\", \"thanks\", \"thanx\", \"that\", \"that'll\", \"thats\", \"that's\", \"that've\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"thered\", \"therefore\", \"therein\", \"there'll\", \"thereof\", \"therere\", \"theres\", \"there's\", \"thereto\", \"thereupon\", \"there've\", \"these\", \"they\", \"theyd\", \"they'd\", \"they'll\", \"theyre\", \"they're\", \"they've\", \"thickv\", \"thin\", \"think\", \"third\", \"this\", \"thorough\", \"thoroughly\", \"those\", \"thou\", \"though\", \"thoughh\", \"thousand\", \"three\", \"throug\", \"through\", \"throughout\", \"thru\", \"thus\", \"ti\", \"til\", \"tip\", \"tj\", \"tl\", \"tm\", \"tn\", \"to\", \"together\", \"too\", \"took\", \"top\", \"toward\", \"towards\", \"tp\", \"tq\", \"tr\", \"tried\", \"tries\", \"truly\", \"try\", \"trying\", \"ts\", \"t's\", \"tt\", \"tv\", \"twelve\", \"twenty\", \"twice\", \"two\", \"tx\", \"u\", \"u201d\", \"ue\", \"ui\", \"uj\", \"uk\", \"um\", \"un\", \"under\", \"unfortunately\", \"unless\", \"unlike\", \"unlikely\", \"until\", \"unto\", \"uo\", \"up\", \"upon\", \"ups\", \"ur\", \"us\", \"use\", \"used\", \"useful\", \"usefully\", \"usefulness\", \"uses\", \"using\", \"usually\", \"ut\", \"v\", \"va\", \"value\", \"various\", \"vd\", \"ve\", \"ve\", \"very\", \"via\", \"viz\", \"vj\", \"vo\", \"vol\", \"vols\", \"volumtype\", \"vq\", \"vs\", \"vt\", \"vu\", \"w\", \"wa\", \"want\", \"wants\", \"was\", \"wasn\", \"wasnt\", \"wasn't\", \"way\", \"we\", \"wed\", \"we'd\", \"welcome\", \"well\", \"we'll\", \"well-b\", \"went\", \"were\", \"we're\", \"weren\", \"werent\", \"weren't\", \"we've\", \"what\", \"whatever\", \"what'll\", \"whats\", \"what's\", \"when\", \"whence\", \"whenever\", \"when's\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"wheres\", \"where's\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whim\", \"whither\", \"who\", \"whod\", \"whoever\", \"whole\", \"who'll\", \"whom\", \"whomever\", \"whos\", \"who's\", \"whose\", \"why\", \"why's\", \"wi\", \"widely\", \"will\", \"willing\", \"wish\", \"with\", \"within\", \"without\", \"wo\", \"won\", \"wonder\", \"wont\", \"won't\", \"words\", \"world\", \"would\", \"wouldn\", \"wouldnt\", \"wouldn't\", \"www\", \"x\", \"x1\", \"x2\", \"x3\", \"xf\", \"xi\", \"xj\", \"xk\", \"xl\", \"xn\", \"xo\", \"xs\", \"xt\", \"xv\", \"xx\", \"y\", \"y2\", \"yes\", \"yet\", \"yj\", \"yl\", \"you\", \"youd\", \"you'd\", \"you'll\", \"your\", \"youre\", \"you're\", \"yours\", \"yourself\", \"yourselves\", \"you've\", \"yr\", \"ys\", \"yt\", \"z\", \"zero\", \"zi\", \"zz\",]\n","\n","nlp        = spacy.load(\"en_core_web_sm\")\n","\n","stemmer = SnowballStemmer(language='english')"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ihjLHcfbfQnX"},"source":["## Custom preprocessing functions for Vocabulary extraction\n","We declare basic functions to perform tasks like removal of stopwords, stemming, lemmatization in batches, removal and emojis etc. "]},{"cell_type":"code","metadata":{"id":"MoheJpokfQnY","executionInfo":{"status":"ok","timestamp":1629109511749,"user_tz":-180,"elapsed":9,"user":{"displayName":"Cedric Manouan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnI55k9QnJV4mtok5wRGE36cZcj1DZIjihnSx6=s64","userId":"00242929915047742214"}}},"source":["def remove_stopwords(text):\n","    '''\n","      Removes stopwords from the text\n","    '''\n","\n","    text_split   = text.split()\n","\n","    text_list    = [word for word in text_split if not word in stopwords_list]\n","\n","    return ' '.join(text_list)\n","\n","def stemming_perform(text):\n","    '''\n","      Performs lemmatization in batches\n","    '''\n","    \n","    token = text.split()\n","    stemmed_text = []\n","    for tok in token:\n","        stemmed_text.append(stemmer.stem(tok))\n","    return ' '.join(stemmed_text)\n","\n","\n","def cleaning_text(text):\n","    '''\n","      Operations performed:- \n","      1. Converting the entire text to lowercase\n","      2. Removal of punctuations from the text\n","      3. Removal of numbers from the text\n","    '''\n","\n","    text = text.lower()\n","\n","    text = re.sub(r'[^\\w\\s]', ' ', text)\n","    text = re.sub(r'\\d+', '', text)\n","    text = remove_stopwords(text)\n","\n","    text = text.replace('amp','') ## this was actually ampersand which kept coming in frequently that made no sense\n","    return text\n","\n","\n","def give_emoji_free_text(text):\n","    ''' \n","      Removes all possible emojis from the text (because our text is basically tweets that can have emoijs).\n","      The input is a text and the output is emoji free text\n","    '''\n","    allchars = [str for str in text.encode().decode('utf-8')]\n","    emoji_list = [c for c in allchars if c in emoji.UNICODE_EMOJI]\n","    clean_text = ' '.join([str for str in text.encode().decode('utf-8').split() if not any(i in str for i in emoji_list)])\n","    return clean_text\n","\n","\n","def preprocess_custom(text):\n","    text = give_emoji_free_text(text)\n","    text = cleaning_text(text)\n","\n","    return text\n","\n","def lemmatize_batches(docs):\n","    '''\n","      Performs lemmatization in batches\n","    '''\n","    lemmatized_text = []\n","\n","    for doc in nlp.pipe(docs, batch_size=256, n_process=3,disable=[\"parser\", \"ner\"]):\n","        lemmatized_text.append(str(' '.join([token.lemma_ for token in doc])))\n","    return lemmatized_text\n","\n","\n","def lemmatize_df(df):\n","    lemmatized = []\n","    for i in tqdm(range(0,df.shape[0],1000)):\n","        z = lemmatize_batches(df['preprocessed_text'].iloc[i:i+1000])\n","        lemmatized.extend(z)\n","    df['lemmatize'] = lemmatized\n","    return df\n","\n","\n","\n","def return_frequency_dict_sorted(df):\n","    sent = df['stemmed'].values\n","    sent = ' '.join(sent)\n","\n","    fdist = FreqDist()\n","    \n","    for word in word_tokenize(sent):\n","        fdist[word.lower()] += 1\n","        \n","    return sorted(fdist.items(), key=lambda x: x[1], reverse=True)\n","\n","def replace(prob,x2,thresh1,thresh2):\n","    \n","    if str(prob)!='nan':\n","        if ((prob>thresh1) and (prob<thresh2)):\n","            return 'Harmful_Traditional_practice'\n","        else:\n","            return x2\n","    else:\n","        return x2\n"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"15hWA0g8fQnY"},"source":["## Vocab extraction and Preprocessing Stage\n","We will preprocess the text in two stages\n","1. Here we will consider only the 'Harmful_traditional practice' class and find the vocabulary corresponding to it\n","2. Next we will consider the class 'sexual_violence' and find the vocabulary corresponding to it.\n","3. Out combined vocabulary will be the vocabulary from both the stages"]},{"cell_type":"markdown","metadata":{"id":"9_3JJUDkfQnY"},"source":["####  Our input data will be the Data from previous stage"]},{"cell_type":"markdown","metadata":{"id":"egro_s-GfQnZ"},"source":["## Text Preprocessing for Vocab extraction for class 'Harmful Traditional Practices'\n","We preprocess the test data tweets and then find the frequency occurance of the preprocessed words from the dataset. We will now select top 20 words from the test data tweets which will be used as a vocabulary for building our model. We consider only the class ' Harmful_Traditional_practice' "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xx2cELUsfQnZ","executionInfo":{"status":"ok","timestamp":1629109554719,"user_tz":-180,"elapsed":11505,"user":{"displayName":"Cedric Manouan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnI55k9QnJV4mtok5wRGE36cZcj1DZIjihnSx6=s64","userId":"00242929915047742214"}},"outputId":"f399f03a-8676-4397-d44d-6265a200f045"},"source":["subdf   = pd.read_csv('/content/gdrive/MyDrive/Hackathon/outcome_stage3.csv')\n","\n","subdf   = subdf[(subdf['type']=='Harmful_Traditional_practice')]\n","\n","print(f'The shape of subsetted data with subsetting on Harmful_Traditional_practice will be {subdf.shape}')\n","\n","test_df = pd.read_csv('/content/gdrive/MyDrive/Hackathon/Test.csv')\n","\n","temp_df = subdf.merge(test_df,on=['Tweet_ID'],how='left')\n","\n","_       = temp_df.rename(columns={'type':'type1'},inplace=True)\n","\n","print(f'The shape of data after assigning the labels attained from BERT to test is {temp_df.shape}')\n","\n","temp_df['preprocessed_text'] = temp_df['tweet'].progress_apply(preprocess_custom)\n","temp_df                      = lemmatize_df(temp_df)\n","temp_df['stemmed']           = temp_df['lemmatize'].progress_apply(stemming_perform)\n","sorted_freq_dict             = return_frequency_dict_sorted(temp_df)\n","\n","threshold2consider           = 5\n","vocab_htp                    = [word[0] for word in sorted_freq_dict[:threshold2consider]]\n"],"execution_count":12,"outputs":[{"output_type":"stream","text":["The shape of subsetted data with subsetting on Harmful_Traditional_practice will be (3158, 2)\n","The shape of data after assigning the labels attained from BERT to test is (3158, 3)\n"],"name":"stdout"},{"output_type":"stream","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3158/3158 [00:01<00:00, 1606.65it/s]\n","100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:06<00:00,  1.66s/it]\n","100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3158/3158 [00:00<00:00, 3512.10it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":376},"id":"Q2p_hiX5fQna","executionInfo":{"status":"ok","timestamp":1629109554727,"user_tz":-180,"elapsed":24,"user":{"displayName":"Cedric Manouan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnI55k9QnJV4mtok5wRGE36cZcj1DZIjihnSx6=s64","userId":"00242929915047742214"}},"outputId":"f74ae8eb-b259-4cc3-bae1-00d69b9b01aa"},"source":["print('The top 10 words (preprocessed) based on the frequency distribution and their relative frequencies are given below')\n","pd.DataFrame(sorted_freq_dict,columns = ['word','Frequency']).head(10)"],"execution_count":13,"outputs":[{"output_type":"stream","text":["The top 10 words (preprocessed) based on the frequency distribution and their relative frequencies are given below\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>word</th>\n","      <th>Frequency</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>forc</td>\n","      <td>3374</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>fgm</td>\n","      <td>2861</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>marriag</td>\n","      <td>2415</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>woman</td>\n","      <td>1513</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>child</td>\n","      <td>1192</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>girl</td>\n","      <td>1187</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>femal</td>\n","      <td>593</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>rape</td>\n","      <td>522</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>sex</td>\n","      <td>485</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>violenc</td>\n","      <td>465</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      word  Frequency\n","0     forc       3374\n","1      fgm       2861\n","2  marriag       2415\n","3    woman       1513\n","4    child       1192\n","5     girl       1187\n","6    femal        593\n","7     rape        522\n","8      sex        485\n","9  violenc        465"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"id":"PO2-Hn8ufQnb","executionInfo":{"status":"ok","timestamp":1629109554728,"user_tz":-180,"elapsed":21,"user":{"displayName":"Cedric Manouan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnI55k9QnJV4mtok5wRGE36cZcj1DZIjihnSx6=s64","userId":"00242929915047742214"}}},"source":[""],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nAr4y6LkfQnc"},"source":["# Classification model on the vocabulary attained\n","We will now preprocess our train and BERT Classified physical and sexual data to finetune it <br>\n","To perform that our test data was already preprocessed in the previous step. We will now preprocess our train dataset"]},{"cell_type":"markdown","metadata":{"id":"9YpkzTV-fQnc"},"source":["## Text Preprocessing on Train data\n","We subset our train data having only Physical Violence and Harmful Traditional Practices labels and perform text preprocessing on train and test data"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8NF2fI-2fQnc","executionInfo":{"status":"ok","timestamp":1629109594788,"user_tz":-180,"elapsed":23395,"user":{"displayName":"Cedric Manouan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnI55k9QnJV4mtok5wRGE36cZcj1DZIjihnSx6=s64","userId":"00242929915047742214"}},"outputId":"f1581f20-87c3-4f9c-8c8a-a37fbe7f3b59"},"source":["test_df                         = pd.read_csv('/content/gdrive/MyDrive/Hackathon/Test.csv')\n","subdf                           = pd.read_csv('/content/gdrive/MyDrive/Hackathon/outcome_stage3.csv')\n","\n","test_df                         = test_df.merge(subdf,on=['Tweet_ID'],how='left')\n","\n","\n","subdf                           = subdf[(subdf['type']=='Harmful_Traditional_practice') | (subdf['type']=='Physical_violence')]\n","\n","\n","temp_df                         = subdf.merge(test_df,on=['Tweet_ID'],how='left')\n","\n","_                               = temp_df.rename(columns={'type':'type1'},inplace=True)\n","\n","temp_df['preprocessed_text']    = temp_df['tweet'].progress_apply(preprocess_custom)\n","temp_df                         = lemmatize_df(temp_df)\n","temp_df['stemmed']              = temp_df['lemmatize'].progress_apply(stemming_perform)\n","\n","\n","train_df                        = pd.read_csv('/content/gdrive/MyDrive/Hackathon/Train.csv')\n","train_data                      = train_df[train_df['type'].isin(['Harmful_Traditional_practice','Physical_violence'])]\n","train_data                      = train_data.reset_index()\n","\n","train_data['preprocessed_text'] = train_data['tweet'].progress_apply(preprocess_custom)\n","train_data                      = lemmatize_df(train_data)\n","train_data['stemmed']           = train_data['lemmatize'].progress_apply(stemming_perform)\n","\n","print(test_df.shape,train_df.shape,temp_df.shape)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6536/6536 [00:03<00:00, 1890.06it/s]\n","100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:07<00:00,  1.08s/it]\n","100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6536/6536 [00:01<00:00, 4373.26it/s]\n","100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6134/6134 [00:02<00:00, 2621.82it/s]\n","100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:06<00:00,  1.12it/s]\n","100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6134/6134 [00:00<00:00, 6750.89it/s]"],"name":"stderr"},{"output_type":"stream","text":["(15581, 3) (39650, 3) (6536, 7)\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"N_yLb1VvfQnc"},"source":["## Count vectorization with binarization\n","We apply count vectorizer on the train and test datasets "]},{"cell_type":"code","metadata":{"id":"-8mODpWxfQnd","executionInfo":{"status":"ok","timestamp":1629109594789,"user_tz":-180,"elapsed":19,"user":{"displayName":"Cedric Manouan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnI55k9QnJV4mtok5wRGE36cZcj1DZIjihnSx6=s64","userId":"00242929915047742214"}}},"source":["sample               = train_data[train_data['type']=='Physical_violence'].sample(500)\n","sample               = sample.append(train_data[train_data['type']!='Physical_violence'])\n","sample               = sample.reset_index(drop=True)\n","# sample = train_data.copy()"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xgbcb4lofQnd","executionInfo":{"status":"ok","timestamp":1629109594789,"user_tz":-180,"elapsed":18,"user":{"displayName":"Cedric Manouan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnI55k9QnJV4mtok5wRGE36cZcj1DZIjihnSx6=s64","userId":"00242929915047742214"}},"outputId":"4be1e047-7ec0-42ef-bc80-2e921566283a"},"source":["train_X              = sample['stemmed'].values\n","test_X               = temp_df['stemmed'].values\n","\n","cv                   = CountVectorizer(vocabulary = vocab_htp,binary=True)\n","\n","train                = cv.fit_transform(train_X)\n","test                 = cv.transform(test_X)\n","\n","#### Converting the sparse matrices to dense \n","train                = train.todense()\n","test                 = test.todense()\n","print(f'The shape of preprocessed train data matrix is {train.shape} and preprocessed test data matrix is {test.shape}')\n","\n","### label encoding done here\n","sample['labels']     = sample['type'].map({'Physical_violence':0,'Harmful_Traditional_practice':1})\n","\n","train_y              = sample['labels'].values"],"execution_count":17,"outputs":[{"output_type":"stream","text":["The shape of preprocessed train data matrix is (688, 5) and preprocessed test data matrix is (6536, 5)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"gPkIWpXEfQnd"},"source":["## Fitting a model on preprocessed data and finetuning\n","* We fit a XGBClassifier model on the preprocessed data \n","* We will now update the previous labels of sexual_violence and Physical_violence with the outcomes attained from the model \n","* The outcomes will be used for second stage classification"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2oYVbtykfQnd","executionInfo":{"status":"ok","timestamp":1629109594789,"user_tz":-180,"elapsed":15,"user":{"displayName":"Cedric Manouan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnI55k9QnJV4mtok5wRGE36cZcj1DZIjihnSx6=s64","userId":"00242929915047742214"}},"outputId":"6c6d0cdf-36c8-457a-db8d-d7e34ee4a9f3"},"source":["sample['labels'].value_counts()"],"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0    500\n","1    188\n","Name: labels, dtype: int64"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"BRqtQWQMfQnd","executionInfo":{"status":"ok","timestamp":1629109594790,"user_tz":-180,"elapsed":13,"user":{"displayName":"Cedric Manouan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnI55k9QnJV4mtok5wRGE36cZcj1DZIjihnSx6=s64","userId":"00242929915047742214"}}},"source":[""],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"xRj_QsMbfQnd","executionInfo":{"status":"ok","timestamp":1629109595559,"user_tz":-180,"elapsed":782,"user":{"displayName":"Cedric Manouan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnI55k9QnJV4mtok5wRGE36cZcj1DZIjihnSx6=s64","userId":"00242929915047742214"}}},"source":["from xgboost import XGBClassifier\n","model      = XGBClassifier(n_estimators =40,random_state=13,max_depth = 2,scale_pos_weight=np.sqrt(500/188))\n","_          = model.fit(train,train_y)"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"bOuo2zCHfQne","executionInfo":{"status":"ok","timestamp":1629109595560,"user_tz":-180,"elapsed":19,"user":{"displayName":"Cedric Manouan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnI55k9QnJV4mtok5wRGE36cZcj1DZIjihnSx6=s64","userId":"00242929915047742214"}}},"source":[""],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"Tm206ZslfQne","executionInfo":{"status":"ok","timestamp":1629109595561,"user_tz":-180,"elapsed":18,"user":{"displayName":"Cedric Manouan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnI55k9QnJV4mtok5wRGE36cZcj1DZIjihnSx6=s64","userId":"00242929915047742214"}}},"source":["\n","temp_df['prob']  = model.predict_proba(test)[:,1]\n","\n","findf            = test_df.merge(temp_df,on = ['Tweet_ID','tweet'],how='left')\n"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YKBGncckfQnf","executionInfo":{"status":"ok","timestamp":1629109595562,"user_tz":-180,"elapsed":18,"user":{"displayName":"Cedric Manouan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnI55k9QnJV4mtok5wRGE36cZcj1DZIjihnSx6=s64","userId":"00242929915047742214"}},"outputId":"d18c0979-ae34-4a4c-e573-cfe266c415f6"},"source":["### We update the previous classes with the updated ones \n","findf['fintype'] = findf.apply(lambda z: replace(z['prob'],z['type'],0.95,0.99),axis=1)\n","findf['flag']    = (findf['type']!=findf['fintype']).astype(int)\n","\n","print(f'Ensuring the shape of outcome dataframe is maintained, we have shape of output data as - {findf.shape}')"],"execution_count":21,"outputs":[{"output_type":"stream","text":["Ensuring the shape of outcome dataframe is maintained, we have shape of output data as - (15581, 11)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r23IlS5nfQnf","executionInfo":{"status":"ok","timestamp":1629109595563,"user_tz":-180,"elapsed":16,"user":{"displayName":"Cedric Manouan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnI55k9QnJV4mtok5wRGE36cZcj1DZIjihnSx6=s64","userId":"00242929915047742214"}},"outputId":"09e1fa35-3d13-4b70-be2e-f0c7f734d472"},"source":["findf['fintype'].value_counts()"],"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["sexual_violence                 8101\n","Physical_violence               3360\n","Harmful_Traditional_practice    3176\n","economic_violence                498\n","emotional_violence               446\n","Name: fintype, dtype: int64"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"markdown","metadata":{"id":"HnJ8dRDPfQnf"},"source":["## Reviewing some modifications \n","We will see some mismatches done by BERT model that were modified in this stage of supervised classification based on machine learning "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":607},"id":"auxJXc66fQnf","executionInfo":{"status":"ok","timestamp":1629109595565,"user_tz":-180,"elapsed":14,"user":{"displayName":"Cedric Manouan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnI55k9QnJV4mtok5wRGE36cZcj1DZIjihnSx6=s64","userId":"00242929915047742214"}},"outputId":"c1a3aedf-228a-4908-957a-55c6aa445371"},"source":["mismatch = findf[findf['flag']==1][['Tweet_ID','tweet','type','fintype','prob']]\n","mismatch = mismatch.sort_values(by = ['prob'])\n","mismatch.head(20)"],"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Tweet_ID</th>\n","      <th>tweet</th>\n","      <th>type</th>\n","      <th>fintype</th>\n","      <th>prob</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>4779</th>\n","      <td>ID_LHF3OKZS</td>\n","      <td>Welcome to the pearl of Africa, where a adult ...</td>\n","      <td>Physical_violence</td>\n","      <td>Harmful_Traditional_practice</td>\n","      <td>0.958514</td>\n","    </tr>\n","    <tr>\n","      <th>145</th>\n","      <td>ID_NVNOM0GA</td>\n","      <td>Halftime stats: WLU:  -Six three-pointers made...</td>\n","      <td>Physical_violence</td>\n","      <td>Harmful_Traditional_practice</td>\n","      <td>0.978076</td>\n","    </tr>\n","    <tr>\n","      <th>13846</th>\n","      <td>ID_677MK7CR</td>\n","      <td>Halftime Stats: D&amp;amp;E: - Shooting 42% from t...</td>\n","      <td>Physical_violence</td>\n","      <td>Harmful_Traditional_practice</td>\n","      <td>0.978076</td>\n","    </tr>\n","    <tr>\n","      <th>13681</th>\n","      <td>ID_UQOQMCRH</td>\n","      <td>Houston with a dominant 1H doubling up the Bea...</td>\n","      <td>Physical_violence</td>\n","      <td>Harmful_Traditional_practice</td>\n","      <td>0.978076</td>\n","    </tr>\n","    <tr>\n","      <th>12834</th>\n","      <td>ID_LDY2322H</td>\n","      <td>Draymond Green had ZERO FGM, but his defensive...</td>\n","      <td>Physical_violence</td>\n","      <td>Harmful_Traditional_practice</td>\n","      <td>0.978076</td>\n","    </tr>\n","    <tr>\n","      <th>12501</th>\n","      <td>ID_MSFPYJKH</td>\n","      <td>Highlights of Yi Jianlian on Guangdong's winni...</td>\n","      <td>Physical_violence</td>\n","      <td>Harmful_Traditional_practice</td>\n","      <td>0.978076</td>\n","    </tr>\n","    <tr>\n","      <th>11947</th>\n","      <td>ID_4IMPJKVT</td>\n","      <td>Here's what Barret Peery did in his FIRST year...</td>\n","      <td>Physical_violence</td>\n","      <td>Harmful_Traditional_practice</td>\n","      <td>0.978076</td>\n","    </tr>\n","    <tr>\n","      <th>11437</th>\n","      <td>ID_SU11V6HP</td>\n","      <td>David Nwaba ranks in the 88th percentile as an...</td>\n","      <td>Physical_violence</td>\n","      <td>Harmful_Traditional_practice</td>\n","      <td>0.978076</td>\n","    </tr>\n","    <tr>\n","      <th>6541</th>\n","      <td>ID_REQW5FGD</td>\n","      <td>Tonight is the lowest-scoring first half of th...</td>\n","      <td>Physical_violence</td>\n","      <td>Harmful_Traditional_practice</td>\n","      <td>0.978076</td>\n","    </tr>\n","    <tr>\n","      <th>5636</th>\n","      <td>ID_PXZ033U0</td>\n","      <td>forced to rely on cuts and split action plays ...</td>\n","      <td>Physical_violence</td>\n","      <td>Harmful_Traditional_practice</td>\n","      <td>0.978076</td>\n","    </tr>\n","    <tr>\n","      <th>4806</th>\n","      <td>ID_XBLW1E81</td>\n","      <td>Nonlinear free and forced vibrations of fracti...</td>\n","      <td>Physical_violence</td>\n","      <td>Harmful_Traditional_practice</td>\n","      <td>0.978076</td>\n","    </tr>\n","    <tr>\n","      <th>4502</th>\n","      <td>ID_VWJUWBVN</td>\n","      <td>ðŸ˜‚ðŸ˜‚ðŸ˜‚wamesema ni forced fgmðŸ˜­</td>\n","      <td>Physical_violence</td>\n","      <td>Harmful_Traditional_practice</td>\n","      <td>0.978076</td>\n","    </tr>\n","    <tr>\n","      <th>4494</th>\n","      <td>ID_MKIY3URD</td>\n","      <td>Contributions such as:  â—¾FGM â—¾GROOMING GANGS â—¾...</td>\n","      <td>Physical_violence</td>\n","      <td>Harmful_Traditional_practice</td>\n","      <td>0.978076</td>\n","    </tr>\n","    <tr>\n","      <th>4316</th>\n","      <td>ID_D1A6K9C8</td>\n","      <td>Raptors' 1st-quarter O: - Scored 37 points on ...</td>\n","      <td>Physical_violence</td>\n","      <td>Harmful_Traditional_practice</td>\n","      <td>0.978076</td>\n","    </tr>\n","    <tr>\n","      <th>2386</th>\n","      <td>ID_VC7PFT39</td>\n","      <td>Joel Embiid (40 PTS, 19 REB) forced OT in the ...</td>\n","      <td>Physical_violence</td>\n","      <td>Harmful_Traditional_practice</td>\n","      <td>0.978076</td>\n","    </tr>\n","    <tr>\n","      <th>306</th>\n","      <td>ID_DBYM2793</td>\n","      <td>Luka Doncic had a ridiculously efficient shoot...</td>\n","      <td>Physical_violence</td>\n","      <td>Harmful_Traditional_practice</td>\n","      <td>0.978076</td>\n","    </tr>\n","    <tr>\n","      <th>14167</th>\n","      <td>ID_LAD6J2JR</td>\n","      <td>55 points 10 assists 6 rebounds 17 FGM 12 3PT ...</td>\n","      <td>Physical_violence</td>\n","      <td>Harmful_Traditional_practice</td>\n","      <td>0.978076</td>\n","    </tr>\n","    <tr>\n","      <th>14938</th>\n","      <td>ID_Y9K08D54</td>\n","      <td>HALF: 32, Western Michigan 26.  -AT led the wa...</td>\n","      <td>Physical_violence</td>\n","      <td>Harmful_Traditional_practice</td>\n","      <td>0.978076</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["          Tweet_ID  ...      prob\n","4779   ID_LHF3OKZS  ...  0.958514\n","145    ID_NVNOM0GA  ...  0.978076\n","13846  ID_677MK7CR  ...  0.978076\n","13681  ID_UQOQMCRH  ...  0.978076\n","12834  ID_LDY2322H  ...  0.978076\n","12501  ID_MSFPYJKH  ...  0.978076\n","11947  ID_4IMPJKVT  ...  0.978076\n","11437  ID_SU11V6HP  ...  0.978076\n","6541   ID_REQW5FGD  ...  0.978076\n","5636   ID_PXZ033U0  ...  0.978076\n","4806   ID_XBLW1E81  ...  0.978076\n","4502   ID_VWJUWBVN  ...  0.978076\n","4494   ID_MKIY3URD  ...  0.978076\n","4316   ID_D1A6K9C8  ...  0.978076\n","2386   ID_VC7PFT39  ...  0.978076\n","306    ID_DBYM2793  ...  0.978076\n","14167  ID_LAD6J2JR  ...  0.978076\n","14938  ID_Y9K08D54  ...  0.978076\n","\n","[18 rows x 5 columns]"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"code","metadata":{"id":"TLN77MBhfQng","executionInfo":{"status":"ok","timestamp":1629109596798,"user_tz":-180,"elapsed":3,"user":{"displayName":"Cedric Manouan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnI55k9QnJV4mtok5wRGE36cZcj1DZIjihnSx6=s64","userId":"00242929915047742214"}}},"source":["# mismatch.to_csv('check5.csv',index=False)"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":142},"id":"pfkqehWOfQng","executionInfo":{"status":"ok","timestamp":1629109597135,"user_tz":-180,"elapsed":4,"user":{"displayName":"Cedric Manouan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnI55k9QnJV4mtok5wRGE36cZcj1DZIjihnSx6=s64","userId":"00242929915047742214"}},"outputId":"2d25e641-5c91-4b25-acc9-0f0b352fc81f"},"source":["final_out = findf[['Tweet_ID','fintype']]\n","final_out = final_out.rename({'fintype':'type'},axis=1)\n","final_out.head(3)"],"execution_count":25,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Tweet_ID</th>\n","      <th>type</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>ID_D9ONL553</td>\n","      <td>sexual_violence</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>ID_263YTILY</td>\n","      <td>emotional_violence</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>ID_62VS6IXC</td>\n","      <td>emotional_violence</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      Tweet_ID                type\n","0  ID_D9ONL553     sexual_violence\n","1  ID_263YTILY  emotional_violence\n","2  ID_62VS6IXC  emotional_violence"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2TMLSb37fQng","executionInfo":{"status":"ok","timestamp":1629109597889,"user_tz":-180,"elapsed":7,"user":{"displayName":"Cedric Manouan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnI55k9QnJV4mtok5wRGE36cZcj1DZIjihnSx6=s64","userId":"00242929915047742214"}},"outputId":"2efb7263-35c1-4cfd-e484-e63bc8d6edac"},"source":["final_out['type'].value_counts()"],"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["sexual_violence                 8101\n","Physical_violence               3360\n","Harmful_Traditional_practice    3176\n","economic_violence                498\n","emotional_violence               446\n","Name: type, dtype: int64"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"code","metadata":{"id":"tYjtjUnDfQng","executionInfo":{"status":"ok","timestamp":1629109607035,"user_tz":-180,"elapsed":340,"user":{"displayName":"Cedric Manouan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnI55k9QnJV4mtok5wRGE36cZcj1DZIjihnSx6=s64","userId":"00242929915047742214"}}},"source":["final_out.to_csv('outcome_stage4.csv',index=False)"],"execution_count":28,"outputs":[]},{"cell_type":"code","metadata":{"id":"BijhbdocfQng","executionInfo":{"status":"aborted","timestamp":1629109512117,"user_tz":-180,"elapsed":26,"user":{"displayName":"Cedric Manouan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnI55k9QnJV4mtok5wRGE36cZcj1DZIjihnSx6=s64","userId":"00242929915047742214"}}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6ZL_PrhMgOfn","executionInfo":{"status":"ok","timestamp":1629109685954,"user_tz":-180,"elapsed":1335,"user":{"displayName":"Cedric Manouan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnI55k9QnJV4mtok5wRGE36cZcj1DZIjihnSx6=s64","userId":"00242929915047742214"}},"outputId":"63d93561-e521-4c76-90e2-4cc51fbabc8d"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":29,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"eLnOEG24gQdi"},"source":[""],"execution_count":null,"outputs":[]}]}