{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"Stage1_Classification_sexual_vs_physical.ipynb","provenance":[]}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aUrqgrH6Od6I","executionInfo":{"status":"ok","timestamp":1629106498811,"user_tz":-180,"elapsed":3291,"user":{"displayName":"Cedric Manouan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnI55k9QnJV4mtok5wRGE36cZcj1DZIjihnSx6=s64","userId":"00242929915047742214"}},"outputId":"5bf4702b-a639-4d8c-f23e-0ea805cde485"},"source":["!pip install emoji"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: emoji in /usr/local/lib/python3.7/dist-packages (1.4.2)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QulnVE3sOC1P","executionInfo":{"status":"ok","timestamp":1629106498812,"user_tz":-180,"elapsed":8,"user":{"displayName":"Cedric Manouan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnI55k9QnJV4mtok5wRGE36cZcj1DZIjihnSx6=s64","userId":"00242929915047742214"}},"outputId":"8b28a7bf-0fc2-482e-b526-a865b37b913d"},"source":["import pandas as pd\n","import numpy as np\n","import re\n","\n","import nltk\n","nltk.download('punkt')\n","from nltk.tokenize import TreebankWordTokenizer\n","from collections import Counter\n","from nltk.tokenize import word_tokenize\n","from nltk.probability import FreqDist\n","import emoji\n","import spacy\n","from operator import itemgetter\n","import itertools\n","from itertools import combinations\n","\n","\n","from nltk.stem.snowball import SnowballStemmer\n","\n","\n","from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n","from tqdm import tqdm\n","tqdm.pandas()\n"],"execution_count":15,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"nDAYFB-XOC1S"},"source":["## Stopwords and spacy model\n","* We extract a list of stopwords from - https://gist.github.com/sebleier/554280 because the available list of NLTK/Spacy stopwords are highly insufficient and incomplete.\n","* We extract the Spacy's english pipeline optimized for CPU and will be using it for stemming and lemmatization tasks. In case the spacy's model is not installed just type \"python -m spacy download en_core_web_sm\" on terminal to get the model installed for preprocessing tasks\n","* We also instantiate SnowballStemmer to perform stemming after lemmatization is done in later stages of the notebook"]},{"cell_type":"code","metadata":{"id":"xGrGiVwlOC1S","executionInfo":{"status":"ok","timestamp":1629106499872,"user_tz":-180,"elapsed":1064,"user":{"displayName":"Cedric Manouan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnI55k9QnJV4mtok5wRGE36cZcj1DZIjihnSx6=s64","userId":"00242929915047742214"}}},"source":["#### Stopwords list taken from - https://gist.github.com/sebleier/554280 (reason - the available list of NLTK/ Spacy stopwords are too less and don't cover all possible words)\n","stopwords_list = [\"0o\", \"0s\", \"3a\", \"3b\", \"3d\", \"6b\", \"6o\", \"a\", \"a1\", \"a2\", \"a3\", \"a4\", \"ab\", \"able\", \"about\", \"above\", \"abst\", \"ac\", \"accordance\", \"according\", \"accordingly\", \"across\", \"act\", \"actually\", \"ad\", \"added\", \"adj\", \"ae\", \"af\", \"affected\", \"affecting\", \"affects\", \"after\", \"afterwards\", \"ag\", \"again\", \"against\", \"ah\", \"ain\", \"ain't\", \"aj\", \"al\", \"all\", \"allow\", \"allows\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\", \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"announce\", \"another\", \"any\", \"anybody\", \"anyhow\", \"anymore\", \"anyone\", \"anything\", \"anyway\", \"anyways\", \"anywhere\", \"ao\", \"ap\", \"apart\", \"apparently\", \"appear\", \"appreciate\", \"appropriate\", \"approximately\", \"ar\", \"are\", \"aren\", \"arent\", \"aren't\", \"arise\", \"around\", \"as\", \"a's\", \"aside\", \"ask\", \"asking\", \"associated\", \"at\", \"au\", \"auth\", \"av\", \"available\", \"aw\", \"away\", \"awfully\", \"ax\", \"ay\", \"az\", \"b\", \"b1\", \"b2\", \"b3\", \"ba\", \"back\", \"bc\", \"bd\", \"be\", \"became\", \"because\", \"become\", \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"begin\", \"beginning\", \"beginnings\", \"begins\", \"behind\", \"being\", \"believe\", \"below\", \"beside\", \"besides\", \"best\", \"better\", \"between\", \"beyond\", \"bi\", \"bill\", \"biol\", \"bj\", \"bk\", \"bl\", \"bn\", \"both\", \"bottom\", \"bp\", \"br\", \"brief\", \"briefly\", \"bs\", \"bt\", \"bu\", \"but\", \"bx\", \"by\", \"c\", \"c1\", \"c2\", \"c3\", \"ca\", \"call\", \"came\", \"can\", \"cannot\", \"cant\", \"can't\", \"cause\", \"causes\", \"cc\", \"cd\", \"ce\", \"certain\", \"certainly\", \"cf\", \"cg\", \"ch\", \"changes\", \"ci\", \"cit\", \"cj\", \"cl\", \"clearly\", \"cm\", \"c'mon\", \"cn\", \"co\", \"com\", \"come\", \"comes\", \"con\", \"concerning\", \"consequently\", \"consider\", \"considering\", \"contain\", \"containing\", \"contains\", \"corresponding\", \"could\", \"couldn\", \"couldnt\", \"couldn't\", \"course\", \"cp\", \"cq\", \"cr\", \"cry\", \"cs\", \"c's\", \"ct\", \"cu\", \"currently\", \"cv\", \"cx\", \"cy\", \"cz\", \"d\", \"d2\", \"da\", \"date\", \"dc\", \"dd\", \"de\", \"definitely\", \"describe\", \"described\", \"despite\", \"detail\", \"df\", \"di\", \"did\", \"didn\", \"didn't\", \"different\", \"dj\", \"dk\", \"dl\", \"do\", \"does\", \"doesn\", \"doesn't\", \"doing\", \"don\", \"done\", \"don't\", \"down\", \"downwards\", \"dp\", \"dr\", \"ds\", \"dt\", \"du\", \"due\", \"during\", \"dx\", \"dy\", \"e\", \"e2\", \"e3\", \"ea\", \"each\", \"ec\", \"ed\", \"edu\", \"ee\", \"ef\", \"effect\", \"eg\", \"ei\", \"eight\", \"eighty\", \"either\", \"ej\", \"el\", \"eleven\", \"else\", \"elsewhere\", \"em\", \"empty\", \"en\", \"end\", \"ending\", \"enough\", \"entirely\", \"eo\", \"ep\", \"eq\", \"er\", \"es\", \"especially\", \"est\", \"et\", \"et-al\", \"etc\", \"eu\", \"ev\", \"even\", \"ever\", \"every\", \"everybody\", \"everyone\", \"everything\", \"everywhere\", \"ex\", \"exactly\", \"example\", \"except\", \"ey\", \"f\", \"f2\", \"fa\", \"far\", \"fc\", \"few\", \"ff\", \"fi\", \"fifteen\", \"fifth\", \"fify\", \"fill\", \"find\", \"fire\", \"first\", \"five\", \"fix\", \"fj\", \"fl\", \"fn\", \"fo\", \"followed\", \"following\", \"follows\", \"for\", \"former\", \"formerly\", \"forth\", \"forty\", \"found\", \"four\", \"fr\", \"from\", \"front\", \"fs\", \"ft\", \"fu\", \"full\", \"further\", \"furthermore\", \"fy\", \"g\", \"ga\", \"gave\", \"ge\", \"get\", \"gets\", \"getting\", \"gi\", \"give\", \"given\", \"gives\", \"giving\", \"gj\", \"gl\", \"go\", \"goes\", \"going\", \"gone\", \"got\", \"gotten\", \"gr\", \"greetings\", \"gs\", \"gy\", \"h\", \"h2\", \"h3\", \"had\", \"hadn\", \"hadn't\", \"happens\", \"hardly\", \"has\", \"hasn\", \"hasnt\", \"hasn't\", \"have\", \"haven\", \"haven't\", \"having\", \"he\", \"hed\", \"he'd\", \"he'll\", \"hello\", \"help\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"heres\", \"here's\", \"hereupon\", \"hers\", \"herself\", \"hes\", \"he's\", \"hh\", \"hi\", \"hid\", \"him\", \"himself\", \"his\", \"hither\", \"hj\", \"ho\", \"home\", \"hopefully\", \"how\", \"howbeit\", \"however\", \"how's\", \"hr\", \"hs\", \"http\", \"hu\", \"hundred\", \"hy\", \"i\", \"i2\", \"i3\", \"i4\", \"i6\", \"i7\", \"i8\", \"ia\", \"ib\", \"ibid\", \"ic\", \"id\", \"i'd\", \"ie\", \"if\", \"ig\", \"ignored\", \"ih\", \"ii\", \"ij\", \"il\", \"i'll\", \"im\", \"i'm\", \"immediate\", \"immediately\", \"importance\", \"important\", \"in\", \"inasmuch\", \"inc\", \"indeed\", \"index\", \"indicate\", \"indicated\", \"indicates\", \"information\", \"inner\", \"insofar\", \"instead\", \"interest\", \"into\", \"invention\", \"inward\", \"io\", \"ip\", \"iq\", \"ir\", \"is\", \"isn\", \"isn't\", \"it\", \"itd\", \"it'd\", \"it'll\", \"its\", \"it's\", \"itself\", \"iv\", \"i've\", \"ix\", \"iy\", \"iz\", \"j\", \"jj\", \"jr\", \"js\", \"jt\", \"ju\", \"just\", \"k\", \"ke\", \"keep\", \"keeps\", \"kept\", \"kg\", \"kj\", \"km\", \"know\", \"known\", \"knows\", \"ko\", \"l\", \"l2\", \"la\", \"largely\", \"last\", \"lately\", \"later\", \"latter\", \"latterly\", \"lb\", \"lc\", \"le\", \"least\", \"les\", \"less\", \"lest\", \"let\", \"lets\", \"let's\", \"lf\", \"like\", \"liked\", \"likely\", \"line\", \"little\", \"lj\", \"ll\", \"ll\", \"ln\", \"lo\", \"look\", \"looking\", \"looks\", \"los\", \"lr\", \"ls\", \"lt\", \"ltd\", \"m\", \"m2\", \"ma\", \"made\", \"mainly\", \"make\", \"makes\", \"many\", \"may\", \"maybe\", \"me\", \"mean\", \"means\", \"meantime\", \"meanwhile\", \"merely\", \"mg\", \"might\", \"mightn\", \"mightn't\", \"mill\", \"million\", \"mine\", \"miss\", \"ml\", \"mn\", \"mo\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"mr\", \"mrs\", \"ms\", \"mt\", \"mu\", \"much\", \"mug\", \"must\", \"mustn\", \"mustn't\", \"my\", \"myself\", \"n\", \"n2\", \"na\", \"name\", \"namely\", \"nay\", \"nc\", \"nd\", \"ne\", \"near\", \"nearly\", \"necessarily\", \"necessary\", \"need\", \"needn\", \"needn't\", \"needs\", \"neither\", \"never\", \"nevertheless\", \"new\", \"next\", \"ng\", \"ni\", \"nine\", \"ninety\", \"nj\", \"nl\", \"nn\", \"no\", \"nobody\", \"non\", \"none\", \"nonetheless\", \"noone\", \"nor\", \"normally\", \"nos\", \"not\", \"noted\", \"nothing\", \"novel\", \"now\", \"nowhere\", \"nr\", \"ns\", \"nt\", \"ny\", \"o\", \"oa\", \"ob\", \"obtain\", \"obtained\", \"obviously\", \"oc\", \"od\", \"of\", \"off\", \"often\", \"og\", \"oh\", \"oi\", \"oj\", \"ok\", \"okay\", \"ol\", \"old\", \"om\", \"omitted\", \"on\", \"once\", \"one\", \"ones\", \"only\", \"onto\", \"oo\", \"op\", \"oq\", \"or\", \"ord\", \"os\", \"ot\", \"other\", \"others\", \"otherwise\", \"ou\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"outside\", \"over\", \"overall\", \"ow\", \"owing\", \"own\", \"ox\", \"oz\", \"p\", \"p1\", \"p2\", \"p3\", \"page\", \"pagecount\", \"pages\", \"par\", \"part\", \"particular\", \"particularly\", \"pas\", \"past\", \"pc\", \"pd\", \"pe\", \"per\", \"perhaps\", \"pf\", \"ph\", \"pi\", \"pj\", \"pk\", \"pl\", \"placed\", \"please\", \"plus\", \"pm\", \"pn\", \"po\", \"poorly\", \"possible\", \"possibly\", \"potentially\", \"pp\", \"pq\", \"pr\", \"predominantly\", \"present\", \"presumably\", \"previously\", \"primarily\", \"probably\", \"promptly\", \"proud\", \"provides\", \"ps\", \"pt\", \"pu\", \"put\", \"py\", \"q\", \"qj\", \"qu\", \"que\", \"quickly\", \"quite\", \"qv\", \"r\", \"r2\", \"ra\", \"ran\", \"rather\", \"rc\", \"rd\", \"re\", \"readily\", \"really\", \"reasonably\", \"recent\", \"recently\", \"ref\", \"refs\", \"regarding\", \"regardless\", \"regards\", \"related\", \"relatively\", \"research\", \"research-articl\", \"respectively\", \"resulted\", \"resulting\", \"results\", \"rf\", \"rh\", \"ri\", \"right\", \"rj\", \"rl\", \"rm\", \"rn\", \"ro\", \"rq\", \"rr\", \"rs\", \"rt\", \"ru\", \"run\", \"rv\", \"ry\", \"s\", \"s2\", \"sa\", \"said\", \"same\", \"saw\", \"say\", \"saying\", \"says\", \"sc\", \"sd\", \"se\", \"sec\", \"second\", \"secondly\", \"section\", \"see\", \"seeing\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"seen\", \"self\", \"selves\", \"sensible\", \"sent\", \"serious\", \"seriously\", \"seven\", \"several\", \"sf\", \"shall\", \"shan\", \"shan't\", \"she\", \"shed\", \"she'd\", \"she'll\", \"shes\", \"she's\", \"should\", \"shouldn\", \"shouldn't\", \"should've\", \"show\", \"showed\", \"shown\", \"showns\", \"shows\", \"si\", \"side\", \"significant\", \"significantly\", \"similar\", \"similarly\", \"since\", \"sincere\", \"six\", \"sixty\", \"sj\", \"sl\", \"slightly\", \"sm\", \"sn\", \"so\", \"some\", \"somebody\", \"somehow\", \"someone\", \"somethan\", \"something\", \"sometime\", \"sometimes\", \"somewhat\", \"somewhere\", \"soon\", \"sorry\", \"sp\", \"specifically\", \"specified\", \"specify\", \"specifying\", \"sq\", \"sr\", \"ss\", \"st\", \"still\", \"stop\", \"strongly\", \"sub\", \"substantially\", \"successfully\", \"such\", \"sufficiently\", \"suggest\", \"sup\", \"sure\", \"sy\", \"system\", \"sz\", \"t\", \"t1\", \"t2\", \"t3\", \"take\", \"taken\", \"taking\", \"tb\", \"tc\", \"td\", \"te\", \"tell\", \"ten\", \"tends\", \"tf\", \"th\", \"than\", \"thank\", \"thanks\", \"thanx\", \"that\", \"that'll\", \"thats\", \"that's\", \"that've\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"thered\", \"therefore\", \"therein\", \"there'll\", \"thereof\", \"therere\", \"theres\", \"there's\", \"thereto\", \"thereupon\", \"there've\", \"these\", \"they\", \"theyd\", \"they'd\", \"they'll\", \"theyre\", \"they're\", \"they've\", \"thickv\", \"thin\", \"think\", \"third\", \"this\", \"thorough\", \"thoroughly\", \"those\", \"thou\", \"though\", \"thoughh\", \"thousand\", \"three\", \"throug\", \"through\", \"throughout\", \"thru\", \"thus\", \"ti\", \"til\", \"tip\", \"tj\", \"tl\", \"tm\", \"tn\", \"to\", \"together\", \"too\", \"took\", \"top\", \"toward\", \"towards\", \"tp\", \"tq\", \"tr\", \"tried\", \"tries\", \"truly\", \"try\", \"trying\", \"ts\", \"t's\", \"tt\", \"tv\", \"twelve\", \"twenty\", \"twice\", \"two\", \"tx\", \"u\", \"u201d\", \"ue\", \"ui\", \"uj\", \"uk\", \"um\", \"un\", \"under\", \"unfortunately\", \"unless\", \"unlike\", \"unlikely\", \"until\", \"unto\", \"uo\", \"up\", \"upon\", \"ups\", \"ur\", \"us\", \"use\", \"used\", \"useful\", \"usefully\", \"usefulness\", \"uses\", \"using\", \"usually\", \"ut\", \"v\", \"va\", \"value\", \"various\", \"vd\", \"ve\", \"ve\", \"very\", \"via\", \"viz\", \"vj\", \"vo\", \"vol\", \"vols\", \"volumtype\", \"vq\", \"vs\", \"vt\", \"vu\", \"w\", \"wa\", \"want\", \"wants\", \"was\", \"wasn\", \"wasnt\", \"wasn't\", \"way\", \"we\", \"wed\", \"we'd\", \"welcome\", \"well\", \"we'll\", \"well-b\", \"went\", \"were\", \"we're\", \"weren\", \"werent\", \"weren't\", \"we've\", \"what\", \"whatever\", \"what'll\", \"whats\", \"what's\", \"when\", \"whence\", \"whenever\", \"when's\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"wheres\", \"where's\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whim\", \"whither\", \"who\", \"whod\", \"whoever\", \"whole\", \"who'll\", \"whom\", \"whomever\", \"whos\", \"who's\", \"whose\", \"why\", \"why's\", \"wi\", \"widely\", \"will\", \"willing\", \"wish\", \"with\", \"within\", \"without\", \"wo\", \"won\", \"wonder\", \"wont\", \"won't\", \"words\", \"world\", \"would\", \"wouldn\", \"wouldnt\", \"wouldn't\", \"www\", \"x\", \"x1\", \"x2\", \"x3\", \"xf\", \"xi\", \"xj\", \"xk\", \"xl\", \"xn\", \"xo\", \"xs\", \"xt\", \"xv\", \"xx\", \"y\", \"y2\", \"yes\", \"yet\", \"yj\", \"yl\", \"you\", \"youd\", \"you'd\", \"you'll\", \"your\", \"youre\", \"you're\", \"yours\", \"yourself\", \"yourselves\", \"you've\", \"yr\", \"ys\", \"yt\", \"z\", \"zero\", \"zi\", \"zz\",]\n","\n","nlp        = spacy.load(\"en_core_web_sm\")\n","\n","stemmer = SnowballStemmer(language='english')"],"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MZOcnFtfOC1X"},"source":["## Custom preprocessing functions for Vocabulary extraction\n","We declare basic functions to perform tasks like removal of stopwords, stemming, lemmatization in batches, removal and emojis etc. "]},{"cell_type":"code","metadata":{"id":"8JkmBDuKOC1X","executionInfo":{"status":"ok","timestamp":1629106499872,"user_tz":-180,"elapsed":11,"user":{"displayName":"Cedric Manouan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnI55k9QnJV4mtok5wRGE36cZcj1DZIjihnSx6=s64","userId":"00242929915047742214"}}},"source":["def remove_stopwords(text):\n","    '''\n","      Removes stopwords from the text\n","    '''\n","\n","    text_split   = text.split()\n","\n","    text_list    = [word for word in text_split if not word in stopwords_list]\n","\n","    return ' '.join(text_list)\n","\n","def stemming_perform(text):\n","    '''\n","      Performs lemmatization in batches\n","    '''\n","    \n","    token = text.split()\n","    stemmed_text = []\n","    for tok in token:\n","        stemmed_text.append(stemmer.stem(tok))\n","    return ' '.join(stemmed_text)\n","\n","\n","def cleaning_text(text):\n","    '''\n","      Operations performed:- \n","      1. Converting the entire text to lowercase\n","      2. Removal of punctuations from the text\n","      3. Removal of numbers from the text\n","    '''\n","\n","    text = text.lower()\n","\n","    text = re.sub(r'[^\\w\\s]', ' ', text)\n","    text = re.sub(r'\\d+', '', text)\n","    text = remove_stopwords(text)\n","\n","    text = text.replace('amp','') ## this was actually ampersand which kept coming in frequently that made no sense\n","    return text\n","\n","\n","def give_emoji_free_text(text):\n","    ''' \n","      Removes all possible emojis from the text (because our text is basically tweets that can have emoijs).\n","      The input is a text and the output is emoji free text\n","    '''\n","    allchars = [str for str in text.encode().decode('utf-8')]\n","    emoji_list = [c for c in allchars if c in emoji.UNICODE_EMOJI]\n","    clean_text = ' '.join([str for str in text.encode().decode('utf-8').split() if not any(i in str for i in emoji_list)])\n","    return clean_text\n","\n","\n","def preprocess_custom(text):\n","    text = give_emoji_free_text(text)\n","    text = cleaning_text(text)\n","\n","    return text\n","\n","def lemmatize_batches(docs):\n","    '''\n","      Performs lemmatization in batches\n","    '''\n","    lemmatized_text = []\n","\n","    for doc in nlp.pipe(docs, batch_size=256, n_process=3,disable=[\"parser\", \"ner\"]):\n","        lemmatized_text.append(str(' '.join([token.lemma_ for token in doc])))\n","    return lemmatized_text\n","\n","\n","def lemmatize_df(df):\n","    lemmatized = []\n","    for i in tqdm(range(0,df.shape[0],1000)):\n","        z = lemmatize_batches(df['preprocessed_text'].iloc[i:i+1000])\n","        lemmatized.extend(z)\n","    df['lemmatize'] = lemmatized\n","    return df\n","\n","\n","\n","def return_frequency_dict_sorted(df):\n","    sent = df['stemmed'].values\n","    sent = ' '.join(sent)\n","\n","    fdist = FreqDist()\n","    \n","    for word in word_tokenize(sent):\n","        fdist[word.lower()] += 1\n","        \n","    return sorted(fdist.items(), key=lambda x: x[1], reverse=True)\n","\n","def replace(x1,x2):\n","    if str(x1) == 'nan':\n","        return x2\n","    else:\n","        return x1\n"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aGZY3MleOqBo","executionInfo":{"status":"ok","timestamp":1629106499873,"user_tz":-180,"elapsed":11,"user":{"displayName":"Cedric Manouan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnI55k9QnJV4mtok5wRGE36cZcj1DZIjihnSx6=s64","userId":"00242929915047742214"}},"outputId":"e3c1be55-e7a6-4b85-9e29-096d923ee3a7"},"source":["from google.colab import drive\n","drive.mount(\"/content/gdrive\")"],"execution_count":18,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Tktdd11sOC1Y"},"source":["## Loading the best result obtained from SOTA models\n","We load the output from the previously trained SOTA model <br>\n","We will no subset the output on just two classes where heavy imbalance was observed - sexual violence and Physical violence. <br>\n","We will create a dataset from where we will draw inferences"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hKkvUA4AOC1Y","executionInfo":{"status":"ok","timestamp":1629106499873,"user_tz":-180,"elapsed":9,"user":{"displayName":"Cedric Manouan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnI55k9QnJV4mtok5wRGE36cZcj1DZIjihnSx6=s64","userId":"00242929915047742214"}},"outputId":"587aa414-833a-4acd-a50a-caf222e0f6a4"},"source":["subdf   = pd.read_csv('/content/gdrive/MyDrive/Hackathon/bert_base_best_result.csv')\n","\n","subdf   = subdf[(subdf['type']=='sexual_violence') | (subdf['type']=='Physical_violence')]\n","\n","print(f'The shape of subsetted data with subsetting on physical and sexual violence will be {subdf.shape}')\n","\n","test_df = pd.read_csv('/content/gdrive/MyDrive/Hackathon/Test.csv')\n","\n","\n","temp_df = subdf.merge(test_df,on=['Tweet_ID'],how='left')\n","\n","_       = temp_df.rename(columns={'type':'type1'},inplace=True)\n","\n","print(f'The shape of data after assigning the labels attained from BERT to test is {temp_df.shape}')"],"execution_count":19,"outputs":[{"output_type":"stream","text":["The shape of subsetted data with subsetting on physical and sexual violence will be (11324, 2)\n","The shape of data after assigning the labels attained from BERT to test is (11324, 3)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"hm-xoHWHOC1Z"},"source":["## Text Preprocessing for Vocab extraction\n","We preprocess the test data tweets and then find the frequency occurance of the preprocessed words from the dataset. We will now select top 20 words from the test data tweets which will be used as a vocabulary for building our model."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s6lcE6ZgOC1a","executionInfo":{"status":"ok","timestamp":1629106526870,"user_tz":-180,"elapsed":27004,"user":{"displayName":"Cedric Manouan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnI55k9QnJV4mtok5wRGE36cZcj1DZIjihnSx6=s64","userId":"00242929915047742214"}},"outputId":"d1bcd3ef-2aac-4f05-c362-2bf58e432b7f"},"source":["temp_df['preprocessed_text'] = temp_df['tweet'].progress_apply(preprocess_custom)\n","temp_df                      = lemmatize_df(temp_df)\n","temp_df['stemmed']           = temp_df['lemmatize'].progress_apply(stemming_perform)\n","sorted_freq_dict             = return_frequency_dict_sorted(temp_df)\n","\n","threshold2consider           = 20\n","vocab_sex_phy                = [word[0] for word in sorted_freq_dict[:threshold2consider]]\n"],"execution_count":20,"outputs":[{"output_type":"stream","text":["100%|██████████| 11324/11324 [00:06<00:00, 1700.28it/s]\n","100%|██████████| 12/12 [00:17<00:00,  1.44s/it]\n","100%|██████████| 11324/11324 [00:02<00:00, 4396.46it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":686},"id":"6nOSJfxaOC1a","executionInfo":{"status":"ok","timestamp":1629106527536,"user_tz":-180,"elapsed":693,"user":{"displayName":"Cedric Manouan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnI55k9QnJV4mtok5wRGE36cZcj1DZIjihnSx6=s64","userId":"00242929915047742214"}},"outputId":"a2c7c87b-09ba-402f-8343-015f5545f26e"},"source":["print('The top 20 words (preprocessed) based on the frequency distribution and their relative frequencies are given below')\n","pd.DataFrame(sorted_freq_dict,columns = ['word','Frequency']).head(20)"],"execution_count":21,"outputs":[{"output_type":"stream","text":["The top 20 words (preprocessed) based on the frequency distribution and their relative frequencies are given below\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>word</th>\n","      <th>Frequency</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>rape</td>\n","      <td>8659</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>boyfriend</td>\n","      <td>3917</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>stab</td>\n","      <td>3399</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>knife</td>\n","      <td>3275</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>student</td>\n","      <td>1862</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>year</td>\n","      <td>1786</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>woman</td>\n","      <td>1748</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>univers</td>\n","      <td>1642</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>sex</td>\n","      <td>1503</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>man</td>\n","      <td>1493</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>girl</td>\n","      <td>1480</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>forc</td>\n","      <td>1378</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>priest</td>\n","      <td>1122</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>friend</td>\n","      <td>1115</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>time</td>\n","      <td>1074</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>pastor</td>\n","      <td>983</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>tell</td>\n","      <td>969</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>kill</td>\n","      <td>957</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>peopl</td>\n","      <td>705</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>murder</td>\n","      <td>703</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         word  Frequency\n","0        rape       8659\n","1   boyfriend       3917\n","2        stab       3399\n","3       knife       3275\n","4     student       1862\n","5        year       1786\n","6       woman       1748\n","7     univers       1642\n","8         sex       1503\n","9         man       1493\n","10       girl       1480\n","11       forc       1378\n","12     priest       1122\n","13     friend       1115\n","14       time       1074\n","15     pastor        983\n","16       tell        969\n","17       kill        957\n","18      peopl        705\n","19     murder        703"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"markdown","metadata":{"id":"PrOspTCmOC1a"},"source":["# Classification model on the vocabulary attained\n","We will now preprocess our train and BERT Classified physical and sexual data to finetune it <br>\n","This will be to make sure that the distribution of train and test dataset is same and noise is removed in terms of redundant features"]},{"cell_type":"markdown","metadata":{"id":"n2m39S69OC1b"},"source":["## Text Preprocessing on Train and Test data\n","We subset our train data having only Physical Violence and Sexual Violence labels and perform the exact same preprocessing steps we did for the test data above. We will subset the test data on the predictions previously obtained from BERT model."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r-KpnnlZOC1b","executionInfo":{"status":"ok","timestamp":1629106631468,"user_tz":-180,"elapsed":103940,"user":{"displayName":"Cedric Manouan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnI55k9QnJV4mtok5wRGE36cZcj1DZIjihnSx6=s64","userId":"00242929915047742214"}},"outputId":"35ea402f-5514-4553-f659-9142b49947fb"},"source":["test_df                         = pd.read_csv('/content/gdrive/MyDrive/Hackathon/Test.csv')\n","subdf                           = pd.read_csv('/content/gdrive/MyDrive/Hackathon/bert_base_best_result.csv')\n","\n","test_df                         = test_df.merge(subdf,on=['Tweet_ID'],how='left')\n","\n","\n","subdf                           = subdf[(subdf['type']=='sexual_violence') | (subdf['type']=='Physical_violence')]\n","\n","\n","temp_df                         = subdf.merge(test_df,on=['Tweet_ID'],how='left')\n","\n","_                               = temp_df.rename(columns={'type':'type1'},inplace=True)\n","\n","temp_df['preprocessed_text']    = temp_df['tweet'].progress_apply(preprocess_custom)\n","temp_df                         = lemmatize_df(temp_df)\n","temp_df['stemmed']              = temp_df['lemmatize'].progress_apply(stemming_perform)\n","\n","\n","train_df                        = pd.read_csv('/content/gdrive/MyDrive/Hackathon/Train.csv')\n","train_data                      = train_df[train_df['type'].isin(['Physical_violence','sexual_violence'])]\n","train_data                      = train_data.reset_index()\n","\n","train_data['preprocessed_text'] = train_data['tweet'].progress_apply(preprocess_custom)\n","train_data                      = lemmatize_df(train_data)\n","train_data['stemmed']           = train_data['lemmatize'].progress_apply(stemming_perform)\n","\n","print(test_df.shape,train_df.shape,temp_df.shape)"],"execution_count":22,"outputs":[{"output_type":"stream","text":["100%|██████████| 11324/11324 [00:06<00:00, 1698.94it/s]\n","100%|██████████| 12/12 [00:11<00:00,  1.01it/s]\n","100%|██████████| 11324/11324 [00:02<00:00, 4365.87it/s]\n","100%|██████████| 38594/38594 [00:24<00:00, 1594.03it/s]\n","100%|██████████| 39/39 [00:48<00:00,  1.25s/it]\n","100%|██████████| 38594/38594 [00:08<00:00, 4569.47it/s]"],"name":"stderr"},{"output_type":"stream","text":["(15581, 3) (39650, 3) (11324, 7)\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"PpJdjjbnOC1b"},"source":["## Count vectorization with binarization\n","We apply count vectorizer on the train and test datasets "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jI2zINDqOC1b","executionInfo":{"status":"ok","timestamp":1629106632130,"user_tz":-180,"elapsed":685,"user":{"displayName":"Cedric Manouan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnI55k9QnJV4mtok5wRGE36cZcj1DZIjihnSx6=s64","userId":"00242929915047742214"}},"outputId":"deef2c78-8bc7-4917-d87c-f78c501bacf7"},"source":["train_X              = train_data['stemmed'].values\n","test_X               = temp_df['stemmed'].values\n","\n","cv                   = CountVectorizer(vocabulary = vocab_sex_phy,binary=True)\n","\n","train                = cv.fit_transform(train_X)\n","test                 = cv.transform(test_X)\n","\n","#### Converting the sparse matrices to dense \n","train                = train.todense()\n","test                 = test.todense()\n","print(f'The shape of preprocessed train data matrix is {train.shape} and preprocessed test data matrix is {test.shape}')\n","\n","### label encoding done here\n","train_data['labels'] = train_data['type'].map({'sexual_violence':0,'Physical_violence':1})\n","\n","train_y              = train_data['labels'].values"],"execution_count":23,"outputs":[{"output_type":"stream","text":["The shape of preprocessed train data matrix is (38594, 20) and preprocessed test data matrix is (11324, 20)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"30B_AxqgOC1c"},"source":["## Fitting a model on preprocessed data and finetuning\n","* We fit a XGBClassifier model on the preprocessed data \n","* We will now update the previous labels of sexual_violence and Physical_violence with the outcomes attained from the model \n","* The outcomes will be used for second stage classification"]},{"cell_type":"code","metadata":{"id":"lTJOeXpMOC1c","executionInfo":{"status":"ok","timestamp":1629106633146,"user_tz":-180,"elapsed":1018,"user":{"displayName":"Cedric Manouan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnI55k9QnJV4mtok5wRGE36cZcj1DZIjihnSx6=s64","userId":"00242929915047742214"}}},"source":["from xgboost import XGBClassifier\n","model      = XGBClassifier(n_estimators =10,random_state=13,depth = 4,scale_pos_weight=np.sqrt(32648/5946))\n","_          = model.fit(train,train_y)"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"id":"UCHGOmbqOC1c","executionInfo":{"status":"ok","timestamp":1629106633146,"user_tz":-180,"elapsed":35,"user":{"displayName":"Cedric Manouan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnI55k9QnJV4mtok5wRGE36cZcj1DZIjihnSx6=s64","userId":"00242929915047742214"}}},"source":["prob_thresh     = 0.45\n","predicted_probs = model.predict_proba(test)[:,1]\n","accuracy_preds  = np.where(predicted_probs>prob_thresh,1,0)"],"execution_count":25,"outputs":[]},{"cell_type":"code","metadata":{"id":"9JztbTVGOC1d","executionInfo":{"status":"ok","timestamp":1629106633147,"user_tz":-180,"elapsed":35,"user":{"displayName":"Cedric Manouan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnI55k9QnJV4mtok5wRGE36cZcj1DZIjihnSx6=s64","userId":"00242929915047742214"}}},"source":[""],"execution_count":25,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QLP0uxeUOC1d","executionInfo":{"status":"ok","timestamp":1629106633150,"user_tz":-180,"elapsed":36,"user":{"displayName":"Cedric Manouan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnI55k9QnJV4mtok5wRGE36cZcj1DZIjihnSx6=s64","userId":"00242929915047742214"}},"outputId":"960e7cc1-2d7f-4f5d-a1df-0936f46f9fe2"},"source":["\n","temp_df['type2'] = accuracy_preds\n","print(temp_df['type2'].value_counts())\n","\n","\n","## Inverse label encoding \n","temp_df['type2'] = temp_df['type2'].map({0:'sexual_violence',1:'Physical_violence'})\n","temp_df['prob']  = predicted_probs\n","\n","findf            = test_df.merge(temp_df,on = ['Tweet_ID','tweet'],how='left')\n"],"execution_count":26,"outputs":[{"output_type":"stream","text":["0    7946\n","1    3378\n","Name: type2, dtype: int64\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ho78RrIBOC1d","executionInfo":{"status":"ok","timestamp":1629106633151,"user_tz":-180,"elapsed":35,"user":{"displayName":"Cedric Manouan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnI55k9QnJV4mtok5wRGE36cZcj1DZIjihnSx6=s64","userId":"00242929915047742214"}},"outputId":"7c0e8feb-b6dd-4005-c022-b3f74fbfd842"},"source":["### We update the previous classes with the updated ones \n","findf['fintype'] = findf.apply(lambda z: replace(z['type2'],z['type']),axis=1)\n","findf['flag']    = (findf['type']!=findf['fintype']).astype(int)\n","\n","print(f'Ensuring the shape of outcome dataframe is maintained, we have shape of output data as - {findf.shape}')"],"execution_count":27,"outputs":[{"output_type":"stream","text":["Ensuring the shape of outcome dataframe is maintained, we have shape of output data as - (15581, 12)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"VbmPDpEaOC1d"},"source":["## Reviewing some modifications \n","We will see some mismatches done by BERT model that were modified in this stage of supervised classification based on machine learning "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":669},"id":"MAuctjJNOC1d","executionInfo":{"status":"ok","timestamp":1629106633151,"user_tz":-180,"elapsed":32,"user":{"displayName":"Cedric Manouan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnI55k9QnJV4mtok5wRGE36cZcj1DZIjihnSx6=s64","userId":"00242929915047742214"}},"outputId":"a706c490-3da5-4dc2-abdd-89eb9390b621"},"source":["mismatch = findf[findf['flag']==1][['Tweet_ID','tweet','type','fintype','prob']]\n","mismatch = mismatch.sort_values(by = ['prob'])\n","mismatch.head(20)"],"execution_count":28,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Tweet_ID</th>\n","      <th>tweet</th>\n","      <th>type</th>\n","      <th>fintype</th>\n","      <th>prob</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>667</th>\n","      <td>ID_W8WXZCYC</td>\n","      <td>When you find out your two year old was raped ...</td>\n","      <td>Physical_violence</td>\n","      <td>sexual_violence</td>\n","      <td>0.186706</td>\n","    </tr>\n","    <tr>\n","      <th>9020</th>\n","      <td>ID_LEQOA1SN</td>\n","      <td>When you find out your two year old was raped ...</td>\n","      <td>Physical_violence</td>\n","      <td>sexual_violence</td>\n","      <td>0.186706</td>\n","    </tr>\n","    <tr>\n","      <th>4766</th>\n","      <td>ID_Z881MYLO</td>\n","      <td>FB Repost   When you find out your two year ol...</td>\n","      <td>Physical_violence</td>\n","      <td>sexual_violence</td>\n","      <td>0.186706</td>\n","    </tr>\n","    <tr>\n","      <th>9580</th>\n","      <td>ID_VNYFXLFW</td>\n","      <td>Women like to be raped👙  Only By their husband...</td>\n","      <td>Physical_violence</td>\n","      <td>sexual_violence</td>\n","      <td>0.186706</td>\n","    </tr>\n","    <tr>\n","      <th>13002</th>\n","      <td>ID_MYRRZNB4</td>\n","      <td>my husband made me drink last night. And then ...</td>\n","      <td>Physical_violence</td>\n","      <td>sexual_violence</td>\n","      <td>0.223479</td>\n","    </tr>\n","    <tr>\n","      <th>7788</th>\n","      <td>ID_IQZOJNG5</td>\n","      <td>Told him yesterday if my husband forced me to ...</td>\n","      <td>Physical_violence</td>\n","      <td>sexual_violence</td>\n","      <td>0.223479</td>\n","    </tr>\n","    <tr>\n","      <th>5411</th>\n","      <td>ID_FW7ZISE6</td>\n","      <td>I ran away from my parents home to have sex wi...</td>\n","      <td>Physical_violence</td>\n","      <td>sexual_violence</td>\n","      <td>0.223479</td>\n","    </tr>\n","    <tr>\n","      <th>5331</th>\n","      <td>ID_PV67S5BH</td>\n","      <td>“I've carried a knife a few times. Was it wort...</td>\n","      <td>sexual_violence</td>\n","      <td>Physical_violence</td>\n","      <td>0.685884</td>\n","    </tr>\n","    <tr>\n","      <th>5313</th>\n","      <td>ID_RJE9DIYT</td>\n","      <td>And 1st year of law school i “locked myself ou...</td>\n","      <td>sexual_violence</td>\n","      <td>Physical_violence</td>\n","      <td>0.685884</td>\n","    </tr>\n","    <tr>\n","      <th>5260</th>\n","      <td>ID_DFQIEWCU</td>\n","      <td>I had a friend in highschool. He stabbed me wi...</td>\n","      <td>sexual_violence</td>\n","      <td>Physical_violence</td>\n","      <td>0.685884</td>\n","    </tr>\n","    <tr>\n","      <th>7714</th>\n","      <td>ID_U46NV6LF</td>\n","      <td>miles stabbed me with the fire poker so i thre...</td>\n","      <td>sexual_violence</td>\n","      <td>Physical_violence</td>\n","      <td>0.685884</td>\n","    </tr>\n","    <tr>\n","      <th>5452</th>\n","      <td>ID_G2JH6W7P</td>\n","      <td>forreal though Thinkin about when slime told m...</td>\n","      <td>sexual_violence</td>\n","      <td>Physical_violence</td>\n","      <td>0.685884</td>\n","    </tr>\n","    <tr>\n","      <th>4656</th>\n","      <td>ID_LQNCFUU3</td>\n","      <td>I had this hallucination one time where my fri...</td>\n","      <td>sexual_violence</td>\n","      <td>Physical_violence</td>\n","      <td>0.685884</td>\n","    </tr>\n","    <tr>\n","      <th>4621</th>\n","      <td>ID_F021XN6A</td>\n","      <td>In fifth grade, my best friend stabbed me in t...</td>\n","      <td>sexual_violence</td>\n","      <td>Physical_violence</td>\n","      <td>0.685884</td>\n","    </tr>\n","    <tr>\n","      <th>4507</th>\n","      <td>ID_DT4J1CNK</td>\n","      <td>Agreed. Maybe they didn’t like the lighting or...</td>\n","      <td>sexual_violence</td>\n","      <td>Physical_violence</td>\n","      <td>0.685884</td>\n","    </tr>\n","    <tr>\n","      <th>4219</th>\n","      <td>ID_LQ3BLJ5N</td>\n","      <td>Wow a friend just told me that Brandon Marshal...</td>\n","      <td>sexual_violence</td>\n","      <td>Physical_violence</td>\n","      <td>0.685884</td>\n","    </tr>\n","    <tr>\n","      <th>4119</th>\n","      <td>ID_BAYDDVIJ</td>\n","      <td>Ill never forget the day I was camping with my...</td>\n","      <td>sexual_violence</td>\n","      <td>Physical_violence</td>\n","      <td>0.685884</td>\n","    </tr>\n","    <tr>\n","      <th>5095</th>\n","      <td>ID_YW9ZGPTH</td>\n","      <td>He also detailed how Danielson threatened him ...</td>\n","      <td>sexual_violence</td>\n","      <td>Physical_violence</td>\n","      <td>0.685884</td>\n","    </tr>\n","    <tr>\n","      <th>4767</th>\n","      <td>ID_PM9W73KM</td>\n","      <td>The weirdest moment in my life was when my bes...</td>\n","      <td>sexual_violence</td>\n","      <td>Physical_violence</td>\n","      <td>0.685884</td>\n","    </tr>\n","    <tr>\n","      <th>5691</th>\n","      <td>ID_QPPLGSXJ</td>\n","      <td>The Story of my life you know these niggas tri...</td>\n","      <td>sexual_violence</td>\n","      <td>Physical_violence</td>\n","      <td>0.685884</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["          Tweet_ID  ...      prob\n","667    ID_W8WXZCYC  ...  0.186706\n","9020   ID_LEQOA1SN  ...  0.186706\n","4766   ID_Z881MYLO  ...  0.186706\n","9580   ID_VNYFXLFW  ...  0.186706\n","13002  ID_MYRRZNB4  ...  0.223479\n","7788   ID_IQZOJNG5  ...  0.223479\n","5411   ID_FW7ZISE6  ...  0.223479\n","5331   ID_PV67S5BH  ...  0.685884\n","5313   ID_RJE9DIYT  ...  0.685884\n","5260   ID_DFQIEWCU  ...  0.685884\n","7714   ID_U46NV6LF  ...  0.685884\n","5452   ID_G2JH6W7P  ...  0.685884\n","4656   ID_LQNCFUU3  ...  0.685884\n","4621   ID_F021XN6A  ...  0.685884\n","4507   ID_DT4J1CNK  ...  0.685884\n","4219   ID_LQ3BLJ5N  ...  0.685884\n","4119   ID_BAYDDVIJ  ...  0.685884\n","5095   ID_YW9ZGPTH  ...  0.685884\n","4767   ID_PM9W73KM  ...  0.685884\n","5691   ID_QPPLGSXJ  ...  0.685884\n","\n","[20 rows x 5 columns]"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"code","metadata":{"id":"dCQeY40HOC1e","executionInfo":{"status":"ok","timestamp":1629106633152,"user_tz":-180,"elapsed":30,"user":{"displayName":"Cedric Manouan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnI55k9QnJV4mtok5wRGE36cZcj1DZIjihnSx6=s64","userId":"00242929915047742214"}}},"source":[""],"execution_count":28,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":142},"id":"MWdGz-nTOC1e","executionInfo":{"status":"ok","timestamp":1629106633152,"user_tz":-180,"elapsed":30,"user":{"displayName":"Cedric Manouan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnI55k9QnJV4mtok5wRGE36cZcj1DZIjihnSx6=s64","userId":"00242929915047742214"}},"outputId":"ff643bb3-90c0-439a-a7a2-877c47b7039b"},"source":["final_out = findf[['Tweet_ID','fintype']]\n","final_out = final_out.rename({'fintype':'type'},axis=1)\n","final_out.head(3)"],"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Tweet_ID</th>\n","      <th>type</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>ID_D9ONL553</td>\n","      <td>sexual_violence</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>ID_263YTILY</td>\n","      <td>emotional_violence</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>ID_62VS6IXC</td>\n","      <td>emotional_violence</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      Tweet_ID                type\n","0  ID_D9ONL553     sexual_violence\n","1  ID_263YTILY  emotional_violence\n","2  ID_62VS6IXC  emotional_violence"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YSAIGA3ZOC1e","executionInfo":{"status":"ok","timestamp":1629106633153,"user_tz":-180,"elapsed":29,"user":{"displayName":"Cedric Manouan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnI55k9QnJV4mtok5wRGE36cZcj1DZIjihnSx6=s64","userId":"00242929915047742214"}},"outputId":"f424abca-9e9e-4780-cec0-89e31a56f037"},"source":["final_out['type'].value_counts()"],"execution_count":30,"outputs":[{"output_type":"execute_result","data":{"text/plain":["sexual_violence                 7946\n","Physical_violence               3378\n","Harmful_Traditional_practice    3100\n","emotional_violence               659\n","economic_violence                498\n","Name: type, dtype: int64"]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"code","metadata":{"id":"g0fNR65iOC1e","executionInfo":{"status":"ok","timestamp":1629106901993,"user_tz":-180,"elapsed":430,"user":{"displayName":"Cedric Manouan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnI55k9QnJV4mtok5wRGE36cZcj1DZIjihnSx6=s64","userId":"00242929915047742214"}}},"source":["final_out.to_csv('outcome_stage1.csv',index=False)"],"execution_count":32,"outputs":[]},{"cell_type":"code","metadata":{"id":"lwnonw7GOC1e","executionInfo":{"status":"aborted","timestamp":1629106633153,"user_tz":-180,"elapsed":25,"user":{"displayName":"Cedric Manouan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnI55k9QnJV4mtok5wRGE36cZcj1DZIjihnSx6=s64","userId":"00242929915047742214"}}},"source":[""],"execution_count":null,"outputs":[]}]}