{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import numpy as np  \n",
    "import seaborn as sns\n",
    "import pandas as pd, os, gc\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings('ignore')\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HELPER FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleValidationEncoderNumerical:\n",
    "    \"\"\"\n",
    "    Encoder with validation within\n",
    "    \"\"\"\n",
    "    def __init__(self, cols: List, encoder, folds):\n",
    "        \"\"\"\n",
    "        :param cols: Categorical columns\n",
    "        :param encoder: Encoder class\n",
    "        :param folds: Folds to split the data\n",
    "        \"\"\"\n",
    "        self.cols = cols\n",
    "        self.encoder = encoder\n",
    "        self.encoders_dict = {}\n",
    "        self.folds = folds\n",
    "\n",
    "    def fit_transform(self, X: pd.DataFrame, y: np.array) -> pd.DataFrame:\n",
    "        X = X.reset_index(drop=True)\n",
    "        y = y.reset_index(drop=True)\n",
    "        for n_fold, (train_idx, val_idx) in enumerate(self.folds.split(X, y)):\n",
    "            X_train, X_val = X.loc[train_idx].reset_index(drop=True), X.loc[val_idx].reset_index(drop=True)\n",
    "            y_train, y_val = y[train_idx], y[val_idx]\n",
    "            _ = self.encoder.fit_transform(X_train, y_train)\n",
    "\n",
    "            # transform validation part and get all necessary cols\n",
    "            val_t = self.encoder.transform(X_val)\n",
    "\n",
    "            if n_fold == 0:\n",
    "                cols_representation = np.zeros((X.shape[0], val_t.shape[1]))\n",
    "            \n",
    "            self.encoders_dict[n_fold] = self.encoder\n",
    "\n",
    "            cols_representation[val_idx, :] += val_t.values\n",
    "\n",
    "        cols_representation = pd.DataFrame(cols_representation, columns=X.columns)\n",
    "\n",
    "        return cols_representation\n",
    "\n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        X = X.reset_index(drop=True)\n",
    "\n",
    "        cols_representation = None\n",
    "\n",
    "        for encoder in self.encoders_dict.values():\n",
    "            test_tr = encoder.transform(X)\n",
    "\n",
    "            if cols_representation is None:\n",
    "                cols_representation = np.zeros(test_tr.shape)\n",
    "\n",
    "            cols_representation = cols_representation + test_tr / self.folds.n_splits\n",
    "\n",
    "        cols_representation = pd.DataFrame(cols_representation, columns=X.columns)\n",
    "        \n",
    "        return cols_representation\n",
    "\n",
    "\n",
    "class FrequencyEncoder:\n",
    "    def __init__(self, cols):\n",
    "        self.cols = cols\n",
    "        self.counts_dict = None\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y=None) -> pd.DataFrame:\n",
    "        counts_dict = {}\n",
    "        for col in self.cols:\n",
    "            values, counts = np.unique(X[col], return_counts=True)\n",
    "            counts_dict[col] = dict(zip(values, counts))\n",
    "        self.counts_dict = counts_dict\n",
    "\n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        counts_dict_test = {}\n",
    "        res = []\n",
    "        for col in self.cols:\n",
    "            values, counts = np.unique(X[col], return_counts=True)\n",
    "            counts_dict_test[col] = dict(zip(values, counts))\n",
    "\n",
    "            # if value is in \"train\" keys - replace \"test\" counts with \"train\" counts\n",
    "            for k in [key for key in counts_dict_test[col].keys() if key in self.counts_dict[col].keys()]:\n",
    "                counts_dict_test[col][k] = self.counts_dict[col][k]\n",
    "\n",
    "            res.append(X[col].map(counts_dict_test[col]).values.reshape(-1, 1))\n",
    "        res = np.hstack(res)\n",
    "\n",
    "        X[self.cols] = res\n",
    "        return X\n",
    "\n",
    "    def fit_transform(self, X: pd.DataFrame, y=None) -> pd.DataFrame:\n",
    "        self.fit(X, y)\n",
    "        X = self.transform(X)\n",
    "        return X\n",
    "\n",
    "## Reduce memory usage\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing Date columns\n",
    "train = pd.read_csv('Train.csv',parse_dates=['TransactionStartTime','PaidOnDate','DueDate','IssuedDateLoan'])\n",
    "test = pd.read_csv('Test.csv',parse_dates=['TransactionStartTime','IssuedDateLoan'])\n",
    "sample =  pd.read_csv('sample_submission.csv')\n",
    "unlinked_masked_final = pd.read_csv('unlinked_masked_final.csv',parse_dates=['TransactionStartTime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (train['Value'].min() , 'and' , train['Value'].max() ,'loan values')\n",
    "print (test['Value'].min() , 'and' , test['Value'].max() ,'loan values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2100, 27) (905, 19) (16327, 12)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape, test.shape, unlinked_masked_final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#categorical and numerical columns into variables\n",
    "num_col = train.select_dtypes(include=np.number).columns\n",
    "cat_col = train.select_dtypes(exclude=np.number).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to  0.35 Mb (19.9% reduction)\n",
      "Mem. usage decreased to  0.11 Mb (13.8% reduction)\n"
     ]
    }
   ],
   "source": [
    "#lets reduce memory usage\n",
    "train = reduce_mem_usage(train)\n",
    "test = reduce_mem_usage(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomerId                 0\n",
       "TransactionStartTime       0\n",
       "Value                      0\n",
       "Amount                     0\n",
       "TransactionId              0\n",
       "BatchId                    0\n",
       "SubscriptionId             0\n",
       "CurrencyCode               0\n",
       "CountryCode                0\n",
       "ProviderId                 0\n",
       "ProductId                  0\n",
       "ProductCategory            0\n",
       "ChannelId                  0\n",
       "TransactionStatus          0\n",
       "IssuedDateLoan           612\n",
       "AmountLoan               612\n",
       "Currency                 612\n",
       "LoanId                   612\n",
       "PaidOnDate               612\n",
       "IsFinalPayBack           612\n",
       "InvestorId               612\n",
       "DueDate                  614\n",
       "LoanApplicationId        617\n",
       "PayBackId                612\n",
       "ThirdPartyId             614\n",
       "IsThirdPartyConfirmed    612\n",
       "IsDefaulted              612\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking for missing value\n",
    "train.apply(lambda x: sum(x.isnull()), axis =0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomerId                0\n",
       "TransactionStartTime      0\n",
       "Value                     0\n",
       "Amount                    0\n",
       "TransactionId             0\n",
       "BatchId                   0\n",
       "SubscriptionId            0\n",
       "CurrencyCode              0\n",
       "CountryCode               0\n",
       "ProviderId                0\n",
       "ProductId                 0\n",
       "ProductCategory           0\n",
       "ChannelId                 0\n",
       "TransactionStatus         0\n",
       "IssuedDateLoan          427\n",
       "LoanId                  427\n",
       "InvestorId              427\n",
       "LoanApplicationId       427\n",
       "ThirdPartyId            427\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking for missing value\n",
    "test.apply(lambda x: sum(x.isnull()), axis =0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#After findings, missing values was as a result of rejected loan or no loan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TransactionStartTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#No transaction has a multiple Default state, its either a transaction was paid totally or defaulted totally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ChannelId', 'Currency', 'CurrencyCode', 'ProviderId', 'InvestorId',\n",
       "       'SubscriptionId', 'ProductCategory', 'ProductId', 'CustomerId',\n",
       "       'DueDate', 'LoanApplicationId', 'IssuedDateLoan', 'LoanId',\n",
       "       'ThirdPartyId', 'PaidOnDate', 'PayBackId', 'BatchId', 'TransactionId',\n",
       "       'TransactionStartTime'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[cat_col].nunique().sort_values().index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not in Test ChannelId  []\n",
      "Not in Train ChannelId  []\n",
      "====================================\n",
      "Not in Test ProviderId  []\n",
      "Not in Train ProviderId  []\n",
      "====================================\n",
      "Not in Test InvestorId  [nan, 'InvestorId_3']\n",
      "Not in Train InvestorId  [nan]\n",
      "====================================\n",
      "Not in Test SubscriptionId  ['SubscriptionId_2', 'SubscriptionId_4', 'SubscriptionId_6']\n",
      "Not in Train SubscriptionId  ['SubscriptionId_3']\n",
      "====================================\n",
      "Not in Test ProductCategory  []\n",
      "Not in Train ProductCategory  ['ticket']\n",
      "====================================\n",
      "Not in Test ProductId  ['ProductId_16']\n",
      "Not in Train ProductId  ['ProductId_14', 'ProductId_12', 'ProductId_11']\n",
      "====================================\n",
      "Not in Test CustomerId  ['CustomerId_305', 'CustomerId_433', 'CustomerId_329', 'CustomerId_405', 'CustomerId_266', 'CustomerId_303', 'CustomerId_144', 'CustomerId_136', 'CustomerId_492', 'CustomerId_125', 'CustomerId_71', 'CustomerId_431', 'CustomerId_1', 'CustomerId_425', 'CustomerId_249', 'CustomerId_493', 'CustomerId_119', 'CustomerId_31', 'CustomerId_453', 'CustomerId_351', 'CustomerId_114', 'CustomerId_357', 'CustomerId_501', 'CustomerId_339', 'CustomerId_82', 'CustomerId_255', 'CustomerId_436', 'CustomerId_484', 'CustomerId_429', 'CustomerId_318', 'CustomerId_396', 'CustomerId_350', 'CustomerId_434', 'CustomerId_439', 'CustomerId_468', 'CustomerId_383', 'CustomerId_271', 'CustomerId_471', 'CustomerId_294', 'CustomerId_234', 'CustomerId_491', 'CustomerId_239', 'CustomerId_438', 'CustomerId_499', 'CustomerId_35', 'CustomerId_328', 'CustomerId_314', 'CustomerId_435', 'CustomerId_291', 'CustomerId_202', 'CustomerId_160', 'CustomerId_298', 'CustomerId_97', 'CustomerId_241', 'CustomerId_48', 'CustomerId_385', 'CustomerId_465', 'CustomerId_470', 'CustomerId_115', 'CustomerId_452', 'CustomerId_229', 'CustomerId_404', 'CustomerId_217', 'CustomerId_358', 'CustomerId_187', 'CustomerId_166', 'CustomerId_26', 'CustomerId_345', 'CustomerId_228', 'CustomerId_503', 'CustomerId_409', 'CustomerId_221', 'CustomerId_319', 'CustomerId_363', 'CustomerId_391', 'CustomerId_246', 'CustomerId_460', 'CustomerId_69', 'CustomerId_411', 'CustomerId_324', 'CustomerId_54', 'CustomerId_347', 'CustomerId_317', 'CustomerId_444', 'CustomerId_132', 'CustomerId_2', 'CustomerId_302', 'CustomerId_9', 'CustomerId_462', 'CustomerId_278', 'CustomerId_103', 'CustomerId_402', 'CustomerId_489', 'CustomerId_440', 'CustomerId_445', 'CustomerId_171', 'CustomerId_147', 'CustomerId_306', 'CustomerId_322', 'CustomerId_430', 'CustomerId_109', 'CustomerId_451', 'CustomerId_257', 'CustomerId_364', 'CustomerId_237', 'CustomerId_349', 'CustomerId_164', 'CustomerId_369', 'CustomerId_162', 'CustomerId_13', 'CustomerId_38', 'CustomerId_394', 'CustomerId_198', 'CustomerId_37', 'CustomerId_342', 'CustomerId_422', 'CustomerId_478', 'CustomerId_494', 'CustomerId_62', 'CustomerId_498', 'CustomerId_281', 'CustomerId_380', 'CustomerId_235', 'CustomerId_395', 'CustomerId_287', 'CustomerId_106', 'CustomerId_20', 'CustomerId_321', 'CustomerId_397', 'CustomerId_73', 'CustomerId_46', 'CustomerId_272', 'CustomerId_354', 'CustomerId_424', 'CustomerId_374', 'CustomerId_222', 'CustomerId_476', 'CustomerId_367', 'CustomerId_176', 'CustomerId_312', 'CustomerId_93', 'CustomerId_61', 'CustomerId_378', 'CustomerId_467', 'CustomerId_126', 'CustomerId_384', 'CustomerId_170', 'CustomerId_124', 'CustomerId_224', 'CustomerId_12', 'CustomerId_392', 'CustomerId_332', 'CustomerId_118', 'CustomerId_215', 'CustomerId_40', 'CustomerId_216', 'CustomerId_223', 'CustomerId_30', 'CustomerId_477', 'CustomerId_382', 'CustomerId_90', 'CustomerId_18', 'CustomerId_178', 'CustomerId_42', 'CustomerId_56', 'CustomerId_94', 'CustomerId_65', 'CustomerId_175', 'CustomerId_11', 'CustomerId_87', 'CustomerId_84', 'CustomerId_177', 'CustomerId_74', 'CustomerId_41', 'CustomerId_112', 'CustomerId_173', 'CustomerId_141', 'CustomerId_475', 'CustomerId_231', 'CustomerId_232', 'CustomerId_204', 'CustomerId_490', 'CustomerId_149', 'CustomerId_137', 'CustomerId_113', 'CustomerId_161', 'CustomerId_227', 'CustomerId_68', 'CustomerId_427', 'CustomerId_78', 'CustomerId_192', 'CustomerId_486', 'CustomerId_139', 'CustomerId_4', 'CustomerId_417', 'CustomerId_95', 'CustomerId_183', 'CustomerId_366', 'CustomerId_191', 'CustomerId_88', 'CustomerId_70', 'CustomerId_206', 'CustomerId_220', 'CustomerId_15', 'CustomerId_8', 'CustomerId_50', 'CustomerId_134', 'CustomerId_226', 'CustomerId_479', 'CustomerId_301', 'CustomerId_299', 'CustomerId_45', 'CustomerId_195', 'CustomerId_10', 'CustomerId_214', 'CustomerId_365', 'CustomerId_244', 'CustomerId_6', 'CustomerId_131', 'CustomerId_76', 'CustomerId_289', 'CustomerId_207', 'CustomerId_80', 'CustomerId_472', 'CustomerId_163', 'CustomerId_181', 'CustomerId_443', 'CustomerId_146', 'CustomerId_51', 'CustomerId_209', 'CustomerId_194', 'CustomerId_64', 'CustomerId_448', 'CustomerId_143', 'CustomerId_174', 'CustomerId_3', 'CustomerId_449', 'CustomerId_196', 'CustomerId_338', 'CustomerId_7', 'CustomerId_157', 'CustomerId_210', 'CustomerId_152', 'CustomerId_25', 'CustomerId_182', 'CustomerId_401', 'CustomerId_111', 'CustomerId_441', 'CustomerId_389', 'CustomerId_22', 'CustomerId_371', 'CustomerId_240', 'CustomerId_86', 'CustomerId_201', 'CustomerId_208', 'CustomerId_410', 'CustomerId_352', 'CustomerId_418', 'CustomerId_230', 'CustomerId_123', 'CustomerId_116', 'CustomerId_167', 'CustomerId_159', 'CustomerId_284', 'CustomerId_100', 'CustomerId_5', 'CustomerId_268', 'CustomerId_238', 'CustomerId_340', 'CustomerId_442', 'CustomerId_450', 'CustomerId_263', 'CustomerId_165', 'CustomerId_150', 'CustomerId_218', 'CustomerId_189', 'CustomerId_117', 'CustomerId_32', 'CustomerId_184', 'CustomerId_307', 'CustomerId_29', 'CustomerId_81', 'CustomerId_286', 'CustomerId_17', 'CustomerId_190', 'CustomerId_172', 'CustomerId_36', 'CustomerId_454', 'CustomerId_500', 'CustomerId_105', 'CustomerId_14', 'CustomerId_252', 'CustomerId_154', 'CustomerId_101', 'CustomerId_128', 'CustomerId_156', 'CustomerId_33', 'CustomerId_140', 'CustomerId_275', 'CustomerId_153', 'CustomerId_455', 'CustomerId_245', 'CustomerId_420', 'CustomerId_43', 'CustomerId_169', 'CustomerId_92', 'CustomerId_59', 'CustomerId_225', 'CustomerId_360', 'CustomerId_91', 'CustomerId_203', 'CustomerId_102', 'CustomerId_63', 'CustomerId_19', 'CustomerId_414', 'CustomerId_158', 'CustomerId_53', 'CustomerId_341', 'CustomerId_495', 'CustomerId_24', 'CustomerId_400', 'CustomerId_211', 'CustomerId_219', 'CustomerId_49', 'CustomerId_387', 'CustomerId_133', 'CustomerId_335', 'CustomerId_421', 'CustomerId_16', 'CustomerId_205']\n",
      "Not in Train CustomerId  ['CustomerId_107', 'CustomerId_250', 'CustomerId_293', 'CustomerId_251', 'CustomerId_77', 'CustomerId_464', 'CustomerId_262', 'CustomerId_373', 'CustomerId_426', 'CustomerId_377', 'CustomerId_333', 'CustomerId_386', 'CustomerId_21', 'CustomerId_23', 'CustomerId_504', 'CustomerId_483', 'CustomerId_96', 'CustomerId_236', 'CustomerId_487', 'CustomerId_416', 'CustomerId_412', 'CustomerId_337', 'CustomerId_375', 'CustomerId_456', 'CustomerId_300', 'CustomerId_186', 'CustomerId_121', 'CustomerId_138', 'CustomerId_463', 'CustomerId_413', 'CustomerId_331', 'CustomerId_485', 'CustomerId_469', 'CustomerId_481', 'CustomerId_327', 'CustomerId_52', 'CustomerId_264', 'CustomerId_148', 'CustomerId_199', 'CustomerId_60', 'CustomerId_260', 'CustomerId_408', 'CustomerId_248', 'CustomerId_457', 'CustomerId_482', 'CustomerId_309', 'CustomerId_497', 'CustomerId_466', 'CustomerId_104', 'CustomerId_459', 'CustomerId_480', 'CustomerId_353', 'CustomerId_428', 'CustomerId_403', 'CustomerId_270', 'CustomerId_330', 'CustomerId_355', 'CustomerId_447', 'CustomerId_290', 'CustomerId_85', 'CustomerId_362', 'CustomerId_320', 'CustomerId_313', 'CustomerId_496', 'CustomerId_67', 'CustomerId_437', 'CustomerId_277', 'CustomerId_58', 'CustomerId_368', 'CustomerId_129', 'CustomerId_283', 'CustomerId_423', 'CustomerId_295', 'CustomerId_55', 'CustomerId_473', 'CustomerId_361', 'CustomerId_415', 'CustomerId_343', 'CustomerId_356', 'CustomerId_200', 'CustomerId_83', 'CustomerId_280', 'CustomerId_372']\n",
      "====================================\n"
     ]
    }
   ],
   "source": [
    "for col in ['ChannelId','ProviderId', 'InvestorId',\n",
    "       'SubscriptionId', 'ProductCategory', 'ProductId', 'CustomerId']:\n",
    "    train_col = train[col].unique()\n",
    "    test_col = test[col].unique()\n",
    "    \n",
    "    print ('Not in Test ' + col + ' ',[i  for i in train_col if i not in test_col])\n",
    "    print ('Not in Train ' + col + ' ',[i  for i in test_col  if i not in train_col])\n",
    "    print('==' * 18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequency Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_FE(train, test, cols, normalize = True, ext_train = None, ext_test= None):\n",
    "    norm = normalize\n",
    "    for col in cols:\n",
    "        if ext_train is None:\n",
    "            df = pd.concat([train[col],test[col]])\n",
    "            nm = col+'_FE'\n",
    "        else:\n",
    "            df = pd.concat([ext_train[col],ext_test[col]])\n",
    "            nm = \"rejected\"+\"_\"+ col +\"_FE\" \n",
    "        vc = df.value_counts(dropna=True, normalize=norm).to_dict()\n",
    "        vc[-1] = -1\n",
    "        train[nm] = train[col].map(vc)\n",
    "        train[nm] = train[nm].astype('float32')\n",
    "        test[nm] = test[col].map(vc)\n",
    "        test[nm] = test[nm].astype('float32')\n",
    "        train[nm].fillna(0,inplace=True)\n",
    "        test[nm].fillna(0,inplace=True)\n",
    "                \n",
    "        del df; x=gc.collect()\n",
    "        print(nm,', ',end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LABEL ENCODE\n",
    "def encode_LE(train,test,cols,verbose=True):\n",
    "    for col in cols:\n",
    "        df_comb = pd.concat([train[col],test[col]],axis=0)\n",
    "        df_comb,_ = df_comb.factorize(sort=True)\n",
    "        nm = col\n",
    "        if df_comb.max()>32000: \n",
    "            train[nm] = df_comb[:len(train)].astype('int32')\n",
    "            test[nm] = df_comb[len(train):].astype('int32')\n",
    "        else:\n",
    "            train[nm] = df_comb[:len(train)].astype('int16')\n",
    "            test[nm] = df_comb[len(train):].astype('int16')\n",
    "        del df_comb; x=gc.collect()\n",
    "        if verbose: print(nm,', ',end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_AG__2(group ,main_columns, aggregations, train_df=train, test_df=test, ext_src=None,\n",
    "              fillna=True, usena=False):\n",
    "    # AGGREGATION OF MAIN WITH UID FOR GIVEN STATISTICS\n",
    "    for main_column in main_columns:  \n",
    "            for agg_type in aggregations:\n",
    "                if ext_src is None: \n",
    "                    temp_df = pd.concat([train_df[group +[main_column]], test_df[group +[main_column]]])\n",
    "                    new_col_name = group[0]+\"_\"+group[1]+\"_\"+main_column+'_'+agg_type\n",
    "                                    \n",
    "                else:\n",
    "                    temp_df = ext_src.copy()\n",
    "                    new_col_name = \"ext_data\"+ \"_\"+group[0]+\"_\"+group[1]+\"_\"+main_column+'_'+agg_type\n",
    "                    \n",
    "                if usena: temp_df.loc[temp_df[main_column]==-1,main_column] = np.nan\n",
    "                temp_df = temp_df.groupby(group)[main_column].agg([agg_type]).reset_index(level=group).rename(\n",
    "                                                        columns={agg_type: new_col_name})\n",
    "                \n",
    "                \n",
    "\n",
    "\n",
    "                train_df[new_col_name] = pd.merge(train_df, temp_df, on=group, how='left')[new_col_name].astype('float32')\n",
    "                test_df[new_col_name]  = pd.merge(test_df, temp_df, on=group, how='left')[new_col_name].astype('float32')\n",
    "                \n",
    "                if fillna:\n",
    "                    train_df[new_col_name].fillna(-1,inplace=True)\n",
    "                    test_df[new_col_name].fillna(-1,inplace=True)\n",
    "                \n",
    "                print(\"'\"+new_col_name+\"'\",', ',end='')\n",
    "                \n",
    "# GROUP AGGREGATION MEAN AND STD\n",
    "# https://www.kaggle.com/kyakovlev/ieee-fe-with-some-eda\n",
    "def encode_AG(uids ,main_columns, aggregations, train_df=train, test_df=test, ext_src=None,\n",
    "              fillna=True, usena=False):\n",
    "    # AGGREGATION OF MAIN WITH UID FOR GIVEN STATISTICS\n",
    "    for main_column in main_columns:  \n",
    "        for col in uids:\n",
    "            for agg_type in aggregations:\n",
    "                if ext_src is None: \n",
    "                    temp_df = pd.concat([train_df[[col, main_column]], test_df[[col,main_column]]])\n",
    "                    new_col_name = main_column+'_'+col+'_'+agg_type\n",
    "                                    \n",
    "                else:\n",
    "                    temp_df = ext_src.copy()\n",
    "                    new_col_name = \"ext_data\"+ \"_\"+main_column+'_'+col+'_'+agg_type\n",
    "\n",
    "                if usena: temp_df.loc[temp_df[main_column]==-1,main_column] = np.nan\n",
    "                temp_df = temp_df.groupby([col])[main_column].agg([agg_type]).reset_index().rename(\n",
    "                                                        columns={agg_type: new_col_name})\n",
    "\n",
    "                temp_df.index = list(temp_df[col])\n",
    "                temp_df = temp_df[new_col_name].to_dict()   \n",
    "\n",
    "                train_df[new_col_name] = train_df[col].map(temp_df).astype('float32')\n",
    "                test_df[new_col_name]  = test_df[col].map(temp_df).astype('float32')\n",
    "                \n",
    "                if fillna:\n",
    "                    train_df[new_col_name].fillna(-1,inplace=True)\n",
    "                    test_df[new_col_name].fillna(-1,inplace=True)\n",
    "                \n",
    "                print(\"'\"+new_col_name+\"'\",', ',end='')\n",
    "                \n",
    "                \n",
    "# COMBINE FEATURES\n",
    "def encode_CB(col1,col2,df1=train,df2=test):\n",
    "    nm = col1+'_'+col2\n",
    "    df1[nm] = df1[col1].astype(str)+'_'+df1[col2].astype(str)\n",
    "    df2[nm] = df2[col1].astype(str)+'_'+df2[col2].astype(str) \n",
    "#     encode_LE(nm,verbose=False)\n",
    "    print(nm,', ',end='')\n",
    "    \n",
    "# GROUP AGGREGATION NUNIQUE\n",
    "def encode_AG2( uids,main_columns, train_df=train, test_df=test):\n",
    "    for main_column in main_columns:  \n",
    "        for col in uids:\n",
    "            comb = pd.concat([train_df[[col]+[main_column]],test_df[[col]+[main_column]]],axis=0)\n",
    "            mp = comb.groupby(col)[main_column].agg(['nunique'])['nunique'].to_dict()\n",
    "            train_df[col+'_'+main_column+'_ct'] = train_df[col].map(mp).astype('float32')\n",
    "            test_df[col+'_'+main_column+'_ct'] = test_df[col].map(mp).astype('float32')\n",
    "            print(col+'_'+main_column+'_ct, ',end='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df rejected loan\n",
    "df_rejected = train[train.IsDefaulted.isnull()]\n",
    "# remove duplicate Transaction to avoid noisy parameters\n",
    "df_rejected.drop_duplicates(subset=['CustomerId','TransactionId'], keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "## Get number of rejected loan for each customer\n",
    "df_rejected_test = test[test.LoanId.isnull()]\n",
    "## remove multiple, duplicate Transaction\n",
    "df_rejected_test.drop_duplicates( subset=['CustomerId','TransactionId'], keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rejected loan\n",
    "cleaned_test = test.iloc[test.IssuedDateLoan[~test.LoanId.isnull()].index]\n",
    "cleaned_train = train.iloc[train.IssuedDateLoan[~train.LoanId.isnull()].index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rejected_CustomerId_FE , CustomerId_TransactionId_ct, "
     ]
    }
   ],
   "source": [
    "# Get count of defaulters rejected loan\n",
    "encode_FE(cleaned_train, cleaned_test, ['CustomerId'], normalize=False, ext_train=df_rejected, ext_test=df_rejected_test)\n",
    "# count of accepted loan\n",
    "encode_AG2(['CustomerId'] ,  ['TransactionId'], train_df=cleaned_train, test_df=cleaned_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get loan ratio\n",
    "cleaned_train[\"rejected_loan_ratio\"] = cleaned_train.rejected_CustomerId_FE/(cleaned_train.rejected_CustomerId_FE + cleaned_train.CustomerId_TransactionId_ct)\n",
    "cleaned_test[\"rejected_loan_ratio\"] = cleaned_test.rejected_CustomerId_FE/(cleaned_test.rejected_CustomerId_FE + cleaned_test.CustomerId_TransactionId_ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9       0.105263\n",
       "10      0.105263\n",
       "11      0.105263\n",
       "13      0.020000\n",
       "14      0.020000\n",
       "          ...   \n",
       "2095    0.200000\n",
       "2096    0.200000\n",
       "2097    0.285714\n",
       "2098    0.285714\n",
       "2099    0.086957\n",
       "Name: rejected_loan_ratio, Length: 1488, dtype: float32"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_train[\"rejected_loan_ratio\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Value_CustomerId_mean' , 'Value_CustomerId_min' , 'Value_CustomerId_max' , 'Value_CustomerId_std' , CustomerId_ProductId_ct, CustomerId_ProductCategory_ct, "
     ]
    }
   ],
   "source": [
    "encode_AG(['CustomerId'] ,  ['Value'], ['mean','min','max','std'], train_df=cleaned_train, test_df=cleaned_test, \n",
    "              fillna=True, usena=False)\n",
    "# how many type of product this customers are used to\n",
    "encode_AG2(['CustomerId'] ,  ['ProductId', 'ProductCategory'], train_df=cleaned_train, test_df=cleaned_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomerId_FE , TransactionId_FE , CustomerId_TransactionId_ct, "
     ]
    }
   ],
   "source": [
    "# Get mean/median number of times a customer pays a loan, this will help to know the customers paying ability\n",
    "#Get count of customer\n",
    "encode_FE(cleaned_train, cleaned_test, ['CustomerId', 'TransactionId'], normalize=False)\n",
    "# Get unique number of transaction of customers\n",
    "encode_AG2(['CustomerId'] ,  ['TransactionId'], train_df=cleaned_train, test_df=cleaned_test)\n",
    "\n",
    "cleaned_train[\"meanTransactionPerLoan\"] = cleaned_train[\"CustomerId_FE\"]/cleaned_train[\"CustomerId_TransactionId_ct\"]\n",
    "cleaned_test[\"meanTransactionPerLoan\"] = cleaned_test[\"CustomerId_FE\"]/cleaned_test[\"CustomerId_TransactionId_ct\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Divide meanTransac by the current trans\n",
    "cleaned_train[\"Value_Mean_Ratio\"] = cleaned_train[\"Value\"]/cleaned_train[\"Value_CustomerId_mean\"]\n",
    "cleaned_test[\"Value_Mean_Ratio\"] = cleaned_test[\"Value\"]/cleaned_test[\"Value_CustomerId_mean\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Subtract meanTransac by the current trans\n",
    "cleaned_train[\"Value_Mean_Minus\"] = cleaned_train[\"Value\"]-cleaned_train[\"Value_CustomerId_mean\"]\n",
    "cleaned_test[\"Value_Mean_Minus\"] = cleaned_test[\"Value\"]-cleaned_test[\"Value_CustomerId_mean\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProductId_ProductCategory , "
     ]
    }
   ],
   "source": [
    "#combine product and ca`btegorical(product category)\n",
    "encode_CB(\"ProductId\",\"ProductCategory\",df1=cleaned_train,df2=cleaned_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'ext_data_Value_CustomerId_mean' , 'ext_data_Value_CustomerId_min' , 'ext_data_Value_CustomerId_max' , 'ext_data_Value_CustomerId_std' , "
     ]
    }
   ],
   "source": [
    "#Get max and min of customer expense per product using previous transaction data\n",
    "encode_AG(['CustomerId'] ,  ['Value'], ['mean','min','max','std'], train_df=cleaned_train, test_df=cleaned_test, \n",
    "              fillna=True, usena=False, ext_src=unlinked_masked_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'CustomerId_ProductId_Value_mean' , 'CustomerId_ProductId_Value_min' , 'CustomerId_ProductId_Value_max' , 'CustomerId_ProductId_Value_std' , 'CustomerId_ProductCategory_Value_mean' , 'CustomerId_ProductCategory_Value_min' , 'CustomerId_ProductCategory_Value_max' , 'CustomerId_ProductCategory_Value_std' , "
     ]
    }
   ],
   "source": [
    "# mean,max,min,std of cost of previous loan per productId\n",
    "encode_AG__2(['CustomerId',\"ProductId\"] ,  ['Value'], ['mean','min','max','std'], train_df=cleaned_train, test_df=cleaned_test, \n",
    "              fillna=True, usena=False)\n",
    "\n",
    "# mean,max,min,std of cost of previous loan per productCategory\n",
    "encode_AG__2(['CustomerId',\"ProductCategory\"] ,  ['Value'], ['mean','min','max','std'], train_df=cleaned_train, test_df=cleaned_test, \n",
    "              fillna=True, usena=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'ext_data_CustomerId_ProductId_Value_mean' , 'ext_data_CustomerId_ProductId_Value_min' , 'ext_data_CustomerId_ProductId_Value_max' , 'ext_data_CustomerId_ProductId_Value_std' , 'ext_data_CustomerId_ProductCategory_Value_mean' , 'ext_data_CustomerId_ProductCategory_Value_min' , 'ext_data_CustomerId_ProductCategory_Value_max' , 'ext_data_CustomerId_ProductCategory_Value_std' , "
     ]
    }
   ],
   "source": [
    "# mean,max,min,std of cost of previous loan per productId\n",
    "encode_AG__2(['CustomerId',\"ProductId\"] ,  ['Value'], ['mean','min','max','std'], train_df=cleaned_train, test_df=cleaned_test, \n",
    "              fillna=True, usena=False, ext_src=unlinked_masked_final)\n",
    "\n",
    "# mean,max,min,std of cost of previous loan per productCategory\n",
    "encode_AG__2(['CustomerId',\"ProductCategory\"] ,  ['Value'], ['mean','min','max','std'], train_df=cleaned_train, test_df=cleaned_test, \n",
    "              fillna=True, usena=False, ext_src=unlinked_masked_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ratio of product_value mean to current value mean\n",
    "cleaned_train[\"CustomerId_ProductCategory_Value_Ratio\"] = cleaned_train[\"Value\"]/cleaned_train[\"CustomerId_ProductCategory_Value_mean\"]\n",
    "cleaned_test[\"CustomerId_ProductCategory_Value_Ratio\"] = cleaned_test[\"Value\"]/cleaned_test[\"CustomerId_ProductCategory_Value_mean\"]\n",
    "\n",
    "# Ratio of product_value mean to current value mean\n",
    "cleaned_train[\"CustomerId_ProductId_Value_Ratio\"] = cleaned_train[\"Value\"]/cleaned_train[\"CustomerId_ProductId_Value_mean\"]\n",
    "cleaned_test[\"CustomerId_ProductId_Value_Ratio\"] = cleaned_test[\"Value\"]/cleaned_test[\"CustomerId_ProductId_Value_mean\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ratio of product_value mean to current value mean\n",
    "cleaned_train[\"ext_data_CustomerId_ProductCategory_Value_Ratio\"] = cleaned_train[\"Value\"]/cleaned_train[\"ext_data_CustomerId_ProductCategory_Value_mean\"]\n",
    "cleaned_test[\"ext_data_CustomerId_ProductCategory_Value_Ratio\"] = cleaned_test[\"Value\"]/cleaned_test[\"ext_data_CustomerId_ProductCategory_Value_mean\"]\n",
    "\n",
    "# Ratio of product_value mean to current value mean\n",
    "cleaned_train[\"ext_data_CustomerId_ProductId_Value_Ratio\"] = cleaned_train[\"Value\"]/cleaned_train[\"ext_data_CustomerId_ProductId_Value_mean\"]\n",
    "cleaned_test[\"ext_data_CustomerId_ProductId_Value_Ratio\"] = cleaned_test[\"Value\"]/cleaned_test[\"ext_data_CustomerId_ProductId_Value_mean\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove duplicate transaction because they have the same transaction id, and we will be able to track the timeframe for transactions\n",
    "cleaned_train.drop_duplicates(subset=['CustomerId','TransactionId'], keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomerId_FE , TransactionId_FE , CustomerId , ProductCategory , ProductId , SubscriptionId , InvestorId , ProductId_ProductCategory , "
     ]
    }
   ],
   "source": [
    "#frequency encode\n",
    "encode_FE(cleaned_train, cleaned_test, ['CustomerId', 'TransactionId'])\n",
    "#Label encode customer Id\n",
    "encode_LE(cleaned_train, cleaned_test, ['CustomerId'])\n",
    "# label Encode\n",
    "encode_LE(cleaned_train, cleaned_test,['ProductCategory', 'ProductId','SubscriptionId','InvestorId',\"ProductId_ProductCategory\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TIME PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_trans(train, cols):\n",
    "    for col in cols:\n",
    "        \n",
    "# Add features from date time\n",
    "        attr = ['Month', 'Week', 'Day', 'Dayofweek', 'Dayofyear',\n",
    "                'Is_month_end', 'Is_month_start', 'Is_quarter_end', 'Is_quarter_start', 'Is_year_end', 'Is_year_start'] +['Hour', 'Minute', 'Second']\n",
    "        for n in attr: train[col + n] = getattr(train[col].dt, n.lower())\n",
    "        train[col + 'Elapsed'] = train[col].astype(np.int64) // 10 ** 9\n",
    "        \n",
    "def date_trans_due(train, cols):\n",
    "    for col in cols:\n",
    "        \n",
    "# Add features from date time\n",
    "        attr = ['Day', 'Dayofweek',\n",
    "                'Is_month_end', 'Is_month_start', 'Is_quarter_end', 'Is_quarter_start', 'Is_year_end', 'Is_year_start']\n",
    "        for n in attr: train[col + n] = getattr(train[col].dt, n.lower())\n",
    "        train[col + 'Elapsed'] = train[col].astype(np.int64) // 10 ** 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill missing dueDate\n",
    "cleaned_train.DueDate.fillna(cleaned_train['IssuedDateLoan'] +  pd.to_timedelta(30, unit='d'),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TransactionTime\n",
    "date_trans(cleaned_train, ['TransactionStartTime'])\n",
    "date_trans(cleaned_test, ['TransactionStartTime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Issue Time\n",
    "cleaned_train['Issue_Trans_Diff'] = (cleaned_train.IssuedDateLoan - cleaned_train.TransactionStartTime).dt.total_seconds()\n",
    "cleaned_test['Issue_Trans_Diff'] = (cleaned_test.IssuedDateLoan - cleaned_test.TransactionStartTime).dt.total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all Due days are 30 Days, so calculate for test\n",
    "# Calculate due date in test since not in train\n",
    "# DueDate\n",
    "cleaned_test['DueDate'] = cleaned_test['IssuedDateLoan'] +  pd.to_timedelta(30, unit='d')\n",
    "date_trans_due(cleaned_train, ['DueDate'])\n",
    "date_trans_due(cleaned_test, ['DueDate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BOOLEAN TO INT\n",
    "cleaned_train[cleaned_train.select_dtypes(include='bool').columns] = cleaned_train.select_dtypes(include='bool').astype(int)\n",
    "cleaned_test[cleaned_test.select_dtypes(include='bool').columns] = cleaned_test.select_dtypes(include='bool').astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRINCIPAL COMPONENT ANALYSIS / CLUSTERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_start = ['Value', 'Amount', 'ProductId', 'ProductCategory', 'TransactionStatus',\n",
    "       'InvestorId', 'rejected_CustomerId_FE', 'CustomerId_TransactionId_ct',\n",
    "       'rejected_loan_ratio', 'Value_CustomerId_mean', 'Value_CustomerId_min',\n",
    "       'Value_CustomerId_max', 'Value_CustomerId_std',\n",
    "       'CustomerId_ProductId_ct', 'CustomerId_ProductCategory_ct',\n",
    "       'CustomerId_FE', 'TransactionId_FE', 'meanTransactionPerLoan',\n",
    "       'Value_Mean_Ratio', 'Value_Mean_Minus', 'ProductId_ProductCategory',\n",
    "       'ext_data_Value_CustomerId_mean', 'ext_data_Value_CustomerId_min',\n",
    "       'ext_data_Value_CustomerId_max', 'ext_data_Value_CustomerId_std',\n",
    "       'CustomerId_ProductId_Value_mean', 'CustomerId_ProductId_Value_min',\n",
    "       'CustomerId_ProductId_Value_max', 'CustomerId_ProductId_Value_std',\n",
    "       'CustomerId_ProductCategory_Value_mean',\n",
    "       'CustomerId_ProductCategory_Value_min',\n",
    "       'CustomerId_ProductCategory_Value_max',\n",
    "       'CustomerId_ProductCategory_Value_std',\n",
    "       'ext_data_CustomerId_ProductId_Value_mean',\n",
    "       'ext_data_CustomerId_ProductId_Value_min',\n",
    "       'ext_data_CustomerId_ProductId_Value_max',\n",
    "       'ext_data_CustomerId_ProductId_Value_std',\n",
    "       'ext_data_CustomerId_ProductCategory_Value_mean',\n",
    "       'ext_data_CustomerId_ProductCategory_Value_min',\n",
    "       'ext_data_CustomerId_ProductCategory_Value_max',\n",
    "       'ext_data_CustomerId_ProductCategory_Value_std',\n",
    "       'CustomerId_ProductCategory_Value_Ratio',\n",
    "       'CustomerId_ProductId_Value_Ratio',\n",
    "       'ext_data_CustomerId_ProductCategory_Value_Ratio',\n",
    "       'ext_data_CustomerId_ProductId_Value_Ratio',\n",
    "       'TransactionStartTimeMonth', 'TransactionStartTimeWeek',\n",
    "       'TransactionStartTimeDay', 'TransactionStartTimeDayofweek',\n",
    "       'TransactionStartTimeDayofyear', 'TransactionStartTimeIs_month_end',\n",
    "       'TransactionStartTimeIs_month_start',\n",
    "       'TransactionStartTimeIs_quarter_end',\n",
    "       'TransactionStartTimeIs_quarter_start',\n",
    "       'TransactionStartTimeIs_year_end', 'TransactionStartTimeIs_year_start',\n",
    "       'TransactionStartTimeHour', 'TransactionStartTimeMinute',\n",
    "       'TransactionStartTimeSecond', 'TransactionStartTimeElapsed',\n",
    "       'Issue_Trans_Diff', 'DueDateDay', 'DueDateDayofweek',\n",
    "       'DueDateIs_month_end', 'DueDateIs_month_start', 'DueDateIs_quarter_end',\n",
    "       'DueDateIs_quarter_start', 'DueDateIs_year_end', 'DueDateIs_year_start',\n",
    "       'DueDateElapsed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.concat([cleaned_train, cleaned_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pca\n",
    "from sklearn import decomposition\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Create a scaler object\n",
    "sc = StandardScaler()\n",
    "# Fit the scaler to the features and transform\n",
    "X_std = sc.fit_transform(all_data[col_start])\n",
    "# Create a pca object with the 2 components as a parameter\n",
    "pca = decomposition.PCA(n_components=3)\n",
    "# Fit the PCA and transform the data\n",
    "X_std_pca = pca.fit_transform(X_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[\"pc1\"] = 0\n",
    "all_data[\"pc2\"] = 0\n",
    "all_data[\"pc3\"] = 0\n",
    "all_data[[\"pc1\",\"pc2\",\"pc3\"]] = X_std_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=5, random_state=1996).fit(all_data[col_start])\n",
    "all_data[\"kmeans\"] = kmeans.predict(all_data[col_start])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_train = all_data[:1153]\n",
    "cleaned_test = all_data[1153:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_col = ['CustomerId', \n",
    "             'Value', \n",
    "            'SubscriptionId', \n",
    "            'ProductId', \n",
    "            'ProductCategory',\n",
    "       \n",
    "        'IsDefaulted', \n",
    "             'rejected_CustomerId_FE',\n",
    "       'CustomerId_TransactionId_ct',\n",
    "             'rejected_loan_ratio',\n",
    "       'Value_CustomerId_mean', 'Value_CustomerId_min', 'Value_CustomerId_max',\n",
    "       'Value_CustomerId_std', 'CustomerId_ProductId_ct',\n",
    "       'CustomerId_ProductCategory_ct', 'CustomerId_FE', 'TransactionId_FE',\n",
    "       'meanTransactionPerLoan', 'Value_Mean_Ratio', 'Value_Mean_Minus',\n",
    "       'ProductId_ProductCategory', 'ext_data_Value_CustomerId_mean',\n",
    "       'ext_data_Value_CustomerId_min', 'ext_data_Value_CustomerId_max',\n",
    "       'ext_data_Value_CustomerId_std', 'CustomerId_ProductId_Value_mean',\n",
    "       'CustomerId_ProductId_Value_min', 'CustomerId_ProductId_Value_max',\n",
    "       'CustomerId_ProductId_Value_std',\n",
    "       'CustomerId_ProductCategory_Value_mean',\n",
    "       'CustomerId_ProductCategory_Value_min',\n",
    "       'CustomerId_ProductCategory_Value_max',\n",
    "       'CustomerId_ProductCategory_Value_std',\n",
    "       'ext_data_CustomerId_ProductId_Value_mean',\n",
    "       'ext_data_CustomerId_ProductId_Value_min',\n",
    "       'ext_data_CustomerId_ProductId_Value_max',\n",
    "       'ext_data_CustomerId_ProductId_Value_std',\n",
    "       'ext_data_CustomerId_ProductCategory_Value_mean',\n",
    "       'ext_data_CustomerId_ProductCategory_Value_min',\n",
    "       'ext_data_CustomerId_ProductCategory_Value_max',\n",
    "       'ext_data_CustomerId_ProductCategory_Value_std',\n",
    "       'CustomerId_ProductCategory_Value_Ratio',\n",
    "       'CustomerId_ProductId_Value_Ratio',\n",
    "       'ext_data_CustomerId_ProductCategory_Value_Ratio',\n",
    "       'ext_data_CustomerId_ProductId_Value_Ratio',\n",
    "       'TransactionStartTimeMonth', 'TransactionStartTimeWeek',\n",
    "       'TransactionStartTimeDay', 'TransactionStartTimeDayofweek',\n",
    "       'TransactionStartTimeDayofyear', 'TransactionStartTimeIs_month_end',\n",
    "       'TransactionStartTimeIs_month_start',\n",
    "       'TransactionStartTimeIs_quarter_end',\n",
    "       'TransactionStartTimeIs_quarter_start',\n",
    "       'TransactionStartTimeHour', 'TransactionStartTimeMinute',\n",
    "       'TransactionStartTimeSecond', 'TransactionStartTimeElapsed',\n",
    "       'Issue_Trans_Diff', 'DueDateDay', 'DueDateDayofweek',\n",
    "       'DueDateIs_month_end', 'DueDateIs_month_start', 'DueDateIs_quarter_end',\n",
    "       'DueDateIs_quarter_start', 'DueDateIs_year_end', 'DueDateIs_year_start',\n",
    "       'DueDateElapsed','pc1', 'pc2', 'pc3', 'kmeans'\n",
    "            ]\n",
    "Target_name=\"IsDefaulted\"\n",
    "not_used_cols=[Target_name,'TransactionStartTime',\"TransactionStartTimeMonth\"]\n",
    "features_name=[ f for f in train_col if f not in not_used_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_train.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are 6 group fold, so do a train test on the 6 fold\n",
    "from sklearn.model_selection import train_test_split, KFold, GroupKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import  auc, roc_auc_score, roc_curve\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "sc = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_train.to_csv('cleaned_train.csv', index=False)\n",
    "cleaned_test.to_csv('cleaned_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets head to modelling"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
